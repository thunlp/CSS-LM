[{"label": "PART-OF", "tokens": "[[ Recognition of proper nouns ]] in Japanese text has been studied as a part of the more general problem of << morphological analysis >> in Japanese text processing -LRB- -LSB- 1 -RSB- -LSB- 2 -RSB- -RRB- .", "h": ["Recognition of proper nouns"], "t": ["morphological analysis"]}, {"label": "PART-OF", "tokens": "Recognition of [[ proper nouns ]] in << Japanese text >> has been studied as a part of the more general problem of morphological analysis in Japanese text processing -LRB- -LSB- 1 -RSB- -LSB- 2 -RSB- -RRB- .", "h": ["proper nouns"], "t": ["Japanese text"]}, {"label": "USED-FOR", "tokens": "Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of [[ morphological analysis ]] in << Japanese text processing >> -LRB- -LSB- 1 -RSB- -LSB- 2 -RSB- -RRB- .", "h": ["morphological analysis"], "t": ["Japanese text processing"]}, {"label": "USED-FOR", "tokens": "<< It >> has also been studied in the framework of [[ Japanese information extraction ]] -LRB- -LSB- 3 -RSB- -RRB- in recent years .", "h": ["Japanese information extraction"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "Our [[ approach ]] to the << Multi-lingual Evaluation Task -LRB- MET -RRB- >> for Japanese text is to consider the given task as a morphological analysis problem in Japanese .", "h": ["approach"], "t": ["Multi-lingual Evaluation Task -LRB- MET -RRB-"]}, {"label": "USED-FOR", "tokens": "Our approach to the [[ Multi-lingual Evaluation Task -LRB- MET -RRB- ]] for << Japanese text >> is to consider the given task as a morphological analysis problem in Japanese .", "h": ["Multi-lingual Evaluation Task -LRB- MET -RRB-"], "t": ["Japanese text"]}, {"label": "USED-FOR", "tokens": "Our approach to the Multi-lingual Evaluation Task -LRB- MET -RRB- for Japanese text is to consider the given << task >> as a [[ morphological analysis problem ]] in Japanese .", "h": ["morphological analysis problem"], "t": ["task"]}, {"label": "USED-FOR", "tokens": "Our approach to the Multi-lingual Evaluation Task -LRB- MET -RRB- for Japanese text is to consider the given task as a << morphological analysis problem >> in [[ Japanese ]] .", "h": ["Japanese"], "t": ["morphological analysis problem"]}, {"label": "USED-FOR", "tokens": "Our [[ morphological analyzer ]] has done all the necessary work for the << recognition and classification of proper names , numerical and temporal expressions >> , i.e. Named Entity -LRB- NE -RRB- items in the Japanese text .", "h": ["morphological analyzer"], "t": ["recognition and classification of proper names , numerical and temporal expressions"]}, {"label": "HYPONYM-OF", "tokens": "Our morphological analyzer has done all the necessary work for the recognition and classification of << proper names , numerical and temporal expressions >> , i.e. [[ Named Entity -LRB- NE -RRB- items ]] in the Japanese text .", "h": ["Named Entity -LRB- NE -RRB- items"], "t": ["proper names , numerical and temporal expressions"]}, {"label": "PART-OF", "tokens": "Our morphological analyzer has done all the necessary work for the recognition and classification of proper names , numerical and temporal expressions , i.e. [[ Named Entity -LRB- NE -RRB- items ]] in the << Japanese text >> .", "h": ["Named Entity -LRB- NE -RRB- items"], "t": ["Japanese text"]}, {"label": "USED-FOR", "tokens": "[[ Amorph ]] recognizes << NE items >> in two stages : dictionary lookup and rule application .", "h": ["Amorph"], "t": ["NE items"]}, {"label": "PART-OF", "tokens": "<< Amorph >> recognizes NE items in two stages : [[ dictionary lookup ]] and rule application .", "h": ["dictionary lookup"], "t": ["Amorph"]}, {"label": "CONJUNCTION", "tokens": "Amorph recognizes NE items in two stages : [[ dictionary lookup ]] and << rule application >> .", "h": ["dictionary lookup"], "t": ["rule application"]}, {"label": "PART-OF", "tokens": "<< Amorph >> recognizes NE items in two stages : dictionary lookup and [[ rule application ]] .", "h": ["rule application"], "t": ["Amorph"]}, {"label": "USED-FOR", "tokens": "First , << it >> uses several kinds of [[ dictionaries ]] to segment and tag Japanese character strings .", "h": ["dictionaries"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "First , it uses several kinds of [[ dictionaries ]] to segment and tag << Japanese character strings >> .", "h": ["dictionaries"], "t": ["Japanese character strings"]}, {"label": "USED-FOR", "tokens": "Second , based on the information resulting from the [[ dictionary lookup stage ]] , a set of << rules >> is applied to the segmented strings in order to identify NE items .", "h": ["dictionary lookup stage"], "t": ["rules"]}, {"label": "USED-FOR", "tokens": "Second , based on the information resulting from the dictionary lookup stage , a set of [[ rules ]] is applied to the segmented strings in order to identify << NE items >> .", "h": ["rules"], "t": ["NE items"]}, {"label": "PART-OF", "tokens": "We propose to incorporate a [[ priori geometric constraints ]] in a << 3 -- D stereo reconstruction scheme >> to cope with the many cases where image information alone is not sufficient to accurately recover 3 -- D shape .", "h": ["priori geometric constraints"], "t": ["3 -- D stereo reconstruction scheme"]}, {"label": "USED-FOR", "tokens": "We propose to incorporate a priori geometric constraints in a 3 -- D stereo reconstruction scheme to cope with the many cases where [[ image information ]] alone is not sufficient to accurately recover << 3 -- D shape >> .", "h": ["image information"], "t": ["3 -- D shape"]}, {"label": "USED-FOR", "tokens": "Our << approach >> is based on the [[ iterative deformation of a 3 -- D surface mesh ]] to minimize an objective function .", "h": ["iterative deformation of a 3 -- D surface mesh"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "Our approach is based on the [[ iterative deformation of a 3 -- D surface mesh ]] to minimize an << objective function >> .", "h": ["iterative deformation of a 3 -- D surface mesh"], "t": ["objective function"]}, {"label": "CONJUNCTION", "tokens": "We show that combining [[ anisotropic meshing ]] with a << non-quadratic approach >> to regularization enables us to obtain satisfactory reconstruction results using triangulations with few vertices .", "h": ["anisotropic meshing"], "t": ["non-quadratic approach"]}, {"label": "USED-FOR", "tokens": "We show that combining [[ anisotropic meshing ]] with a non-quadratic approach to regularization enables us to obtain satisfactory << reconstruction >> results using triangulations with few vertices .", "h": ["anisotropic meshing"], "t": ["reconstruction"]}, {"label": "USED-FOR", "tokens": "We show that combining anisotropic meshing with a [[ non-quadratic approach ]] to << regularization >> enables us to obtain satisfactory reconstruction results using triangulations with few vertices .", "h": ["non-quadratic approach"], "t": ["regularization"]}, {"label": "USED-FOR", "tokens": "We show that combining anisotropic meshing with a [[ non-quadratic approach ]] to regularization enables us to obtain satisfactory << reconstruction >> results using triangulations with few vertices .", "h": ["non-quadratic approach"], "t": ["reconstruction"]}, {"label": "USED-FOR", "tokens": "We show that combining anisotropic meshing with a non-quadratic approach to regularization enables us to obtain satisfactory << reconstruction >> results using [[ triangulations ]] with few vertices .", "h": ["triangulations"], "t": ["reconstruction"]}, {"label": "USED-FOR", "tokens": "[[ Structural or numerical constraints ]] can then be added locally to the << reconstruction process >> through a constrained optimization scheme .", "h": ["Structural or numerical constraints"], "t": ["reconstruction process"]}, {"label": "USED-FOR", "tokens": "<< Structural or numerical constraints >> can then be added locally to the reconstruction process through a [[ constrained optimization scheme ]] .", "h": ["constrained optimization scheme"], "t": ["Structural or numerical constraints"]}, {"label": "USED-FOR", "tokens": "[[ They ]] improve the << reconstruction >> results and enforce their consistency with a priori knowledge about object shape .", "h": ["They"], "t": ["reconstruction"]}, {"label": "FEATURE-OF", "tokens": "They improve the reconstruction results and enforce their consistency with a << priori knowledge >> about [[ object shape ]] .", "h": ["object shape"], "t": ["priori knowledge"]}, {"label": "USED-FOR", "tokens": "The strong description and modeling properties of differential features make [[ them ]] useful tools that can be efficiently used as constraints for << 3 -- D reconstruction >> .", "h": ["them"], "t": ["3 -- D reconstruction"]}, {"label": "USED-FOR", "tokens": "It is based on a [[ weakly supervised dependency parser ]] that can model << speech syntax >> without relying on any annotated training corpus .", "h": ["weakly supervised dependency parser"], "t": ["speech syntax"]}, {"label": "USED-FOR", "tokens": "Labeled data is replaced by a few [[ hand-crafted rules ]] that encode basic << syntactic knowledge >> .", "h": ["hand-crafted rules"], "t": ["syntactic knowledge"]}, {"label": "USED-FOR", "tokens": "[[ Bayesian inference ]] then samples the << rules >> , disambiguating and combining them to create complex tree structures that maximize a discriminative model 's posterior on a target unlabeled corpus .", "h": ["Bayesian inference"], "t": ["rules"]}, {"label": "USED-FOR", "tokens": "Bayesian inference then samples the rules , disambiguating and combining [[ them ]] to create << complex tree structures >> that maximize a discriminative model 's posterior on a target unlabeled corpus .", "h": ["them"], "t": ["complex tree structures"]}, {"label": "USED-FOR", "tokens": "Bayesian inference then samples the rules , disambiguating and combining them to create [[ complex tree structures ]] that maximize a << discriminative model 's posterior >> on a target unlabeled corpus .", "h": ["complex tree structures"], "t": ["discriminative model 's posterior"]}, {"label": "USED-FOR", "tokens": "Bayesian inference then samples the rules , disambiguating and combining them to create complex tree structures that maximize a << discriminative model 's posterior >> on a target [[ unlabeled corpus ]] .", "h": ["unlabeled corpus"], "t": ["discriminative model 's posterior"]}, {"label": "USED-FOR", "tokens": "This [[ posterior ]] encodes << sparse se-lectional preferences >> between a head word and its dependents .", "h": ["posterior"], "t": ["sparse se-lectional preferences"]}, {"label": "EVALUATE-FOR", "tokens": "The << model >> is evaluated on [[ English and Czech newspaper texts ]] , and is then validated on French broadcast news transcriptions .", "h": ["English and Czech newspaper texts"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "The << model >> is evaluated on English and Czech newspaper texts , and is then validated on [[ French broadcast news transcriptions ]] .", "h": ["French broadcast news transcriptions"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "[[ Listen-Communicate-Show -LRB- LCS -RRB- ]] is a new paradigm for << human interaction with data sources >> .", "h": ["Listen-Communicate-Show -LRB- LCS -RRB-"], "t": ["human interaction with data sources"]}, {"label": "PART-OF", "tokens": "We integrate a << spoken language understanding system >> with [[ intelligent mobile agents ]] that mediate between users and information sources .", "h": ["intelligent mobile agents"], "t": ["spoken language understanding system"]}, {"label": "USED-FOR", "tokens": "We have built and will demonstrate an application of this [[ approach ]] called << LCS-Marine >> .", "h": ["approach"], "t": ["LCS-Marine"]}, {"label": "USED-FOR", "tokens": "A [[ domain independent model ]] is proposed for the << automated interpretation of nominal compounds >> in English .", "h": ["domain independent model"], "t": ["automated interpretation of nominal compounds"]}, {"label": "FEATURE-OF", "tokens": "A domain independent model is proposed for the automated interpretation of << nominal compounds >> in [[ English ]] .", "h": ["English"], "t": ["nominal compounds"]}, {"label": "USED-FOR", "tokens": "This [[ model ]] is meant to account for << productive rules of interpretation >> which are inferred from the morpho-syntactic and semantic characteristics of the nominal constituents .", "h": ["model"], "t": ["productive rules of interpretation"]}, {"label": "USED-FOR", "tokens": "This model is meant to account for << productive rules of interpretation >> which are inferred from the [[ morpho-syntactic and semantic characteristics ]] of the nominal constituents .", "h": ["morpho-syntactic and semantic characteristics"], "t": ["productive rules of interpretation"]}, {"label": "FEATURE-OF", "tokens": "This model is meant to account for productive rules of interpretation which are inferred from the [[ morpho-syntactic and semantic characteristics ]] of the << nominal constituents >> .", "h": ["morpho-syntactic and semantic characteristics"], "t": ["nominal constituents"]}, {"label": "FEATURE-OF", "tokens": "In particular , we make extensive use of Pustejovsky 's principles concerning the << predicative information >> associated with [[ nominals ]] .", "h": ["nominals"], "t": ["predicative information"]}, {"label": "COMPARE", "tokens": "We argue that it is necessary to draw a line between [[ generalizable semantic principles ]] and << domain-specific semantic information >> .", "h": ["generalizable semantic principles"], "t": ["domain-specific semantic information"]}, {"label": "USED-FOR", "tokens": "We explain this distinction and we show how this [[ model ]] may be applied to the << interpretation of compounds >> in real texts , provided that complementary semantic information are retrieved .", "h": ["model"], "t": ["interpretation of compounds"]}, {"label": "USED-FOR", "tokens": "We present a new [[ method ]] for << detecting interest points >> using histogram information .", "h": ["method"], "t": ["detecting interest points"]}, {"label": "USED-FOR", "tokens": "We present a new method for << detecting interest points >> using [[ histogram information ]] .", "h": ["histogram information"], "t": ["detecting interest points"]}, {"label": "EVALUATE-FOR", "tokens": "Unlike existing << interest point detectors >> , which measure [[ pixel-wise differences in image intensity ]] , our detectors incorporate histogram-based representations , and thus can find image regions that present a distinct distribution in the neighborhood .", "h": ["pixel-wise differences in image intensity"], "t": ["interest point detectors"]}, {"label": "PART-OF", "tokens": "Unlike existing interest point detectors , which measure pixel-wise differences in image intensity , our << detectors >> incorporate [[ histogram-based representations ]] , and thus can find image regions that present a distinct distribution in the neighborhood .", "h": ["histogram-based representations"], "t": ["detectors"]}, {"label": "USED-FOR", "tokens": "The proposed [[ detectors ]] are able to capture << large-scale structures >> and distinctive textured patterns , and exhibit strong invariance to rotation , illumination variation , and blur .", "h": ["detectors"], "t": ["large-scale structures"]}, {"label": "USED-FOR", "tokens": "The proposed [[ detectors ]] are able to capture large-scale structures and << distinctive textured patterns >> , and exhibit strong invariance to rotation , illumination variation , and blur .", "h": ["detectors"], "t": ["distinctive textured patterns"]}, {"label": "USED-FOR", "tokens": "The proposed [[ detectors ]] are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to << rotation >> , illumination variation , and blur .", "h": ["detectors"], "t": ["rotation"]}, {"label": "USED-FOR", "tokens": "The proposed [[ detectors ]] are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to rotation , << illumination variation >> , and blur .", "h": ["detectors"], "t": ["illumination variation"]}, {"label": "USED-FOR", "tokens": "The proposed [[ detectors ]] are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to rotation , illumination variation , and << blur >> .", "h": ["detectors"], "t": ["blur"]}, {"label": "CONJUNCTION", "tokens": "The proposed detectors are able to capture [[ large-scale structures ]] and << distinctive textured patterns >> , and exhibit strong invariance to rotation , illumination variation , and blur .", "h": ["large-scale structures"], "t": ["distinctive textured patterns"]}, {"label": "CONJUNCTION", "tokens": "The proposed detectors are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to [[ rotation ]] , << illumination variation >> , and blur .", "h": ["rotation"], "t": ["illumination variation"]}, {"label": "CONJUNCTION", "tokens": "The proposed detectors are able to capture large-scale structures and distinctive textured patterns , and exhibit strong invariance to rotation , [[ illumination variation ]] , and << blur >> .", "h": ["illumination variation"], "t": ["blur"]}, {"label": "USED-FOR", "tokens": "The experimental results show that the proposed [[ histogram-based interest point detectors ]] perform particularly well for the tasks of << matching textured scenes >> under blur and illumination changes , in terms of repeatability and distinctiveness .", "h": ["histogram-based interest point detectors"], "t": ["matching textured scenes"]}, {"label": "EVALUATE-FOR", "tokens": "The experimental results show that the proposed << histogram-based interest point detectors >> perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of [[ repeatability ]] and distinctiveness .", "h": ["repeatability"], "t": ["histogram-based interest point detectors"]}, {"label": "CONJUNCTION", "tokens": "The experimental results show that the proposed histogram-based interest point detectors perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of [[ repeatability ]] and << distinctiveness >> .", "h": ["repeatability"], "t": ["distinctiveness"]}, {"label": "EVALUATE-FOR", "tokens": "The experimental results show that the proposed << histogram-based interest point detectors >> perform particularly well for the tasks of matching textured scenes under blur and illumination changes , in terms of repeatability and [[ distinctiveness ]] .", "h": ["distinctiveness"], "t": ["histogram-based interest point detectors"]}, {"label": "USED-FOR", "tokens": "An extension of our [[ method ]] to << space-time interest point detection >> for action classification is also presented .", "h": ["method"], "t": ["space-time interest point detection"]}, {"label": "USED-FOR", "tokens": "An extension of our method to [[ space-time interest point detection ]] for << action classification >> is also presented .", "h": ["space-time interest point detection"], "t": ["action classification"]}, {"label": "HYPONYM-OF", "tokens": "We have implemented a << restricted domain parser >> called [[ Plume ]] .", "h": ["Plume"], "t": ["restricted domain parser"]}, {"label": "USED-FOR", "tokens": "Building on previous work at Carnegie-Mellon University e.g. -LSB- 4 , 5 , 8 -RSB- , [[ Plume 's approach ]] to << parsing >> is based on semantic caseframe instantiation .", "h": ["Plume 's approach"], "t": ["parsing"]}, {"label": "USED-FOR", "tokens": "Building on previous work at Carnegie-Mellon University e.g. -LSB- 4 , 5 , 8 -RSB- , << Plume 's approach >> to parsing is based on [[ semantic caseframe instantiation ]] .", "h": ["semantic caseframe instantiation"], "t": ["Plume 's approach"]}, {"label": "FEATURE-OF", "tokens": "This has the advantages of efficiency on grammatical input , and << robustness >> in the face of [[ ungrammatical input ]] .", "h": ["ungrammatical input"], "t": ["robustness"]}, {"label": "USED-FOR", "tokens": "While [[ Plume ]] is well adapted to simple << declarative and imperative utterances >> , it handles passives , relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage .", "h": ["Plume"], "t": ["declarative and imperative utterances"]}, {"label": "USED-FOR", "tokens": "While Plume is well adapted to simple declarative and imperative utterances , [[ it ]] handles << passives >> , relative clauses and interrogatives in an ad hoc manner leading to patchy syntactic coverage .", "h": ["it"], "t": ["passives"]}, {"label": "USED-FOR", "tokens": "While Plume is well adapted to simple declarative and imperative utterances , [[ it ]] handles passives , << relative clauses >> and interrogatives in an ad hoc manner leading to patchy syntactic coverage .", "h": ["it"], "t": ["relative clauses"]}, {"label": "USED-FOR", "tokens": "While Plume is well adapted to simple declarative and imperative utterances , [[ it ]] handles passives , relative clauses and << interrogatives >> in an ad hoc manner leading to patchy syntactic coverage .", "h": ["it"], "t": ["interrogatives"]}, {"label": "CONJUNCTION", "tokens": "While Plume is well adapted to simple declarative and imperative utterances , it handles [[ passives ]] , << relative clauses >> and interrogatives in an ad hoc manner leading to patchy syntactic coverage .", "h": ["passives"], "t": ["relative clauses"]}, {"label": "CONJUNCTION", "tokens": "While Plume is well adapted to simple declarative and imperative utterances , it handles passives , [[ relative clauses ]] and << interrogatives >> in an ad hoc manner leading to patchy syntactic coverage .", "h": ["relative clauses"], "t": ["interrogatives"]}, {"label": "USED-FOR", "tokens": "This paper outlines Plume as it currently exists and describes our detailed design for extending [[ Plume ]] to handle << passives >> , relative clauses , and interrogatives in a general manner .", "h": ["Plume"], "t": ["passives"]}, {"label": "USED-FOR", "tokens": "This paper outlines Plume as it currently exists and describes our detailed design for extending [[ Plume ]] to handle passives , << relative clauses >> , and interrogatives in a general manner .", "h": ["Plume"], "t": ["relative clauses"]}, {"label": "USED-FOR", "tokens": "This paper outlines Plume as it currently exists and describes our detailed design for extending [[ Plume ]] to handle passives , relative clauses , and << interrogatives >> in a general manner .", "h": ["Plume"], "t": ["interrogatives"]}, {"label": "CONJUNCTION", "tokens": "This paper outlines Plume as it currently exists and describes our detailed design for extending Plume to handle [[ passives ]] , << relative clauses >> , and interrogatives in a general manner .", "h": ["passives"], "t": ["relative clauses"]}, {"label": "CONJUNCTION", "tokens": "This paper outlines Plume as it currently exists and describes our detailed design for extending Plume to handle passives , [[ relative clauses ]] , and << interrogatives >> in a general manner .", "h": ["relative clauses"], "t": ["interrogatives"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an [[ unlexicalized parser ]] for << German >> which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus .", "h": ["unlexicalized parser"], "t": ["German"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an << unlexicalized parser >> for German which employs [[ smoothing ]] and suffix analysis to achieve a labelled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus .", "h": ["smoothing"], "t": ["unlexicalized parser"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we present an unlexicalized parser for German which employs [[ smoothing ]] and << suffix analysis >> to achieve a labelled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus .", "h": ["smoothing"], "t": ["suffix analysis"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an << unlexicalized parser >> for German which employs smoothing and [[ suffix analysis ]] to achieve a labelled bracket F-score of 76.2 , higher than previously reported results on the NEGRA corpus .", "h": ["suffix analysis"], "t": ["unlexicalized parser"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper , we present an << unlexicalized parser >> for German which employs smoothing and suffix analysis to achieve a [[ labelled bracket F-score ]] of 76.2 , higher than previously reported results on the NEGRA corpus .", "h": ["labelled bracket F-score"], "t": ["unlexicalized parser"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper , we present an << unlexicalized parser >> for German which employs smoothing and suffix analysis to achieve a labelled bracket F-score of 76.2 , higher than previously reported results on the [[ NEGRA corpus ]] .", "h": ["NEGRA corpus"], "t": ["unlexicalized parser"]}, {"label": "EVALUATE-FOR", "tokens": "In addition to the high [[ accuracy ]] of the << model >> , the use of smoothing in an unlexicalized parser allows us to better examine the interplay between smoothing and parsing results .", "h": ["accuracy"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "In addition to the high accuracy of the model , the use of [[ smoothing ]] in an << unlexicalized parser >> allows us to better examine the interplay between smoothing and parsing results .", "h": ["smoothing"], "t": ["unlexicalized parser"]}, {"label": "USED-FOR", "tokens": "This paper presents an [[ unsupervised learning approach ]] to disambiguate various << relations between named entities >> by use of various lexical and syntactic features from the contexts .", "h": ["unsupervised learning approach"], "t": ["relations between named entities"]}, {"label": "USED-FOR", "tokens": "This paper presents an << unsupervised learning approach >> to disambiguate various relations between named entities by use of various [[ lexical and syntactic features ]] from the contexts .", "h": ["lexical and syntactic features"], "t": ["unsupervised learning approach"]}, {"label": "USED-FOR", "tokens": "[[ It ]] works by calculating eigenvectors of an adjacency graph 's Laplacian to recover a << submanifold >> of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors .", "h": ["It"], "t": ["submanifold"]}, {"label": "USED-FOR", "tokens": "<< It >> works by calculating [[ eigenvectors ]] of an adjacency graph 's Laplacian to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors .", "h": ["eigenvectors"], "t": ["It"]}, {"label": "FEATURE-OF", "tokens": "It works by calculating << eigenvectors >> of an [[ adjacency graph 's Laplacian ]] to recover a submanifold of data from a high dimensionality space and then performing cluster number estimation on the eigenvectors .", "h": ["adjacency graph 's Laplacian"], "t": ["eigenvectors"]}, {"label": "USED-FOR", "tokens": "It works by calculating eigenvectors of an adjacency graph 's Laplacian to recover a << submanifold >> of data from a [[ high dimensionality space ]] and then performing cluster number estimation on the eigenvectors .", "h": ["high dimensionality space"], "t": ["submanifold"]}, {"label": "USED-FOR", "tokens": "<< It >> works by calculating eigenvectors of an adjacency graph 's Laplacian to recover a submanifold of data from a high dimensionality space and then performing [[ cluster number estimation ]] on the eigenvectors .", "h": ["cluster number estimation"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "It works by calculating eigenvectors of an adjacency graph 's Laplacian to recover a submanifold of data from a high dimensionality space and then performing [[ cluster number estimation ]] on the << eigenvectors >> .", "h": ["cluster number estimation"], "t": ["eigenvectors"]}, {"label": "EVALUATE-FOR", "tokens": "Experiment results on [[ ACE corpora ]] show that this << spectral clustering based approach >> outperforms the other clustering methods .", "h": ["ACE corpora"], "t": ["spectral clustering based approach"]}, {"label": "EVALUATE-FOR", "tokens": "Experiment results on [[ ACE corpora ]] show that this spectral clustering based approach outperforms the other << clustering methods >> .", "h": ["ACE corpora"], "t": ["clustering methods"]}, {"label": "COMPARE", "tokens": "Experiment results on ACE corpora show that this [[ spectral clustering based approach ]] outperforms the other << clustering methods >> .", "h": ["spectral clustering based approach"], "t": ["clustering methods"]}, {"label": "HYPONYM-OF", "tokens": "This paper proposes a generic mathematical formalism for the combination of various << structures >> : [[ strings ]] , trees , dags , graphs , and products of them .", "h": ["strings"], "t": ["structures"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes a generic mathematical formalism for the combination of various structures : [[ strings ]] , << trees >> , dags , graphs , and products of them .", "h": ["strings"], "t": ["trees"]}, {"label": "HYPONYM-OF", "tokens": "This paper proposes a generic mathematical formalism for the combination of various << structures >> : strings , [[ trees ]] , dags , graphs , and products of them .", "h": ["trees"], "t": ["structures"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes a generic mathematical formalism for the combination of various structures : strings , [[ trees ]] , << dags >> , graphs , and products of them .", "h": ["trees"], "t": ["dags"]}, {"label": "HYPONYM-OF", "tokens": "This paper proposes a generic mathematical formalism for the combination of various << structures >> : strings , trees , [[ dags ]] , graphs , and products of them .", "h": ["dags"], "t": ["structures"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes a generic mathematical formalism for the combination of various structures : strings , trees , [[ dags ]] , << graphs >> , and products of them .", "h": ["dags"], "t": ["graphs"]}, {"label": "HYPONYM-OF", "tokens": "This paper proposes a generic mathematical formalism for the combination of various << structures >> : strings , trees , dags , [[ graphs ]] , and products of them .", "h": ["graphs"], "t": ["structures"]}, {"label": "USED-FOR", "tokens": "This [[ formalism ]] is both elementary and powerful enough to strongly simulate many << grammar formalisms >> , such as rewriting systems , dependency grammars , TAG , HPSG and LFG .", "h": ["formalism"], "t": ["grammar formalisms"]}, {"label": "HYPONYM-OF", "tokens": "This formalism is both elementary and powerful enough to strongly simulate many << grammar formalisms >> , such as [[ rewriting systems ]] , dependency grammars , TAG , HPSG and LFG .", "h": ["rewriting systems"], "t": ["grammar formalisms"]}, {"label": "CONJUNCTION", "tokens": "This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as [[ rewriting systems ]] , << dependency grammars >> , TAG , HPSG and LFG .", "h": ["rewriting systems"], "t": ["dependency grammars"]}, {"label": "HYPONYM-OF", "tokens": "This formalism is both elementary and powerful enough to strongly simulate many << grammar formalisms >> , such as rewriting systems , [[ dependency grammars ]] , TAG , HPSG and LFG .", "h": ["dependency grammars"], "t": ["grammar formalisms"]}, {"label": "CONJUNCTION", "tokens": "This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , [[ dependency grammars ]] , << TAG >> , HPSG and LFG .", "h": ["dependency grammars"], "t": ["TAG"]}, {"label": "HYPONYM-OF", "tokens": "This formalism is both elementary and powerful enough to strongly simulate many << grammar formalisms >> , such as rewriting systems , dependency grammars , [[ TAG ]] , HPSG and LFG .", "h": ["TAG"], "t": ["grammar formalisms"]}, {"label": "CONJUNCTION", "tokens": "This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , [[ TAG ]] , << HPSG >> and LFG .", "h": ["TAG"], "t": ["HPSG"]}, {"label": "HYPONYM-OF", "tokens": "This formalism is both elementary and powerful enough to strongly simulate many << grammar formalisms >> , such as rewriting systems , dependency grammars , TAG , [[ HPSG ]] and LFG .", "h": ["HPSG"], "t": ["grammar formalisms"]}, {"label": "CONJUNCTION", "tokens": "This formalism is both elementary and powerful enough to strongly simulate many grammar formalisms , such as rewriting systems , dependency grammars , TAG , [[ HPSG ]] and << LFG >> .", "h": ["HPSG"], "t": ["LFG"]}, {"label": "HYPONYM-OF", "tokens": "This formalism is both elementary and powerful enough to strongly simulate many << grammar formalisms >> , such as rewriting systems , dependency grammars , TAG , HPSG and [[ LFG ]] .", "h": ["LFG"], "t": ["grammar formalisms"]}, {"label": "USED-FOR", "tokens": "A [[ mixed-signal paradigm ]] is presented for << high-resolution parallel inner-product computation >> in very high dimensions , suitable for efficient implementation of kernels in image processing .", "h": ["mixed-signal paradigm"], "t": ["high-resolution parallel inner-product computation"]}, {"label": "USED-FOR", "tokens": "A [[ mixed-signal paradigm ]] is presented for high-resolution parallel inner-product computation in very high dimensions , suitable for efficient implementation of << kernels >> in image processing .", "h": ["mixed-signal paradigm"], "t": ["kernels"]}, {"label": "USED-FOR", "tokens": "A mixed-signal paradigm is presented for high-resolution parallel inner-product computation in very high dimensions , suitable for efficient implementation of [[ kernels ]] in << image processing >> .", "h": ["kernels"], "t": ["image processing"]}, {"label": "PART-OF", "tokens": "At the core of the << externally digital architecture >> is a [[ high-density , low-power analog array ]] performing binary-binary partial matrix-vector multiplication .", "h": ["high-density , low-power analog array"], "t": ["externally digital architecture"]}, {"label": "USED-FOR", "tokens": "At the core of the externally digital architecture is a << high-density , low-power analog array >> performing [[ binary-binary partial matrix-vector multiplication ]] .", "h": ["binary-binary partial matrix-vector multiplication"], "t": ["high-density , low-power analog array"]}, {"label": "PART-OF", "tokens": "Full digital resolution is maintained even with low-resolution analog-to-digital conversion , owing to [[ random statistics ]] in the << analog summation of binary products >> .", "h": ["random statistics"], "t": ["analog summation of binary products"]}, {"label": "USED-FOR", "tokens": "A [[ random modulation scheme ]] produces << near-Bernoulli statistics >> even for highly correlated inputs .", "h": ["random modulation scheme"], "t": ["near-Bernoulli statistics"]}, {"label": "USED-FOR", "tokens": "A << random modulation scheme >> produces near-Bernoulli statistics even for [[ highly correlated inputs ]] .", "h": ["highly correlated inputs"], "t": ["random modulation scheme"]}, {"label": "EVALUATE-FOR", "tokens": "The << approach >> is validated with [[ real image data ]] , and with experimental results from a CID/DRAM analog array prototype in 0.5 cents m CMOS .", "h": ["real image data"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "In this paper we specialize the [[ projective unifocal , bifo-cal , and trifocal tensors ]] to the << affine case >> , and show how the tensors obtained relate to the registered tensors encountered in previous work .", "h": ["projective unifocal , bifo-cal , and trifocal tensors"], "t": ["affine case"]}, {"label": "USED-FOR", "tokens": "Finally , we show how the << estimation of the tensors >> from [[ point correspondences ]] is achieved through factorization , and discuss the estimation from line correspondences .", "h": ["point correspondences"], "t": ["estimation of the tensors"]}, {"label": "USED-FOR", "tokens": "Finally , we show how the estimation of the << tensors >> from point correspondences is achieved through [[ factorization ]] , and discuss the estimation from line correspondences .", "h": ["factorization"], "t": ["tensors"]}, {"label": "USED-FOR", "tokens": "Finally , we show how the estimation of the tensors from point correspondences is achieved through factorization , and discuss the << estimation >> from [[ line correspondences ]] .", "h": ["line correspondences"], "t": ["estimation"]}, {"label": "USED-FOR", "tokens": "We propose a [[ corpus-based method ]] -LRB- Biber ,1993 ; Nagao ,1993 ; Smadja ,1993 -RRB- which generates << Noun Classifier Associations -LRB- NCA -RRB- >> to overcome the problems in classifier assignment and semantic construction of noun phrase .", "h": ["corpus-based method"], "t": ["Noun Classifier Associations -LRB- NCA -RRB-"]}, {"label": "USED-FOR", "tokens": "We propose a [[ corpus-based method ]] -LRB- Biber ,1993 ; Nagao ,1993 ; Smadja ,1993 -RRB- which generates Noun Classifier Associations -LRB- NCA -RRB- to overcome the problems in << classifier assignment >> and semantic construction of noun phrase .", "h": ["corpus-based method"], "t": ["classifier assignment"]}, {"label": "USED-FOR", "tokens": "We propose a [[ corpus-based method ]] -LRB- Biber ,1993 ; Nagao ,1993 ; Smadja ,1993 -RRB- which generates Noun Classifier Associations -LRB- NCA -RRB- to overcome the problems in classifier assignment and << semantic construction of noun phrase >> .", "h": ["corpus-based method"], "t": ["semantic construction of noun phrase"]}, {"label": "USED-FOR", "tokens": "We propose a corpus-based method -LRB- Biber ,1993 ; Nagao ,1993 ; Smadja ,1993 -RRB- which generates [[ Noun Classifier Associations -LRB- NCA -RRB- ]] to overcome the problems in << classifier assignment >> and semantic construction of noun phrase .", "h": ["Noun Classifier Associations -LRB- NCA -RRB-"], "t": ["classifier assignment"]}, {"label": "USED-FOR", "tokens": "We propose a corpus-based method -LRB- Biber ,1993 ; Nagao ,1993 ; Smadja ,1993 -RRB- which generates [[ Noun Classifier Associations -LRB- NCA -RRB- ]] to overcome the problems in classifier assignment and << semantic construction of noun phrase >> .", "h": ["Noun Classifier Associations -LRB- NCA -RRB-"], "t": ["semantic construction of noun phrase"]}, {"label": "CONJUNCTION", "tokens": "We propose a corpus-based method -LRB- Biber ,1993 ; Nagao ,1993 ; Smadja ,1993 -RRB- which generates Noun Classifier Associations -LRB- NCA -RRB- to overcome the problems in [[ classifier assignment ]] and << semantic construction of noun phrase >> .", "h": ["classifier assignment"], "t": ["semantic construction of noun phrase"]}, {"label": "USED-FOR", "tokens": "The << NCA >> is created statistically from a large corpus and recomposed under [[ concept hierarchy constraints ]] and frequency of occurrences .", "h": ["concept hierarchy constraints"], "t": ["NCA"]}, {"label": "USED-FOR", "tokens": "The << NCA >> is created statistically from a large corpus and recomposed under concept hierarchy constraints and [[ frequency of occurrences ]] .", "h": ["frequency of occurrences"], "t": ["NCA"]}, {"label": "USED-FOR", "tokens": "The << perception of transparent objects >> from [[ images ]] is known to be a very hard problem in vision .", "h": ["images"], "t": ["perception of transparent objects"]}, {"label": "COMPARE", "tokens": "We show how << features >> that are imaged through a transparent object behave differently from [[ those ]] that are rigidly attached to the scene .", "h": ["those"], "t": ["features"]}, {"label": "USED-FOR", "tokens": "We present a novel [[ model-based approach ]] to recover the << shapes and the poses of transparent objects >> from known motion .", "h": ["model-based approach"], "t": ["shapes and the poses of transparent objects"]}, {"label": "USED-FOR", "tokens": "We present a novel model-based approach to recover the << shapes and the poses of transparent objects >> from [[ known motion ]] .", "h": ["known motion"], "t": ["shapes and the poses of transparent objects"]}, {"label": "PART-OF", "tokens": "The objects can be complex in that << they >> may be composed of [[ multiple layers ]] with different refractive indices .", "h": ["multiple layers"], "t": ["they"]}, {"label": "FEATURE-OF", "tokens": "The objects can be complex in that they may be composed of << multiple layers >> with different [[ refractive indices ]] .", "h": ["refractive indices"], "t": ["multiple layers"]}, {"label": "USED-FOR", "tokens": "We have applied [[ it ]] to << real scenes >> that include transparent objects and recovered the shapes of the objects with high accuracy .", "h": ["it"], "t": ["real scenes"]}, {"label": "USED-FOR", "tokens": "We have applied [[ it ]] to real scenes that include transparent objects and recovered the << shapes of the objects >> with high accuracy .", "h": ["it"], "t": ["shapes of the objects"]}, {"label": "PART-OF", "tokens": "We have applied it to << real scenes >> that include [[ transparent objects ]] and recovered the shapes of the objects with high accuracy .", "h": ["transparent objects"], "t": ["real scenes"]}, {"label": "EVALUATE-FOR", "tokens": "We have applied it to real scenes that include transparent objects and recovered the << shapes of the objects >> with high [[ accuracy ]] .", "h": ["accuracy"], "t": ["shapes of the objects"]}, {"label": "USED-FOR", "tokens": "We propose a novel [[ probabilistic framework ]] for learning << visual models of 3D object categories >> by combining appearance information and geometric constraints .", "h": ["probabilistic framework"], "t": ["visual models of 3D object categories"]}, {"label": "USED-FOR", "tokens": "We propose a novel << probabilistic framework >> for learning visual models of 3D object categories by combining [[ appearance information ]] and geometric constraints .", "h": ["appearance information"], "t": ["probabilistic framework"]}, {"label": "CONJUNCTION", "tokens": "We propose a novel probabilistic framework for learning visual models of 3D object categories by combining [[ appearance information ]] and << geometric constraints >> .", "h": ["appearance information"], "t": ["geometric constraints"]}, {"label": "USED-FOR", "tokens": "We propose a novel << probabilistic framework >> for learning visual models of 3D object categories by combining appearance information and [[ geometric constraints ]] .", "h": ["geometric constraints"], "t": ["probabilistic framework"]}, {"label": "USED-FOR", "tokens": "A [[ generative framework ]] is used for learning a << model >> that captures the relative position of parts within each of the discretized viewpoints .", "h": ["generative framework"], "t": ["model"]}, {"label": "COMPARE", "tokens": "Contrary to most of the existing << mixture of viewpoints models >> , our [[ model ]] establishes explicit correspondences of parts across different viewpoints of the object class .", "h": ["model"], "t": ["mixture of viewpoints models"]}, {"label": "USED-FOR", "tokens": "Given a new [[ image ]] , << detection >> and classification are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects .", "h": ["image"], "t": ["detection"]}, {"label": "USED-FOR", "tokens": "Given a new [[ image ]] , detection and << classification >> are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects .", "h": ["image"], "t": ["classification"]}, {"label": "CONJUNCTION", "tokens": "Given a new image , [[ detection ]] and << classification >> are achieved by determining the position and viewpoint of the model that maximize recognition scores of the candidate objects .", "h": ["detection"], "t": ["classification"]}, {"label": "USED-FOR", "tokens": "Given a new image , << detection >> and classification are achieved by determining the [[ position ]] and viewpoint of the model that maximize recognition scores of the candidate objects .", "h": ["position"], "t": ["detection"]}, {"label": "USED-FOR", "tokens": "Given a new image , detection and << classification >> are achieved by determining the [[ position ]] and viewpoint of the model that maximize recognition scores of the candidate objects .", "h": ["position"], "t": ["classification"]}, {"label": "CONJUNCTION", "tokens": "Given a new image , detection and classification are achieved by determining the [[ position ]] and << viewpoint >> of the model that maximize recognition scores of the candidate objects .", "h": ["position"], "t": ["viewpoint"]}, {"label": "USED-FOR", "tokens": "Given a new image , << detection >> and classification are achieved by determining the position and [[ viewpoint ]] of the model that maximize recognition scores of the candidate objects .", "h": ["viewpoint"], "t": ["detection"]}, {"label": "USED-FOR", "tokens": "Given a new image , detection and << classification >> are achieved by determining the position and [[ viewpoint ]] of the model that maximize recognition scores of the candidate objects .", "h": ["viewpoint"], "t": ["classification"]}, {"label": "USED-FOR", "tokens": "Our approach is among the first to propose a [[ generative proba-bilistic framework ]] for << 3D object categorization >> .", "h": ["generative proba-bilistic framework"], "t": ["3D object categorization"]}, {"label": "USED-FOR", "tokens": "We test our [[ algorithm ]] on the << detection task >> and the viewpoint classification task by using '' car '' category from both the Savarese et al. 2007 and PASCAL VOC 2006 datasets .", "h": ["algorithm"], "t": ["detection task"]}, {"label": "USED-FOR", "tokens": "We test our [[ algorithm ]] on the detection task and the << viewpoint classification task >> by using '' car '' category from both the Savarese et al. 2007 and PASCAL VOC 2006 datasets .", "h": ["algorithm"], "t": ["viewpoint classification task"]}, {"label": "CONJUNCTION", "tokens": "We test our algorithm on the [[ detection task ]] and the << viewpoint classification task >> by using '' car '' category from both the Savarese et al. 2007 and PASCAL VOC 2006 datasets .", "h": ["detection task"], "t": ["viewpoint classification task"]}, {"label": "EVALUATE-FOR", "tokens": "We test our << algorithm >> on the detection task and the viewpoint classification task by using '' car '' category from both the Savarese et al. 2007 and [[ PASCAL VOC 2006 datasets ]] .", "h": ["PASCAL VOC 2006 datasets"], "t": ["algorithm"]}, {"label": "EVALUATE-FOR", "tokens": "We show promising results in both the << detection and viewpoint classification tasks >> on these two challenging [[ datasets ]] .", "h": ["datasets"], "t": ["detection and viewpoint classification tasks"]}, {"label": "USED-FOR", "tokens": "We present an application of [[ ambiguity packing and stochastic disambiguation techniques ]] for << Lexical-Functional Grammars -LRB- LFG -RRB- >> to the domain of sentence condensation .", "h": ["ambiguity packing and stochastic disambiguation techniques"], "t": ["Lexical-Functional Grammars -LRB- LFG -RRB-"]}, {"label": "USED-FOR", "tokens": "We present an application of [[ ambiguity packing and stochastic disambiguation techniques ]] for Lexical-Functional Grammars -LRB- LFG -RRB- to the domain of << sentence condensation >> .", "h": ["ambiguity packing and stochastic disambiguation techniques"], "t": ["sentence condensation"]}, {"label": "PART-OF", "tokens": "Our << system >> incorporates a [[ linguistic parser/generator ]] for LFG , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .", "h": ["linguistic parser/generator"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "Our system incorporates a [[ linguistic parser/generator ]] for << LFG >> , a transfer component for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .", "h": ["linguistic parser/generator"], "t": ["LFG"]}, {"label": "CONJUNCTION", "tokens": "Our system incorporates a [[ linguistic parser/generator ]] for LFG , a << transfer component >> for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .", "h": ["linguistic parser/generator"], "t": ["transfer component"]}, {"label": "PART-OF", "tokens": "Our << system >> incorporates a linguistic parser/generator for LFG , a [[ transfer component ]] for parse reduction operating on packed parse forests , and a maximum-entropy model for stochastic output selection .", "h": ["transfer component"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "Our system incorporates a linguistic parser/generator for LFG , a [[ transfer component ]] for << parse reduction >> operating on packed parse forests , and a maximum-entropy model for stochastic output selection .", "h": ["transfer component"], "t": ["parse reduction"]}, {"label": "CONJUNCTION", "tokens": "Our system incorporates a linguistic parser/generator for LFG , a [[ transfer component ]] for parse reduction operating on packed parse forests , and a << maximum-entropy model >> for stochastic output selection .", "h": ["transfer component"], "t": ["maximum-entropy model"]}, {"label": "USED-FOR", "tokens": "Our system incorporates a linguistic parser/generator for LFG , a transfer component for << parse reduction >> operating on [[ packed parse forests ]] , and a maximum-entropy model for stochastic output selection .", "h": ["packed parse forests"], "t": ["parse reduction"]}, {"label": "PART-OF", "tokens": "Our << system >> incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a [[ maximum-entropy model ]] for stochastic output selection .", "h": ["maximum-entropy model"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "Our system incorporates a linguistic parser/generator for LFG , a transfer component for parse reduction operating on packed parse forests , and a [[ maximum-entropy model ]] for << stochastic output selection >> .", "h": ["maximum-entropy model"], "t": ["stochastic output selection"]}, {"label": "EVALUATE-FOR", "tokens": "Furthermore , we propose the use of standard [[ parser evaluation methods ]] for automatically evaluating the << summarization quality >> of sentence condensation systems .", "h": ["parser evaluation methods"], "t": ["summarization quality"]}, {"label": "EVALUATE-FOR", "tokens": "Furthermore , we propose the use of standard parser evaluation methods for automatically evaluating the [[ summarization quality ]] of << sentence condensation systems >> .", "h": ["summarization quality"], "t": ["sentence condensation systems"]}, {"label": "EVALUATE-FOR", "tokens": "An experimental evaluation of [[ summarization quality ]] shows a close correlation between the << automatic parse-based evaluation >> and a manual evaluation of generated strings .", "h": ["summarization quality"], "t": ["automatic parse-based evaluation"]}, {"label": "COMPARE", "tokens": "An experimental evaluation of summarization quality shows a close correlation between the [[ automatic parse-based evaluation ]] and a << manual evaluation >> of generated strings .", "h": ["automatic parse-based evaluation"], "t": ["manual evaluation"]}, {"label": "EVALUATE-FOR", "tokens": "Overall [[ summarization quality ]] of the proposed << system >> is state-of-the-art , with guaranteed grammaticality of the system output due to the use of a constraint-based parser/generator .", "h": ["summarization quality"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "Overall summarization quality of the proposed << system >> is state-of-the-art , with guaranteed [[ grammaticality ]] of the system output due to the use of a constraint-based parser/generator .", "h": ["grammaticality"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "Overall summarization quality of the proposed << system >> is state-of-the-art , with guaranteed grammaticality of the system output due to the use of a [[ constraint-based parser/generator ]] .", "h": ["constraint-based parser/generator"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "The [[ robust principal component analysis -LRB- robust PCA -RRB- problem ]] has been considered in many << machine learning applications >> , where the goal is to decompose the data matrix to a low rank part plus a sparse residual .", "h": ["robust principal component analysis -LRB- robust PCA -RRB- problem"], "t": ["machine learning applications"]}, {"label": "PART-OF", "tokens": "The robust principal component analysis -LRB- robust PCA -RRB- problem has been considered in many machine learning applications , where the goal is to decompose the << data matrix >> to a [[ low rank part ]] plus a sparse residual .", "h": ["low rank part"], "t": ["data matrix"]}, {"label": "CONJUNCTION", "tokens": "The robust principal component analysis -LRB- robust PCA -RRB- problem has been considered in many machine learning applications , where the goal is to decompose the data matrix to a [[ low rank part ]] plus a << sparse residual >> .", "h": ["low rank part"], "t": ["sparse residual"]}, {"label": "PART-OF", "tokens": "The robust principal component analysis -LRB- robust PCA -RRB- problem has been considered in many machine learning applications , where the goal is to decompose the << data matrix >> to a low rank part plus a [[ sparse residual ]] .", "h": ["sparse residual"], "t": ["data matrix"]}, {"label": "USED-FOR", "tokens": "While current << approaches >> are developed by only considering the [[ low rank plus sparse structure ]] , in many applications , side information of row and/or column entities may also be given , and it is still unclear to what extent could such information help robust PCA .", "h": ["low rank plus sparse structure"], "t": ["approaches"]}, {"label": "USED-FOR", "tokens": "While current approaches are developed by only considering the low rank plus sparse structure , in many applications , side information of row and/or column entities may also be given , and it is still unclear to what extent could such [[ information ]] help << robust PCA >> .", "h": ["information"], "t": ["robust PCA"]}, {"label": "USED-FOR", "tokens": "Thus , in this paper , we study the problem of << robust PCA >> with [[ side information ]] , where both prior structure and features of entities are exploited for recovery .", "h": ["side information"], "t": ["robust PCA"]}, {"label": "CONJUNCTION", "tokens": "Thus , in this paper , we study the problem of robust PCA with side information , where both [[ prior structure ]] and << features of entities >> are exploited for recovery .", "h": ["prior structure"], "t": ["features of entities"]}, {"label": "USED-FOR", "tokens": "Thus , in this paper , we study the problem of robust PCA with side information , where both [[ prior structure ]] and features of entities are exploited for << recovery >> .", "h": ["prior structure"], "t": ["recovery"]}, {"label": "USED-FOR", "tokens": "Thus , in this paper , we study the problem of robust PCA with side information , where both prior structure and [[ features of entities ]] are exploited for << recovery >> .", "h": ["features of entities"], "t": ["recovery"]}, {"label": "USED-FOR", "tokens": "We propose a [[ convex problem ]] to incorporate << side information >> in robust PCA and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions .", "h": ["convex problem"], "t": ["side information"]}, {"label": "PART-OF", "tokens": "We propose a convex problem to incorporate [[ side information ]] in << robust PCA >> and show that the low rank matrix can be exactly recovered via the proposed method under certain conditions .", "h": ["side information"], "t": ["robust PCA"]}, {"label": "USED-FOR", "tokens": "We propose a convex problem to incorporate side information in robust PCA and show that the << low rank matrix >> can be exactly recovered via the proposed [[ method ]] under certain conditions .", "h": ["method"], "t": ["low rank matrix"]}, {"label": "USED-FOR", "tokens": "In particular , our guarantee suggests that a substantial amount of << low rank matrices >> , which can not be recovered by standard robust PCA , become re-coverable by our proposed [[ method ]] .", "h": ["method"], "t": ["low rank matrices"]}, {"label": "FEATURE-OF", "tokens": "The result theoretically justifies the effectiveness of [[ features ]] in << robust PCA >> .", "h": ["features"], "t": ["robust PCA"]}, {"label": "EVALUATE-FOR", "tokens": "In addition , we conduct synthetic experiments as well as a real application on [[ noisy image classification ]] to show that our << method >> also improves the performance in practice by exploiting side information .", "h": ["noisy image classification"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "In addition , we conduct synthetic experiments as well as a real application on noisy image classification to show that our << method >> also improves the performance in practice by exploiting [[ side information ]] .", "h": ["side information"], "t": ["method"]}, {"label": "FEATURE-OF", "tokens": "This paper presents necessary and sufficient conditions for the use of [[ demonstrative expressions ]] in << English >> and discusses implications for current discourse processing algorithms .", "h": ["demonstrative expressions"], "t": ["English"]}, {"label": "USED-FOR", "tokens": "This paper presents necessary and sufficient conditions for the use of demonstrative expressions in English and discusses [[ implications ]] for current << discourse processing algorithms >> .", "h": ["implications"], "t": ["discourse processing algorithms"]}, {"label": "USED-FOR", "tokens": "This research is part of a larger study of [[ anaphoric expressions ]] , the results of which will be incorporated into a << natural language generation system >> .", "h": ["anaphoric expressions"], "t": ["natural language generation system"]}, {"label": "USED-FOR", "tokens": "Using the [[ IEMOCAP database ]] , << discrete -LRB- categorical -RRB- and continuous -LRB- attribute -RRB- emotional assessments >> evaluated by the actors and na \u00a8 \u0131ve listeners are compared .", "h": ["IEMOCAP database"], "t": ["discrete -LRB- categorical -RRB- and continuous -LRB- attribute -RRB- emotional assessments"]}, {"label": "USED-FOR", "tokens": "The problem of << blind separation of underdetermined instantaneous mixtures of independent signals >> is addressed through a [[ method ]] relying on nonstationarity of the original signals .", "h": ["method"], "t": ["blind separation of underdetermined instantaneous mixtures of independent signals"]}, {"label": "USED-FOR", "tokens": "The problem of blind separation of underdetermined instantaneous mixtures of independent signals is addressed through a << method >> relying on [[ nonstationarity ]] of the original signals .", "h": ["nonstationarity"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "In comparison with previous works , in this paper it is assumed that the << signals >> are not i.i.d. in each epoch , but obey a [[ first-order autoregressive model ]] .", "h": ["first-order autoregressive model"], "t": ["signals"]}, {"label": "USED-FOR", "tokens": "This [[ model ]] was shown to be more appropriate for << blind separation of natural speech signals . >>", "h": ["model"], "t": ["blind separation of natural speech signals ."]}, {"label": "FEATURE-OF", "tokens": "A << separation method >> is proposed that is nearly statistically efficient -LRB- approaching the corresponding [[ Cram\u00e9r-Rao lower bound -RRB- ]] , if the separated signals obey the assumed model .", "h": ["Cram\u00e9r-Rao lower bound -RRB-"], "t": ["separation method"]}, {"label": "USED-FOR", "tokens": "In the case of << natural speech signals >> , the [[ method ]] is shown to have separation accuracy better than the state-of-the-art methods .", "h": ["method"], "t": ["natural speech signals"]}, {"label": "COMPARE", "tokens": "In the case of natural speech signals , the [[ method ]] is shown to have separation accuracy better than the state-of-the-art << methods >> .", "h": ["method"], "t": ["methods"]}, {"label": "EVALUATE-FOR", "tokens": "In the case of natural speech signals , the << method >> is shown to have [[ separation accuracy ]] better than the state-of-the-art methods .", "h": ["separation accuracy"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "In the case of natural speech signals , the method is shown to have [[ separation accuracy ]] better than the state-of-the-art << methods >> .", "h": ["separation accuracy"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "In the case of << natural speech signals >> , the method is shown to have separation accuracy better than the state-of-the-art [[ methods ]] .", "h": ["methods"], "t": ["natural speech signals"]}, {"label": "USED-FOR", "tokens": "This paper proposes to use a [[ convolution kernel over parse trees ]] to model << syntactic structure information >> for relation extraction .", "h": ["convolution kernel over parse trees"], "t": ["syntactic structure information"]}, {"label": "USED-FOR", "tokens": "This paper proposes to use a convolution kernel over parse trees to model [[ syntactic structure information ]] for << relation extraction >> .", "h": ["syntactic structure information"], "t": ["relation extraction"]}, {"label": "FEATURE-OF", "tokens": "Our study reveals that the [[ syntactic structure features ]] embedded in a << parse tree >> are very effective for relation extraction and these features can be well captured by the convolution tree kernel .", "h": ["syntactic structure features"], "t": ["parse tree"]}, {"label": "USED-FOR", "tokens": "Our study reveals that the [[ syntactic structure features ]] embedded in a parse tree are very effective for << relation extraction >> and these features can be well captured by the convolution tree kernel .", "h": ["syntactic structure features"], "t": ["relation extraction"]}, {"label": "USED-FOR", "tokens": "Our study reveals that the syntactic structure features embedded in a parse tree are very effective for relation extraction and these << features >> can be well captured by the [[ convolution tree kernel ]] .", "h": ["convolution tree kernel"], "t": ["features"]}, {"label": "EVALUATE-FOR", "tokens": "Evaluation on the [[ ACE 2003 corpus ]] shows that the << convolution kernel over parse trees >> can achieve comparable performance with the previous best-reported feature-based methods on the 24 ACE relation subtypes .", "h": ["ACE 2003 corpus"], "t": ["convolution kernel over parse trees"]}, {"label": "COMPARE", "tokens": "Evaluation on the ACE 2003 corpus shows that the << convolution kernel over parse trees >> can achieve comparable performance with the previous best-reported [[ feature-based methods ]] on the 24 ACE relation subtypes .", "h": ["feature-based methods"], "t": ["convolution kernel over parse trees"]}, {"label": "COMPARE", "tokens": "It also shows that our [[ method ]] significantly outperforms the previous two << dependency tree kernels >> on the 5 ACE relation major types .", "h": ["method"], "t": ["dependency tree kernels"]}, {"label": "PART-OF", "tokens": "This paper presents the results of automatically inducing a [[ Combinatory Categorial Grammar -LRB- CCG -RRB- lexicon ]] from a << Turkish dependency treebank >> .", "h": ["Combinatory Categorial Grammar -LRB- CCG -RRB- lexicon"], "t": ["Turkish dependency treebank"]}, {"label": "HYPONYM-OF", "tokens": "The fact that [[ Turkish ]] is an << agglutinating free word order language >> presents a challenge for language theories .", "h": ["Turkish"], "t": ["agglutinating free word order language"]}, {"label": "PART-OF", "tokens": "We explored possible ways to obtain a [[ compact lexicon ]] , consistent with CCG principles , from a << treebank >> which is an order of magnitude smaller than Penn WSJ .", "h": ["compact lexicon"], "t": ["treebank"]}, {"label": "COMPARE", "tokens": "We explored possible ways to obtain a compact lexicon , consistent with CCG principles , from a [[ treebank ]] which is an order of magnitude smaller than << Penn WSJ >> .", "h": ["treebank"], "t": ["Penn WSJ"]}, {"label": "USED-FOR", "tokens": "While [[ sentence extraction ]] as an approach to << summarization >> has been shown to work in documents of certain genres , because of the conversational nature of email communication where utterances are made in relation to one made previously , sentence extraction may not capture the necessary segments of dialogue that would make a summary coherent .", "h": ["sentence extraction"], "t": ["summarization"]}, {"label": "USED-FOR", "tokens": "In this paper , we present our work on the [[ detection of question-answer pairs ]] in an email conversation for the task of << email summarization >> .", "h": ["detection of question-answer pairs"], "t": ["email summarization"]}, {"label": "USED-FOR", "tokens": "In this paper , we present our work on the << detection of question-answer pairs >> in an [[ email conversation ]] for the task of email summarization .", "h": ["email conversation"], "t": ["detection of question-answer pairs"]}, {"label": "USED-FOR", "tokens": "We show that various [[ features ]] based on the structure of email-threads can be used to improve upon << lexical similarity >> of discourse segments for question-answer pairing .", "h": ["features"], "t": ["lexical similarity"]}, {"label": "USED-FOR", "tokens": "We show that various [[ features ]] based on the structure of email-threads can be used to improve upon lexical similarity of discourse segments for << question-answer pairing >> .", "h": ["features"], "t": ["question-answer pairing"]}, {"label": "USED-FOR", "tokens": "We show that various << features >> based on the [[ structure of email-threads ]] can be used to improve upon lexical similarity of discourse segments for question-answer pairing .", "h": ["structure of email-threads"], "t": ["features"]}, {"label": "FEATURE-OF", "tokens": "We show that various features based on the structure of email-threads can be used to improve upon [[ lexical similarity ]] of << discourse segments >> for question-answer pairing .", "h": ["lexical similarity"], "t": ["discourse segments"]}, {"label": "USED-FOR", "tokens": "Specifically , we show how to incorporate a simple [[ prior on the distribution of natural images ]] into << support vector machines >> .", "h": ["prior on the distribution of natural images"], "t": ["support vector machines"]}, {"label": "USED-FOR", "tokens": "[[ SVMs ]] are known to be robust to << overfitting >> ; however , a few training examples usually do not represent well the structure of the class .", "h": ["SVMs"], "t": ["overfitting"]}, {"label": "EVALUATE-FOR", "tokens": "Our experiments on [[ real data sets ]] show that the resulting << detector >> is more robust to the choice of training examples , and substantially improves both linear and kernel SVM when trained on 10 positive and 10 negative examples .", "h": ["real data sets"], "t": ["detector"]}, {"label": "COMPARE", "tokens": "Our experiments on real data sets show that the resulting [[ detector ]] is more robust to the choice of training examples , and substantially improves both << linear and kernel SVM >> when trained on 10 positive and 10 negative examples .", "h": ["detector"], "t": ["linear and kernel SVM"]}, {"label": "USED-FOR", "tokens": "Although the study of clustering is centered around an intuitively compelling goal , it has been very difficult to develop a [[ unified framework ]] for << reasoning >> about it at a technical level , and profoundly diverse approaches to clustering abound in the research community .", "h": ["unified framework"], "t": ["reasoning"]}, {"label": "HYPONYM-OF", "tokens": "Relaxations of these properties expose some of the interesting -LRB- and unavoidable -RRB- trade-offs at work in << well-studied clustering techniques >> such as [[ single-linkage ]] , sum-of-pairs , k-means , and k-median .", "h": ["single-linkage"], "t": ["well-studied clustering techniques"]}, {"label": "CONJUNCTION", "tokens": "Relaxations of these properties expose some of the interesting -LRB- and unavoidable -RRB- trade-offs at work in well-studied clustering techniques such as [[ single-linkage ]] , << sum-of-pairs >> , k-means , and k-median .", "h": ["single-linkage"], "t": ["sum-of-pairs"]}, {"label": "HYPONYM-OF", "tokens": "Relaxations of these properties expose some of the interesting -LRB- and unavoidable -RRB- trade-offs at work in << well-studied clustering techniques >> such as single-linkage , [[ sum-of-pairs ]] , k-means , and k-median .", "h": ["sum-of-pairs"], "t": ["well-studied clustering techniques"]}, {"label": "CONJUNCTION", "tokens": "Relaxations of these properties expose some of the interesting -LRB- and unavoidable -RRB- trade-offs at work in well-studied clustering techniques such as single-linkage , [[ sum-of-pairs ]] , << k-means >> , and k-median .", "h": ["sum-of-pairs"], "t": ["k-means"]}, {"label": "HYPONYM-OF", "tokens": "Relaxations of these properties expose some of the interesting -LRB- and unavoidable -RRB- trade-offs at work in << well-studied clustering techniques >> such as single-linkage , sum-of-pairs , [[ k-means ]] , and k-median .", "h": ["k-means"], "t": ["well-studied clustering techniques"]}, {"label": "CONJUNCTION", "tokens": "Relaxations of these properties expose some of the interesting -LRB- and unavoidable -RRB- trade-offs at work in well-studied clustering techniques such as single-linkage , sum-of-pairs , [[ k-means ]] , and << k-median >> .", "h": ["k-means"], "t": ["k-median"]}, {"label": "HYPONYM-OF", "tokens": "Relaxations of these properties expose some of the interesting -LRB- and unavoidable -RRB- trade-offs at work in << well-studied clustering techniques >> such as single-linkage , sum-of-pairs , k-means , and [[ k-median ]] .", "h": ["k-median"], "t": ["well-studied clustering techniques"]}, {"label": "USED-FOR", "tokens": "With << relevant approach >> , we identify important contents by [[ PageRank algorithm ]] on the event map constructed from documents .", "h": ["PageRank algorithm"], "t": ["relevant approach"]}, {"label": "USED-FOR", "tokens": "With relevant approach , we identify important contents by << PageRank algorithm >> on the [[ event map ]] constructed from documents .", "h": ["event map"], "t": ["PageRank algorithm"]}, {"label": "USED-FOR", "tokens": "With relevant approach , we identify important contents by PageRank algorithm on the << event map >> constructed from [[ documents ]] .", "h": ["documents"], "t": ["event map"]}, {"label": "USED-FOR", "tokens": "We present a [[ scanning method ]] that recovers << dense sub-pixel camera-projector correspondence >> without requiring any photometric calibration nor preliminary knowledge of their relative geometry .", "h": ["scanning method"], "t": ["dense sub-pixel camera-projector correspondence"]}, {"label": "USED-FOR", "tokens": "<< Subpixel accuracy >> is achieved by considering several [[ zero-crossings ]] defined by the difference between pairs of unstructured patterns .", "h": ["zero-crossings"], "t": ["Subpixel accuracy"]}, {"label": "EVALUATE-FOR", "tokens": "We use << gray-level band-pass white noise patterns >> that increase [[ robustness ]] to indirect lighting and scene discontinuities .", "h": ["robustness"], "t": ["gray-level band-pass white noise patterns"]}, {"label": "FEATURE-OF", "tokens": "We use gray-level band-pass white noise patterns that increase << robustness >> to [[ indirect lighting ]] and scene discontinuities .", "h": ["indirect lighting"], "t": ["robustness"]}, {"label": "CONJUNCTION", "tokens": "We use gray-level band-pass white noise patterns that increase robustness to [[ indirect lighting ]] and << scene discontinuities >> .", "h": ["indirect lighting"], "t": ["scene discontinuities"]}, {"label": "FEATURE-OF", "tokens": "We use gray-level band-pass white noise patterns that increase << robustness >> to indirect lighting and [[ scene discontinuities ]] .", "h": ["scene discontinuities"], "t": ["robustness"]}, {"label": "USED-FOR", "tokens": "Simulated and experimental results show that our [[ method ]] recovers << scene geometry >> with high subpixel precision , and that it can handle many challenges of active reconstruction systems .", "h": ["method"], "t": ["scene geometry"]}, {"label": "FEATURE-OF", "tokens": "Simulated and experimental results show that our method recovers << scene geometry >> with high [[ subpixel precision ]] , and that it can handle many challenges of active reconstruction systems .", "h": ["subpixel precision"], "t": ["scene geometry"]}, {"label": "USED-FOR", "tokens": "Simulated and experimental results show that our method recovers scene geometry with high subpixel precision , and that [[ it ]] can handle many challenges of << active reconstruction systems >> .", "h": ["it"], "t": ["active reconstruction systems"]}, {"label": "HYPONYM-OF", "tokens": "We compare our results to << state of the art methods >> such as [[ mi-cro phase shifting ]] and modulated phase shifting .", "h": ["mi-cro phase shifting"], "t": ["state of the art methods"]}, {"label": "CONJUNCTION", "tokens": "We compare our results to state of the art methods such as [[ mi-cro phase shifting ]] and << modulated phase shifting >> .", "h": ["mi-cro phase shifting"], "t": ["modulated phase shifting"]}, {"label": "HYPONYM-OF", "tokens": "We compare our results to << state of the art methods >> such as mi-cro phase shifting and [[ modulated phase shifting ]] .", "h": ["modulated phase shifting"], "t": ["state of the art methods"]}, {"label": "USED-FOR", "tokens": "This paper describes a novel [[ system ]] for << acquiring adjectival subcategorization frames -LRB- scfs -RRB- >> and associated frequency information from English corpus data .", "h": ["system"], "t": ["acquiring adjectival subcategorization frames -LRB- scfs -RRB-"]}, {"label": "PART-OF", "tokens": "The << system >> incorporates a [[ decision-tree classifier ]] for 30 scf types which tests for the presence of grammatical relations -LRB- grs -RRB- in the output of a robust statistical parser .", "h": ["decision-tree classifier"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "The system incorporates a [[ decision-tree classifier ]] for 30 scf types which tests for the presence of << grammatical relations -LRB- grs -RRB- >> in the output of a robust statistical parser .", "h": ["decision-tree classifier"], "t": ["grammatical relations -LRB- grs -RRB-"]}, {"label": "USED-FOR", "tokens": "<< It >> uses a powerful [[ pattern-matching language ]] to classify grs into frames hierarchically in a way that mirrors inheritance-based lexica .", "h": ["pattern-matching language"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "It uses a powerful [[ pattern-matching language ]] to classify << grs >> into frames hierarchically in a way that mirrors inheritance-based lexica .", "h": ["pattern-matching language"], "t": ["grs"]}, {"label": "EVALUATE-FOR", "tokens": "The experiments show that the << system >> is able to detect scf types with 70 % [[ precision ]] and 66 % recall rate .", "h": ["precision"], "t": ["system"]}, {"label": "CONJUNCTION", "tokens": "The experiments show that the system is able to detect scf types with 70 % [[ precision ]] and 66 % << recall >> rate .", "h": ["precision"], "t": ["recall"]}, {"label": "EVALUATE-FOR", "tokens": "The experiments show that the << system >> is able to detect scf types with 70 % precision and 66 % [[ recall ]] rate .", "h": ["recall"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "A new [[ tool ]] for << linguistic annotation of scfs >> in corpus data is also introduced which can considerably alleviate the process of obtaining training and test data for subcategorization acquisition .", "h": ["tool"], "t": ["linguistic annotation of scfs"]}, {"label": "USED-FOR", "tokens": "A new tool for linguistic annotation of scfs in corpus data is also introduced which can considerably alleviate the process of obtaining [[ training and test data ]] for << subcategorization acquisition >> .", "h": ["training and test data"], "t": ["subcategorization acquisition"]}, {"label": "USED-FOR", "tokens": "[[ Machine transliteration/back-transliteration ]] plays an important role in many << multilingual speech and language applications >> .", "h": ["Machine transliteration/back-transliteration"], "t": ["multilingual speech and language applications"]}, {"label": "USED-FOR", "tokens": "In this paper , a novel [[ framework ]] for << machine transliteration/backtransliteration >> that allows us to carry out direct orthographical mapping -LRB- DOM -RRB- between two different languages is presented .", "h": ["framework"], "t": ["machine transliteration/backtransliteration"]}, {"label": "USED-FOR", "tokens": "In this paper , a novel framework for [[ machine transliteration/backtransliteration ]] that allows us to carry out << direct orthographical mapping -LRB- DOM -RRB- >> between two different languages is presented .", "h": ["machine transliteration/backtransliteration"], "t": ["direct orthographical mapping -LRB- DOM -RRB-"]}, {"label": "USED-FOR", "tokens": "Under this [[ framework ]] , a << joint source-channel transliteration model >> , also called n-gram transliteration model -LRB- n-gram TM -RRB- , is further proposed to model the transliteration process .", "h": ["framework"], "t": ["joint source-channel transliteration model"]}, {"label": "USED-FOR", "tokens": "Under this framework , a joint source-channel transliteration model , also called [[ n-gram transliteration model -LRB- n-gram TM -RRB- ]] , is further proposed to model the << transliteration process >> .", "h": ["n-gram transliteration model -LRB- n-gram TM -RRB-"], "t": ["transliteration process"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate the proposed << methods >> through several [[ transliteration/backtransliteration ]] experiments for English/Chinese and English/Japanese language pairs .", "h": ["transliteration/backtransliteration"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "We evaluate the proposed methods through several [[ transliteration/backtransliteration ]] experiments for << English/Chinese and English/Japanese language pairs >> .", "h": ["transliteration/backtransliteration"], "t": ["English/Chinese and English/Japanese language pairs"]}, {"label": "EVALUATE-FOR", "tokens": "Our study reveals that the proposed << method >> not only reduces an extensive system development effort but also improves the [[ transliteration accuracy ]] significantly .", "h": ["transliteration accuracy"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "A [[ bio-inspired model ]] for an << analog programmable array processor -LRB- APAP -RRB- >> , based on studies on the vertebrate retina , has permitted the realization of complex programmable spatio-temporal dynamics in VLSI .", "h": ["bio-inspired model"], "t": ["analog programmable array processor -LRB- APAP -RRB-"]}, {"label": "USED-FOR", "tokens": "A [[ bio-inspired model ]] for an analog programmable array processor -LRB- APAP -RRB- , based on studies on the vertebrate retina , has permitted the realization of << complex programmable spatio-temporal dynamics >> in VLSI .", "h": ["bio-inspired model"], "t": ["complex programmable spatio-temporal dynamics"]}, {"label": "USED-FOR", "tokens": "A << bio-inspired model >> for an analog programmable array processor -LRB- APAP -RRB- , based on studies on the [[ vertebrate retina ]] , has permitted the realization of complex programmable spatio-temporal dynamics in VLSI .", "h": ["vertebrate retina"], "t": ["bio-inspired model"]}, {"label": "FEATURE-OF", "tokens": "A bio-inspired model for an analog programmable array processor -LRB- APAP -RRB- , based on studies on the vertebrate retina , has permitted the realization of [[ complex programmable spatio-temporal dynamics ]] in << VLSI >> .", "h": ["complex programmable spatio-temporal dynamics"], "t": ["VLSI"]}, {"label": "USED-FOR", "tokens": "This model mimics the way in which << images >> are processed in the [[ visual pathway ]] , rendering a feasible alternative for the implementation of early vision applications in standard technologies .", "h": ["visual pathway"], "t": ["images"]}, {"label": "CONJUNCTION", "tokens": "[[ Computing power per area ]] and << power consumption >> is amongst the highest reported for a single chip .", "h": ["Computing power per area"], "t": ["power consumption"]}, {"label": "FEATURE-OF", "tokens": "Another problem with << determiners >> is their inherent [[ ambiguity ]] .", "h": ["ambiguity"], "t": ["determiners"]}, {"label": "USED-FOR", "tokens": "In this paper we propose a [[ logical formalism ]] , which , among other things , is suitable for representing << determiners >> without forcing a particular interpretation when their meaning is still not clear .", "h": ["logical formalism"], "t": ["determiners"]}, {"label": "USED-FOR", "tokens": "We investigate the [[ verbal and nonverbal means ]] for << grounding >> , and propose a design for embodied conversational agents that relies on both kinds of signals to establish common ground in human-computer interaction .", "h": ["verbal and nonverbal means"], "t": ["grounding"]}, {"label": "USED-FOR", "tokens": "We investigate the verbal and nonverbal means for grounding , and propose a [[ design ]] for << embodied conversational agents >> that relies on both kinds of signals to establish common ground in human-computer interaction .", "h": ["design"], "t": ["embodied conversational agents"]}, {"label": "USED-FOR", "tokens": "We investigate the verbal and nonverbal means for grounding , and propose a design for embodied conversational agents that relies on both kinds of signals to establish [[ common ground ]] in << human-computer interaction >> .", "h": ["common ground"], "t": ["human-computer interaction"]}, {"label": "CONJUNCTION", "tokens": "We analyzed [[ eye gaze ]] , << head nods >> and attentional focus in the context of a direction-giving task .", "h": ["eye gaze"], "t": ["head nods"]}, {"label": "PART-OF", "tokens": "We analyzed [[ eye gaze ]] , head nods and attentional focus in the context of a << direction-giving task >> .", "h": ["eye gaze"], "t": ["direction-giving task"]}, {"label": "CONJUNCTION", "tokens": "We analyzed eye gaze , [[ head nods ]] and << attentional focus >> in the context of a direction-giving task .", "h": ["head nods"], "t": ["attentional focus"]}, {"label": "PART-OF", "tokens": "We analyzed eye gaze , [[ head nods ]] and attentional focus in the context of a << direction-giving task >> .", "h": ["head nods"], "t": ["direction-giving task"]}, {"label": "PART-OF", "tokens": "We analyzed eye gaze , head nods and [[ attentional focus ]] in the context of a << direction-giving task >> .", "h": ["attentional focus"], "t": ["direction-giving task"]}, {"label": "USED-FOR", "tokens": "Based on these results , we present an << ECA >> that uses [[ verbal and nonverbal grounding acts ]] to update dialogue state .", "h": ["verbal and nonverbal grounding acts"], "t": ["ECA"]}, {"label": "USED-FOR", "tokens": "Based on these results , we present an ECA that uses [[ verbal and nonverbal grounding acts ]] to update << dialogue state >> .", "h": ["verbal and nonverbal grounding acts"], "t": ["dialogue state"]}, {"label": "USED-FOR", "tokens": "[[ Sentence boundary detection ]] in speech is important for enriching << speech recognition output >> , making it easier for humans to read and downstream modules to process .", "h": ["Sentence boundary detection"], "t": ["speech recognition output"]}, {"label": "USED-FOR", "tokens": "<< Sentence boundary detection >> in [[ speech ]] is important for enriching speech recognition output , making it easier for humans to read and downstream modules to process .", "h": ["speech"], "t": ["Sentence boundary detection"]}, {"label": "USED-FOR", "tokens": "In previous work , we have developed [[ hidden Markov model -LRB- HMM -RRB- and maximum entropy -LRB- Maxent -RRB- classifiers ]] that integrate textual and prosodic knowledge sources for << detecting sentence boundaries >> .", "h": ["hidden Markov model -LRB- HMM -RRB- and maximum entropy -LRB- Maxent -RRB- classifiers"], "t": ["detecting sentence boundaries"]}, {"label": "USED-FOR", "tokens": "In previous work , we have developed << hidden Markov model -LRB- HMM -RRB- and maximum entropy -LRB- Maxent -RRB- classifiers >> that integrate [[ textual and prosodic knowledge sources ]] for detecting sentence boundaries .", "h": ["textual and prosodic knowledge sources"], "t": ["hidden Markov model -LRB- HMM -RRB- and maximum entropy -LRB- Maxent -RRB- classifiers"]}, {"label": "USED-FOR", "tokens": "In this paper , we evaluate the use of a [[ conditional random field -LRB- CRF -RRB- ]] for this << task >> and relate results with this model to our prior work .", "h": ["conditional random field -LRB- CRF -RRB-"], "t": ["task"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate across two [[ corpora ]] -LRB- conversational telephone speech and broadcast news speech -RRB- on both << human transcriptions >> and speech recognition output .", "h": ["corpora"], "t": ["human transcriptions"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate across two [[ corpora ]] -LRB- conversational telephone speech and broadcast news speech -RRB- on both human transcriptions and << speech recognition output >> .", "h": ["corpora"], "t": ["speech recognition output"]}, {"label": "HYPONYM-OF", "tokens": "We evaluate across two << corpora >> -LRB- [[ conversational telephone speech ]] and broadcast news speech -RRB- on both human transcriptions and speech recognition output .", "h": ["conversational telephone speech"], "t": ["corpora"]}, {"label": "CONJUNCTION", "tokens": "We evaluate across two corpora -LRB- [[ conversational telephone speech ]] and << broadcast news speech >> -RRB- on both human transcriptions and speech recognition output .", "h": ["conversational telephone speech"], "t": ["broadcast news speech"]}, {"label": "HYPONYM-OF", "tokens": "We evaluate across two << corpora >> -LRB- conversational telephone speech and [[ broadcast news speech ]] -RRB- on both human transcriptions and speech recognition output .", "h": ["broadcast news speech"], "t": ["corpora"]}, {"label": "CONJUNCTION", "tokens": "We evaluate across two corpora -LRB- conversational telephone speech and broadcast news speech -RRB- on both [[ human transcriptions ]] and << speech recognition output >> .", "h": ["human transcriptions"], "t": ["speech recognition output"]}, {"label": "COMPARE", "tokens": "In general , our [[ CRF model ]] yields a lower error rate than the << HMM and Max-ent models >> on the NIST sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .", "h": ["CRF model"], "t": ["HMM and Max-ent models"]}, {"label": "EVALUATE-FOR", "tokens": "In general , our << CRF model >> yields a lower [[ error rate ]] than the HMM and Max-ent models on the NIST sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .", "h": ["error rate"], "t": ["CRF model"]}, {"label": "EVALUATE-FOR", "tokens": "In general , our CRF model yields a lower [[ error rate ]] than the << HMM and Max-ent models >> on the NIST sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .", "h": ["error rate"], "t": ["HMM and Max-ent models"]}, {"label": "EVALUATE-FOR", "tokens": "In general , our << CRF model >> yields a lower error rate than the HMM and Max-ent models on the [[ NIST sentence boundary detection task ]] in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .", "h": ["NIST sentence boundary detection task"], "t": ["CRF model"]}, {"label": "EVALUATE-FOR", "tokens": "In general , our CRF model yields a lower error rate than the << HMM and Max-ent models >> on the [[ NIST sentence boundary detection task ]] in speech , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .", "h": ["NIST sentence boundary detection task"], "t": ["HMM and Max-ent models"]}, {"label": "FEATURE-OF", "tokens": "In general , our CRF model yields a lower error rate than the HMM and Max-ent models on the << NIST sentence boundary detection task >> in [[ speech ]] , although it is interesting to note that the best results are achieved by three-way voting among the classifiers .", "h": ["speech"], "t": ["NIST sentence boundary detection task"]}, {"label": "USED-FOR", "tokens": "In general , our CRF model yields a lower error rate than the HMM and Max-ent models on the NIST sentence boundary detection task in speech , although it is interesting to note that the best results are achieved by << three-way voting >> among the [[ classifiers ]] .", "h": ["classifiers"], "t": ["three-way voting"]}, {"label": "USED-FOR", "tokens": "This probably occurs because each [[ model ]] has different strengths and weaknesses for modeling the << knowledge sources >> .", "h": ["model"], "t": ["knowledge sources"]}, {"label": "USED-FOR", "tokens": "We propose a novel [[ approach ]] to associate objects across multiple PTZ cameras that can be used to perform << camera handoff in wide-area surveillance scenarios >> .", "h": ["approach"], "t": ["camera handoff in wide-area surveillance scenarios"]}, {"label": "USED-FOR", "tokens": "While previous << approaches >> relied on [[ geometric , appearance , or correlation-based information ]] for establishing correspondences between static cameras , they each have well-known limitations and are not extendable to wide-area settings with PTZ cameras .", "h": ["geometric , appearance , or correlation-based information"], "t": ["approaches"]}, {"label": "USED-FOR", "tokens": "Towards this goal , we also propose a novel << Multiple Instance Learning -LRB- MIL -RRB- formulation >> for the problem based on the [[ logistic softmax function of covariance-based region features ]] within a MAP estimation framework .", "h": ["logistic softmax function of covariance-based region features"], "t": ["Multiple Instance Learning -LRB- MIL -RRB- formulation"]}, {"label": "USED-FOR", "tokens": "Towards this goal , we also propose a novel << Multiple Instance Learning -LRB- MIL -RRB- formulation >> for the problem based on the logistic softmax function of covariance-based region features within a [[ MAP estimation framework ]] .", "h": ["MAP estimation framework"], "t": ["Multiple Instance Learning -LRB- MIL -RRB- formulation"]}, {"label": "USED-FOR", "tokens": "We demonstrate our [[ approach ]] with multiple PTZ camera sequences in typical << outdoor surveillance settings >> and show a comparison with state-of-the-art approaches .", "h": ["approach"], "t": ["outdoor surveillance settings"]}, {"label": "COMPARE", "tokens": "We demonstrate our [[ approach ]] with multiple PTZ camera sequences in typical outdoor surveillance settings and show a comparison with << state-of-the-art approaches >> .", "h": ["approach"], "t": ["state-of-the-art approaches"]}, {"label": "USED-FOR", "tokens": "We demonstrate our << approach >> with [[ multiple PTZ camera sequences ]] in typical outdoor surveillance settings and show a comparison with state-of-the-art approaches .", "h": ["multiple PTZ camera sequences"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "This paper solves a [[ specialized regression problem ]] to obtain << sampling probabilities >> for records in databases .", "h": ["specialized regression problem"], "t": ["sampling probabilities"]}, {"label": "USED-FOR", "tokens": "This paper solves a specialized regression problem to obtain [[ sampling probabilities ]] for << records >> in databases .", "h": ["sampling probabilities"], "t": ["records"]}, {"label": "PART-OF", "tokens": "This paper solves a specialized regression problem to obtain sampling probabilities for [[ records ]] in << databases >> .", "h": ["records"], "t": ["databases"]}, {"label": "EVALUATE-FOR", "tokens": "The goal is to sample a small set of << records >> over which evaluating [[ aggregate queries ]] can be done both efficiently and accurately .", "h": ["aggregate queries"], "t": ["records"]}, {"label": "USED-FOR", "tokens": "We provide a [[ principled and provable solution ]] for this << problem >> ; it is parameterless and requires no data insights .", "h": ["principled and provable solution"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "Moreover , a << cost zero solution >> always exists and can only be excluded by [[ hard budget constraints ]] .", "h": ["hard budget constraints"], "t": ["cost zero solution"]}, {"label": "CONJUNCTION", "tokens": "Our extensive experimental results significantly improve over both [[ uniform sampling ]] and standard << stratified sampling >> which are de-facto the industry standards .", "h": ["uniform sampling"], "t": ["stratified sampling"]}, {"label": "COMPARE", "tokens": "We consider the problem of computing the Kullback-Leibler distance , also called the relative entropy , between a [[ probabilistic context-free grammar ]] and a << probabilistic finite automaton >> .", "h": ["probabilistic context-free grammar"], "t": ["probabilistic finite automaton"]}, {"label": "USED-FOR", "tokens": "We show that there is a [[ closed-form -LRB- analytical -RRB- solution ]] for one part of the << Kullback-Leibler distance >> , viz the cross-entropy .", "h": ["closed-form -LRB- analytical -RRB- solution"], "t": ["Kullback-Leibler distance"]}, {"label": "USED-FOR", "tokens": "We show that there is a [[ closed-form -LRB- analytical -RRB- solution ]] for one part of the Kullback-Leibler distance , viz the << cross-entropy >> .", "h": ["closed-form -LRB- analytical -RRB- solution"], "t": ["cross-entropy"]}, {"label": "PART-OF", "tokens": "We show that there is a closed-form -LRB- analytical -RRB- solution for one part of the << Kullback-Leibler distance >> , viz the [[ cross-entropy ]] .", "h": ["cross-entropy"], "t": ["Kullback-Leibler distance"]}, {"label": "FEATURE-OF", "tokens": "We discuss several applications of the result to the problem of [[ distributional approximation ]] of << probabilistic context-free grammars >> by means of probabilistic finite automata .", "h": ["distributional approximation"], "t": ["probabilistic context-free grammars"]}, {"label": "USED-FOR", "tokens": "We discuss several applications of the result to the problem of << distributional approximation >> of probabilistic context-free grammars by means of [[ probabilistic finite automata ]] .", "h": ["probabilistic finite automata"], "t": ["distributional approximation"]}, {"label": "CONJUNCTION", "tokens": "In spite of over two decades of intense research , [[ illumination ]] and << pose invariance >> remain prohibitively challenging aspects of face recognition for most practical applications .", "h": ["illumination"], "t": ["pose invariance"]}, {"label": "PART-OF", "tokens": "In spite of over two decades of intense research , [[ illumination ]] and pose invariance remain prohibitively challenging aspects of << face recognition >> for most practical applications .", "h": ["illumination"], "t": ["face recognition"]}, {"label": "PART-OF", "tokens": "In spite of over two decades of intense research , illumination and [[ pose invariance ]] remain prohibitively challenging aspects of << face recognition >> for most practical applications .", "h": ["pose invariance"], "t": ["face recognition"]}, {"label": "CONJUNCTION", "tokens": "The objective of this work is to recognize faces using video sequences both for training and recognition input , in a realistic , unconstrained setup in which [[ lighting ]] , << pose >> and user motion pattern have a wide variability and face images are of low resolution .", "h": ["lighting"], "t": ["pose"]}, {"label": "CONJUNCTION", "tokens": "The objective of this work is to recognize faces using video sequences both for training and recognition input , in a realistic , unconstrained setup in which lighting , [[ pose ]] and << user motion pattern >> have a wide variability and face images are of low resolution .", "h": ["pose"], "t": ["user motion pattern"]}, {"label": "FEATURE-OF", "tokens": "The objective of this work is to recognize faces using video sequences both for training and recognition input , in a realistic , unconstrained setup in which lighting , pose and user motion pattern have a wide variability and << face images >> are of low [[ resolution ]] .", "h": ["resolution"], "t": ["face images"]}, {"label": "USED-FOR", "tokens": "In particular there are three areas of novelty : -LRB- i -RRB- we show how a [[ photometric model ]] of << image formation >> can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .", "h": ["photometric model"], "t": ["image formation"]}, {"label": "CONJUNCTION", "tokens": "In particular there are three areas of novelty : -LRB- i -RRB- we show how a [[ photometric model ]] of image formation can be combined with a << statistical model >> of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .", "h": ["photometric model"], "t": ["statistical model"]}, {"label": "USED-FOR", "tokens": "In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a [[ statistical model ]] of << generic face appearance variation >> , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .", "h": ["statistical model"], "t": ["generic face appearance variation"]}, {"label": "USED-FOR", "tokens": "In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a [[ statistical model ]] of generic face appearance variation , learnt offline , to generalize in the presence of << extreme illumination changes >> ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .", "h": ["statistical model"], "t": ["extreme illumination changes"]}, {"label": "FEATURE-OF", "tokens": "In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the [[ smoothness ]] of << geodesically local appearance manifold structure >> and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .", "h": ["smoothness"], "t": ["geodesically local appearance manifold structure"]}, {"label": "CONJUNCTION", "tokens": "In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of [[ geodesically local appearance manifold structure ]] and a << robust same-identity likelihood >> to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to face motion patterns in video .", "h": ["geodesically local appearance manifold structure"], "t": ["robust same-identity likelihood"]}, {"label": "EVALUATE-FOR", "tokens": "In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate << video sequence '' reillumination '' algorithm >> to achieve [[ robustness ]] to face motion patterns in video .", "h": ["robustness"], "t": ["video sequence '' reillumination '' algorithm"]}, {"label": "FEATURE-OF", "tokens": "In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve << robustness >> to [[ face motion patterns ]] in video .", "h": ["face motion patterns"], "t": ["robustness"]}, {"label": "PART-OF", "tokens": "In particular there are three areas of novelty : -LRB- i -RRB- we show how a photometric model of image formation can be combined with a statistical model of generic face appearance variation , learnt offline , to generalize in the presence of extreme illumination changes ; -LRB- ii -RRB- we use the smoothness of geodesically local appearance manifold structure and a robust same-identity likelihood to achieve invariance to unseen head poses ; and -LRB- iii -RRB- we introduce an accurate video sequence '' reillumination '' algorithm to achieve robustness to [[ face motion patterns ]] in << video >> .", "h": ["face motion patterns"], "t": ["video"]}, {"label": "USED-FOR", "tokens": "We describe a << fully automatic recognition system >> based on the proposed [[ method ]] and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme illumination , pose and head motion variation .", "h": ["method"], "t": ["fully automatic recognition system"]}, {"label": "EVALUATE-FOR", "tokens": "We describe a << fully automatic recognition system >> based on the proposed method and an extensive evaluation on 171 individuals and over 1300 [[ video sequences ]] with extreme illumination , pose and head motion variation .", "h": ["video sequences"], "t": ["fully automatic recognition system"]}, {"label": "FEATURE-OF", "tokens": "We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 << video sequences >> with extreme [[ illumination ]] , pose and head motion variation .", "h": ["illumination"], "t": ["video sequences"]}, {"label": "CONJUNCTION", "tokens": "We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme [[ illumination ]] , << pose >> and head motion variation .", "h": ["illumination"], "t": ["pose"]}, {"label": "FEATURE-OF", "tokens": "We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 << video sequences >> with extreme illumination , [[ pose ]] and head motion variation .", "h": ["pose"], "t": ["video sequences"]}, {"label": "CONJUNCTION", "tokens": "We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 video sequences with extreme illumination , [[ pose ]] and << head motion variation >> .", "h": ["pose"], "t": ["head motion variation"]}, {"label": "FEATURE-OF", "tokens": "We describe a fully automatic recognition system based on the proposed method and an extensive evaluation on 171 individuals and over 1300 << video sequences >> with extreme illumination , pose and [[ head motion variation ]] .", "h": ["head motion variation"], "t": ["video sequences"]}, {"label": "EVALUATE-FOR", "tokens": "On this challenging [[ data set ]] our << system >> consistently demonstrated a nearly perfect recognition rate -LRB- over 99.7 % on all three databases -RRB- , significantly out-performing state-of-the-art commercial software and methods from the literature .", "h": ["data set"], "t": ["system"]}, {"label": "COMPARE", "tokens": "On this challenging data set our [[ system ]] consistently demonstrated a nearly perfect recognition rate -LRB- over 99.7 % on all three databases -RRB- , significantly out-performing state-of-the-art << commercial software >> and methods from the literature .", "h": ["system"], "t": ["commercial software"]}, {"label": "COMPARE", "tokens": "On this challenging data set our [[ system ]] consistently demonstrated a nearly perfect recognition rate -LRB- over 99.7 % on all three databases -RRB- , significantly out-performing state-of-the-art commercial software and << methods >> from the literature .", "h": ["system"], "t": ["methods"]}, {"label": "EVALUATE-FOR", "tokens": "On this challenging data set our << system >> consistently demonstrated a nearly perfect [[ recognition rate ]] -LRB- over 99.7 % on all three databases -RRB- , significantly out-performing state-of-the-art commercial software and methods from the literature .", "h": ["recognition rate"], "t": ["system"]}, {"label": "CONJUNCTION", "tokens": "On this challenging data set our system consistently demonstrated a nearly perfect recognition rate -LRB- over 99.7 % on all three databases -RRB- , significantly out-performing state-of-the-art [[ commercial software ]] and << methods >> from the literature .", "h": ["commercial software"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "We present [[ Minimum Bayes-Risk -LRB- MBR -RRB- decoding ]] for << statistical machine translation >> .", "h": ["Minimum Bayes-Risk -LRB- MBR -RRB- decoding"], "t": ["statistical machine translation"]}, {"label": "EVALUATE-FOR", "tokens": "This statistical approach aims to minimize expected loss of translation errors under [[ loss functions ]] that measure << translation >> performance .", "h": ["loss functions"], "t": ["translation"]}, {"label": "USED-FOR", "tokens": "We describe a hierarchy of << loss functions >> that incorporate different levels of [[ linguistic information ]] from word strings , word-to-word alignments from an MT system , and syntactic structure from parse-trees of source and target language sentences .", "h": ["linguistic information"], "t": ["loss functions"]}, {"label": "USED-FOR", "tokens": "We describe a hierarchy of << loss functions >> that incorporate different levels of linguistic information from word strings , [[ word-to-word alignments ]] from an MT system , and syntactic structure from parse-trees of source and target language sentences .", "h": ["word-to-word alignments"], "t": ["loss functions"]}, {"label": "PART-OF", "tokens": "We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , [[ word-to-word alignments ]] from an << MT system >> , and syntactic structure from parse-trees of source and target language sentences .", "h": ["word-to-word alignments"], "t": ["MT system"]}, {"label": "USED-FOR", "tokens": "We describe a hierarchy of << loss functions >> that incorporate different levels of linguistic information from word strings , word-to-word alignments from an MT system , and [[ syntactic structure ]] from parse-trees of source and target language sentences .", "h": ["syntactic structure"], "t": ["loss functions"]}, {"label": "PART-OF", "tokens": "We describe a hierarchy of loss functions that incorporate different levels of linguistic information from word strings , word-to-word alignments from an MT system , and << syntactic structure >> from [[ parse-trees ]] of source and target language sentences .", "h": ["parse-trees"], "t": ["syntactic structure"]}, {"label": "USED-FOR", "tokens": "We report the performance of the [[ MBR decoders ]] on a << Chinese-to-English translation task >> .", "h": ["MBR decoders"], "t": ["Chinese-to-English translation task"]}, {"label": "USED-FOR", "tokens": "Our results show that [[ MBR decoding ]] can be used to tune << statistical MT >> performance for specific loss functions .", "h": ["MBR decoding"], "t": ["statistical MT"]}, {"label": "USED-FOR", "tokens": "Our results show that [[ MBR decoding ]] can be used to tune statistical MT performance for specific << loss functions >> .", "h": ["MBR decoding"], "t": ["loss functions"]}, {"label": "USED-FOR", "tokens": "This paper presents a critical discussion of the various [[ approaches ]] that have been used in the << evaluation of Natural Language systems >> .", "h": ["approaches"], "t": ["evaluation of Natural Language systems"]}, {"label": "EVALUATE-FOR", "tokens": "We conclude that previous [[ approaches ]] have neglected to evaluate << systems >> in the context of their use , e.g. solving a task requiring data retrieval .", "h": ["approaches"], "t": ["systems"]}, {"label": "USED-FOR", "tokens": "We conclude that previous approaches have neglected to evaluate [[ systems ]] in the context of their use , e.g. solving a << task >> requiring data retrieval .", "h": ["systems"], "t": ["task"]}, {"label": "PART-OF", "tokens": "We conclude that previous approaches have neglected to evaluate systems in the context of their use , e.g. solving a << task >> requiring [[ data retrieval ]] .", "h": ["data retrieval"], "t": ["task"]}, {"label": "USED-FOR", "tokens": "In the second half of the paper , we report a laboratory study using the [[ Wizard of Oz technique ]] to identify << NL requirements >> for carrying out this task .", "h": ["Wizard of Oz technique"], "t": ["NL requirements"]}, {"label": "USED-FOR", "tokens": "In the second half of the paper , we report a laboratory study using the [[ Wizard of Oz technique ]] to identify NL requirements for carrying out this << task >> .", "h": ["Wizard of Oz technique"], "t": ["task"]}, {"label": "USED-FOR", "tokens": "We evaluate the demands that [[ task dialogues ]] collected using this technique , place upon a << prototype Natural Language system >> .", "h": ["task dialogues"], "t": ["prototype Natural Language system"]}, {"label": "USED-FOR", "tokens": "We evaluate the demands that << task dialogues >> collected using this [[ technique ]] , place upon a prototype Natural Language system .", "h": ["technique"], "t": ["task dialogues"]}, {"label": "USED-FOR", "tokens": "We present results on << addressee identification in four-participants face-to-face meetings >> using [[ Bayesian Network ]] and Naive Bayes classifiers .", "h": ["Bayesian Network"], "t": ["addressee identification in four-participants face-to-face meetings"]}, {"label": "USED-FOR", "tokens": "We present results on << addressee identification in four-participants face-to-face meetings >> using Bayesian Network and [[ Naive Bayes classifiers ]] .", "h": ["Naive Bayes classifiers"], "t": ["addressee identification in four-participants face-to-face meetings"]}, {"label": "CONJUNCTION", "tokens": "We present results on addressee identification in four-participants face-to-face meetings using << Bayesian Network >> and [[ Naive Bayes classifiers ]] .", "h": ["Naive Bayes classifiers"], "t": ["Bayesian Network"]}, {"label": "USED-FOR", "tokens": "First , we investigate how well the << addressee of a dialogue act >> can be predicted based on [[ gaze ]] , utterance and conversational context features .", "h": ["gaze"], "t": ["addressee of a dialogue act"]}, {"label": "CONJUNCTION", "tokens": "First , we investigate how well the addressee of a dialogue act can be predicted based on [[ gaze ]] , << utterance >> and conversational context features .", "h": ["gaze"], "t": ["utterance"]}, {"label": "USED-FOR", "tokens": "First , we investigate how well the << addressee of a dialogue act >> can be predicted based on gaze , [[ utterance ]] and conversational context features .", "h": ["utterance"], "t": ["addressee of a dialogue act"]}, {"label": "CONJUNCTION", "tokens": "First , we investigate how well the addressee of a dialogue act can be predicted based on gaze , [[ utterance ]] and << conversational context features >> .", "h": ["utterance"], "t": ["conversational context features"]}, {"label": "USED-FOR", "tokens": "First , we investigate how well the << addressee of a dialogue act >> can be predicted based on gaze , utterance and [[ conversational context features ]] .", "h": ["conversational context features"], "t": ["addressee of a dialogue act"]}, {"label": "USED-FOR", "tokens": "Both << classifiers >> perform the best when [[ conversational context ]] and utterance features are combined with speaker 's gaze information .", "h": ["conversational context"], "t": ["classifiers"]}, {"label": "CONJUNCTION", "tokens": "Both classifiers perform the best when [[ conversational context ]] and << utterance features >> are combined with speaker 's gaze information .", "h": ["conversational context"], "t": ["utterance features"]}, {"label": "USED-FOR", "tokens": "Both << classifiers >> perform the best when conversational context and [[ utterance features ]] are combined with speaker 's gaze information .", "h": ["utterance features"], "t": ["classifiers"]}, {"label": "USED-FOR", "tokens": "Both << classifiers >> perform the best when conversational context and utterance features are combined with [[ speaker 's gaze information ]] .", "h": ["speaker 's gaze information"], "t": ["classifiers"]}, {"label": "CONJUNCTION", "tokens": "Both classifiers perform the best when conversational context and << utterance features >> are combined with [[ speaker 's gaze information ]] .", "h": ["speaker 's gaze information"], "t": ["utterance features"]}, {"label": "USED-FOR", "tokens": "Towards deep analysis of << compositional classes of paraphrases >> , we have examined a [[ class-oriented framework ]] for collecting paraphrase examples , in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement .", "h": ["class-oriented framework"], "t": ["compositional classes of paraphrases"]}, {"label": "USED-FOR", "tokens": "Towards deep analysis of compositional classes of paraphrases , we have examined a [[ class-oriented framework ]] for collecting << paraphrase examples >> , in which sentential paraphrases are collected for each paraphrase class separately by means of automatic candidate generation and manual judgement .", "h": ["class-oriented framework"], "t": ["paraphrase examples"]}, {"label": "USED-FOR", "tokens": "Towards deep analysis of compositional classes of paraphrases , we have examined a class-oriented framework for collecting paraphrase examples , in which << sentential paraphrases >> are collected for each paraphrase class separately by means of [[ automatic candidate generation ]] and manual judgement .", "h": ["automatic candidate generation"], "t": ["sentential paraphrases"]}, {"label": "CONJUNCTION", "tokens": "Towards deep analysis of compositional classes of paraphrases , we have examined a class-oriented framework for collecting paraphrase examples , in which sentential paraphrases are collected for each paraphrase class separately by means of [[ automatic candidate generation ]] and << manual judgement >> .", "h": ["automatic candidate generation"], "t": ["manual judgement"]}, {"label": "USED-FOR", "tokens": "Towards deep analysis of compositional classes of paraphrases , we have examined a class-oriented framework for collecting paraphrase examples , in which << sentential paraphrases >> are collected for each paraphrase class separately by means of automatic candidate generation and [[ manual judgement ]] .", "h": ["manual judgement"], "t": ["sentential paraphrases"]}, {"label": "USED-FOR", "tokens": "The purpose of this research is to test the efficacy of applying [[ automated evaluation techniques ]] , originally devised for the << evaluation of human language learners >> , to the output of machine translation -LRB- MT -RRB- systems .", "h": ["automated evaluation techniques"], "t": ["evaluation of human language learners"]}, {"label": "USED-FOR", "tokens": "We believe that these [[ evaluation techniques ]] will provide information about both the << human language learning process >> , the translation process and the development of machine translation systems .", "h": ["evaluation techniques"], "t": ["human language learning process"]}, {"label": "USED-FOR", "tokens": "We believe that these [[ evaluation techniques ]] will provide information about both the human language learning process , the << translation process >> and the development of machine translation systems .", "h": ["evaluation techniques"], "t": ["translation process"]}, {"label": "USED-FOR", "tokens": "We believe that these [[ evaluation techniques ]] will provide information about both the human language learning process , the translation process and the development of << machine translation systems >> .", "h": ["evaluation techniques"], "t": ["machine translation systems"]}, {"label": "CONJUNCTION", "tokens": "We believe that these evaluation techniques will provide information about both the [[ human language learning process ]] , the << translation process >> and the development of machine translation systems .", "h": ["human language learning process"], "t": ["translation process"]}, {"label": "CONJUNCTION", "tokens": "We believe that these evaluation techniques will provide information about both the human language learning process , the [[ translation process ]] and the development of << machine translation systems >> .", "h": ["translation process"], "t": ["machine translation systems"]}, {"label": "EVALUATE-FOR", "tokens": "A [[ language learning ]] experiment showed that << assessors >> can differentiate native from non-native language essays in less than 100 words .", "h": ["language learning"], "t": ["assessors"]}, {"label": "CONJUNCTION", "tokens": "Some of the extracts were << expert human translations >> , others were [[ machine translation outputs ]] .", "h": ["machine translation outputs"], "t": ["expert human translations"]}, {"label": "COMPARE", "tokens": "The subjects were given three minutes per extract to determine whether they believed the sample output to be an [[ expert human translation ]] or a << machine translation >> .", "h": ["expert human translation"], "t": ["machine translation"]}, {"label": "USED-FOR", "tokens": "This paper presents a [[ machine learning approach ]] to << bare slice disambiguation >> in dialogue .", "h": ["machine learning approach"], "t": ["bare slice disambiguation"]}, {"label": "USED-FOR", "tokens": "This paper presents a machine learning approach to << bare slice disambiguation >> in [[ dialogue ]] .", "h": ["dialogue"], "t": ["bare slice disambiguation"]}, {"label": "USED-FOR", "tokens": "We extract a set of << heuristic principles >> from a [[ corpus-based sample ]] and formulate them as probabilistic Horn clauses .", "h": ["corpus-based sample"], "t": ["heuristic principles"]}, {"label": "FEATURE-OF", "tokens": "We extract a set of << heuristic principles >> from a corpus-based sample and formulate them as [[ probabilistic Horn clauses ]] .", "h": ["probabilistic Horn clauses"], "t": ["heuristic principles"]}, {"label": "HYPONYM-OF", "tokens": "We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : [[ SLIPPER ]] , a << rule-based learning algorithm >> , and TiMBL , a memory-based system .", "h": ["SLIPPER"], "t": ["rule-based learning algorithm"]}, {"label": "PART-OF", "tokens": "We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different << machine learning algorithms >> : SLIPPER , a [[ rule-based learning algorithm ]] , and TiMBL , a memory-based system .", "h": ["rule-based learning algorithm"], "t": ["machine learning algorithms"]}, {"label": "COMPARE", "tokens": "We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a [[ rule-based learning algorithm ]] , and TiMBL , a << memory-based system >> .", "h": ["rule-based learning algorithm"], "t": ["memory-based system"]}, {"label": "HYPONYM-OF", "tokens": "We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different machine learning algorithms : SLIPPER , a rule-based learning algorithm , and [[ TiMBL ]] , a << memory-based system >> .", "h": ["TiMBL"], "t": ["memory-based system"]}, {"label": "PART-OF", "tokens": "We then use the predicates of such clauses to create a set of domain independent features to annotate an input dataset , and run two different << machine learning algorithms >> : SLIPPER , a rule-based learning algorithm , and TiMBL , a [[ memory-based system ]] .", "h": ["memory-based system"], "t": ["machine learning algorithms"]}, {"label": "FEATURE-OF", "tokens": "The results show that the [[ features ]] in terms of which we formulate our << heuristic principles >> have significant predictive power , and that rules that closely resemble our Horn clauses can be learnt automatically from these features .", "h": ["features"], "t": ["heuristic principles"]}, {"label": "USED-FOR", "tokens": "We suggest a new goal and [[ evaluation criterion ]] for << word similarity measures >> .", "h": ["evaluation criterion"], "t": ["word similarity measures"]}, {"label": "USED-FOR", "tokens": "The new criterion -- [[ meaning-entailing substitutability ]] -- fits the needs of << semantic-oriented NLP applications >> and can be evaluated directly -LRB- independent of an application -RRB- at a good level of human agreement .", "h": ["meaning-entailing substitutability"], "t": ["semantic-oriented NLP applications"]}, {"label": "EVALUATE-FOR", "tokens": "The new criterion -- << meaning-entailing substitutability >> -- fits the needs of semantic-oriented NLP applications and can be evaluated directly -LRB- independent of an application -RRB- at a good level of [[ human agreement ]] .", "h": ["human agreement"], "t": ["meaning-entailing substitutability"]}, {"label": "EVALUATE-FOR", "tokens": "Motivated by this [[ semantic criterion ]] we analyze the empirical quality of << distributional word feature vectors >> and its impact on word similarity results , proposing an objective measure for evaluating feature vector quality .", "h": ["semantic criterion"], "t": ["distributional word feature vectors"]}, {"label": "USED-FOR", "tokens": "Motivated by this semantic criterion we analyze the empirical quality of [[ distributional word feature vectors ]] and its impact on << word similarity >> results , proposing an objective measure for evaluating feature vector quality .", "h": ["distributional word feature vectors"], "t": ["word similarity"]}, {"label": "EVALUATE-FOR", "tokens": "Motivated by this semantic criterion we analyze the empirical quality of distributional word feature vectors and its impact on word similarity results , proposing an objective [[ measure ]] for evaluating << feature vector quality >> .", "h": ["measure"], "t": ["feature vector quality"]}, {"label": "USED-FOR", "tokens": "Finally , a novel [[ feature weighting and selection function ]] is presented , which yields superior << feature vectors >> and better word similarity performance .", "h": ["feature weighting and selection function"], "t": ["feature vectors"]}, {"label": "USED-FOR", "tokens": "Finally , a novel [[ feature weighting and selection function ]] is presented , which yields superior feature vectors and better << word similarity >> performance .", "h": ["feature weighting and selection function"], "t": ["word similarity"]}, {"label": "CONJUNCTION", "tokens": "Finally , a novel feature weighting and selection function is presented , which yields superior [[ feature vectors ]] and better << word similarity >> performance .", "h": ["feature vectors"], "t": ["word similarity"]}, {"label": "CONJUNCTION", "tokens": "This phenomenon causes many image processing techniques to fail as they assume the presence of only one layer at each examined site e.g. [[ motion estimation ]] and << object recognition >> .", "h": ["motion estimation"], "t": ["object recognition"]}, {"label": "USED-FOR", "tokens": "This work presents an automated [[ technique ]] for << detecting reflections in image sequences >> by analyzing motion trajectories of feature points .", "h": ["technique"], "t": ["detecting reflections in image sequences"]}, {"label": "USED-FOR", "tokens": "This work presents an automated << technique >> for detecting reflections in image sequences by analyzing [[ motion trajectories ]] of feature points .", "h": ["motion trajectories"], "t": ["technique"]}, {"label": "FEATURE-OF", "tokens": "This work presents an automated technique for detecting reflections in image sequences by analyzing << motion trajectories >> of [[ feature points ]] .", "h": ["feature points"], "t": ["motion trajectories"]}, {"label": "USED-FOR", "tokens": "[[ It ]] models << reflection >> as regions containing two different layers moving over each other .", "h": ["It"], "t": ["reflection"]}, {"label": "USED-FOR", "tokens": "We present a strong << detector >> based on combining a set of weak [[ detectors ]] .", "h": ["detectors"], "t": ["detector"]}, {"label": "USED-FOR", "tokens": "We use novel [[ priors ]] , generate << sparse and dense detection maps >> and our results show high detection rate with rejection to pathological motion and occlusion .", "h": ["priors"], "t": ["sparse and dense detection maps"]}, {"label": "CONJUNCTION", "tokens": "We use novel priors , generate sparse and dense detection maps and our results show high detection rate with rejection to [[ pathological motion ]] and << occlusion >> .", "h": ["pathological motion"], "t": ["occlusion"]}, {"label": "USED-FOR", "tokens": "This paper considers the problem of << reconstructing the motion of a 3D articulated tree >> from [[ 2D point correspondences ]] subject to some temporal prior .", "h": ["2D point correspondences"], "t": ["reconstructing the motion of a 3D articulated tree"]}, {"label": "USED-FOR", "tokens": "Hitherto , << smooth motion >> has been encouraged using a [[ trajectory basis ]] , yielding a hard combinatorial problem with time complexity growing exponentially in the number of frames .", "h": ["trajectory basis"], "t": ["smooth motion"]}, {"label": "EVALUATE-FOR", "tokens": "Hitherto , smooth motion has been encouraged using a trajectory basis , yielding a << hard combinatorial problem >> with [[ time complexity ]] growing exponentially in the number of frames .", "h": ["time complexity"], "t": ["hard combinatorial problem"]}, {"label": "USED-FOR", "tokens": "[[ Branch and bound strategies ]] have previously attempted to curb this << complexity >> whilst maintaining global optimality .", "h": ["Branch and bound strategies"], "t": ["complexity"]}, {"label": "FEATURE-OF", "tokens": "<< Branch and bound strategies >> have previously attempted to curb this complexity whilst maintaining [[ global optimality ]] .", "h": ["global optimality"], "t": ["Branch and bound strategies"]}, {"label": "COMPARE", "tokens": "However , [[ they ]] provide no guarantee of being more efficient than << exhaustive search >> .", "h": ["they"], "t": ["exhaustive search"]}, {"label": "USED-FOR", "tokens": "Extension to [[ affine projection ]] enables << reconstruction >> without estimating cameras .", "h": ["affine projection"], "t": ["reconstruction"]}, {"label": "HYPONYM-OF", "tokens": "[[ Topical blog post retrieval ]] is the task of << ranking blog posts >> with respect to their relevance for a given topic .", "h": ["Topical blog post retrieval"], "t": ["ranking blog posts"]}, {"label": "FEATURE-OF", "tokens": "Topical blog post retrieval is the task of ranking << blog posts >> with respect to their [[ relevance ]] for a given topic .", "h": ["relevance"], "t": ["blog posts"]}, {"label": "USED-FOR", "tokens": "To improve << topical blog post retrieval >> we incorporate [[ textual credibility indicators ]] in the retrieval process .", "h": ["textual credibility indicators"], "t": ["topical blog post retrieval"]}, {"label": "PART-OF", "tokens": "To improve topical blog post retrieval we incorporate [[ textual credibility indicators ]] in the << retrieval process >> .", "h": ["textual credibility indicators"], "t": ["retrieval process"]}, {"label": "PART-OF", "tokens": "We describe how to estimate these indicators and how to integrate [[ them ]] into a << retrieval approach >> based on language models .", "h": ["them"], "t": ["retrieval approach"]}, {"label": "USED-FOR", "tokens": "We describe how to estimate these indicators and how to integrate << them >> into a retrieval approach based on [[ language models ]] .", "h": ["language models"], "t": ["them"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on the [[ TREC Blog track test set ]] show that both groups of << credibility indicators >> significantly improve retrieval effectiveness ; the best performance is achieved when combining them .", "h": ["TREC Blog track test set"], "t": ["credibility indicators"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on the TREC Blog track test set show that both groups of << credibility indicators >> significantly improve [[ retrieval effectiveness ]] ; the best performance is achieved when combining them .", "h": ["retrieval effectiveness"], "t": ["credibility indicators"]}, {"label": "USED-FOR", "tokens": "We investigate the problem of learning to predict moves in the << board game of Go >> from [[ game records of expert players ]] .", "h": ["game records of expert players"], "t": ["board game of Go"]}, {"label": "USED-FOR", "tokens": "This [[ distribution ]] has numerous applications in << computer Go >> , including serving as an efficient stand-alone Go player .", "h": ["distribution"], "t": ["computer Go"]}, {"label": "USED-FOR", "tokens": "[[ It ]] would also be effective as a << move selector >> and move sorter for game tree search and as a training tool for Go players .", "h": ["It"], "t": ["move selector"]}, {"label": "USED-FOR", "tokens": "[[ It ]] would also be effective as a move selector and << move sorter >> for game tree search and as a training tool for Go players .", "h": ["It"], "t": ["move sorter"]}, {"label": "USED-FOR", "tokens": "[[ It ]] would also be effective as a move selector and move sorter for game tree search and as a << training tool >> for Go players .", "h": ["It"], "t": ["training tool"]}, {"label": "CONJUNCTION", "tokens": "It would also be effective as a [[ move selector ]] and << move sorter >> for game tree search and as a training tool for Go players .", "h": ["move selector"], "t": ["move sorter"]}, {"label": "USED-FOR", "tokens": "It would also be effective as a [[ move selector ]] and move sorter for << game tree search >> and as a training tool for Go players .", "h": ["move selector"], "t": ["game tree search"]}, {"label": "USED-FOR", "tokens": "It would also be effective as a move selector and [[ move sorter ]] for << game tree search >> and as a training tool for Go players .", "h": ["move sorter"], "t": ["game tree search"]}, {"label": "USED-FOR", "tokens": "It would also be effective as a move selector and move sorter for game tree search and as a [[ training tool ]] for << Go players >> .", "h": ["training tool"], "t": ["Go players"]}, {"label": "PART-OF", "tokens": "Our << method >> has two major components : a -RRB- a [[ pattern extraction scheme ]] for efficiently harvesting patterns of given size and shape from expert game records and b -RRB- a Bayesian learning algorithm -LRB- in two variants -RRB- that learns a distribution over the values of a move given a board position based on the local pattern context .", "h": ["pattern extraction scheme"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "Our method has two major components : a -RRB- a [[ pattern extraction scheme ]] for efficiently harvesting patterns of given size and shape from expert game records and b -RRB- a << Bayesian learning algorithm >> -LRB- in two variants -RRB- that learns a distribution over the values of a move given a board position based on the local pattern context .", "h": ["pattern extraction scheme"], "t": ["Bayesian learning algorithm"]}, {"label": "PART-OF", "tokens": "Our << method >> has two major components : a -RRB- a pattern extraction scheme for efficiently harvesting patterns of given size and shape from expert game records and b -RRB- a [[ Bayesian learning algorithm ]] -LRB- in two variants -RRB- that learns a distribution over the values of a move given a board position based on the local pattern context .", "h": ["Bayesian learning algorithm"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The << system >> is trained on 181,000 [[ expert games ]] and shows excellent prediction performance as indicated by its ability to perfectly predict the moves made by professional Go players in 34 % of test positions .", "h": ["expert games"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "We present a novel [[ approach ]] for << automatically acquiring English topic signatures >> .", "h": ["approach"], "t": ["automatically acquiring English topic signatures"]}, {"label": "USED-FOR", "tokens": "[[ Topic signatures ]] can be useful in a number of << Natural Language Processing -LRB- NLP -RRB- applications >> , such as Word Sense Disambiguation -LRB- WSD -RRB- and Text Summarisation .", "h": ["Topic signatures"], "t": ["Natural Language Processing -LRB- NLP -RRB- applications"]}, {"label": "USED-FOR", "tokens": "[[ Topic signatures ]] can be useful in a number of Natural Language Processing -LRB- NLP -RRB- applications , such as << Word Sense Disambiguation -LRB- WSD -RRB- >> and Text Summarisation .", "h": ["Topic signatures"], "t": ["Word Sense Disambiguation -LRB- WSD -RRB-"]}, {"label": "USED-FOR", "tokens": "[[ Topic signatures ]] can be useful in a number of Natural Language Processing -LRB- NLP -RRB- applications , such as Word Sense Disambiguation -LRB- WSD -RRB- and << Text Summarisation >> .", "h": ["Topic signatures"], "t": ["Text Summarisation"]}, {"label": "HYPONYM-OF", "tokens": "Topic signatures can be useful in a number of << Natural Language Processing -LRB- NLP -RRB- applications >> , such as [[ Word Sense Disambiguation -LRB- WSD -RRB- ]] and Text Summarisation .", "h": ["Word Sense Disambiguation -LRB- WSD -RRB-"], "t": ["Natural Language Processing -LRB- NLP -RRB- applications"]}, {"label": "CONJUNCTION", "tokens": "Topic signatures can be useful in a number of Natural Language Processing -LRB- NLP -RRB- applications , such as [[ Word Sense Disambiguation -LRB- WSD -RRB- ]] and << Text Summarisation >> .", "h": ["Word Sense Disambiguation -LRB- WSD -RRB-"], "t": ["Text Summarisation"]}, {"label": "HYPONYM-OF", "tokens": "Topic signatures can be useful in a number of << Natural Language Processing -LRB- NLP -RRB- applications >> , such as Word Sense Disambiguation -LRB- WSD -RRB- and [[ Text Summarisation ]] .", "h": ["Text Summarisation"], "t": ["Natural Language Processing -LRB- NLP -RRB- applications"]}, {"label": "PART-OF", "tokens": "Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of [[ Chinese text ]] available in << corpora >> and on the Web .", "h": ["Chinese text"], "t": ["corpora"]}, {"label": "PART-OF", "tokens": "Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of [[ Chinese text ]] available in corpora and on the << Web >> .", "h": ["Chinese text"], "t": ["Web"]}, {"label": "CONJUNCTION", "tokens": "Our method takes advantage of the different way in which word senses are lexicalised in English and Chinese , and also exploits the large amount of Chinese text available in [[ corpora ]] and on the << Web >> .", "h": ["corpora"], "t": ["Web"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluated the << topic signatures >> on a [[ WSD task ]] , where we trained a second-order vector cooccurrence algorithm on standard WSD datasets , with promising results .", "h": ["WSD task"], "t": ["topic signatures"]}, {"label": "USED-FOR", "tokens": "We evaluated the topic signatures on a WSD task , where we trained a << second-order vector cooccurrence algorithm >> on standard [[ WSD datasets ]] , with promising results .", "h": ["WSD datasets"], "t": ["second-order vector cooccurrence algorithm"]}, {"label": "USED-FOR", "tokens": "[[ Joint matrix triangularization ]] is often used for estimating the << joint eigenstructure >> of a set M of matrices , with applications in signal processing and machine learning .", "h": ["Joint matrix triangularization"], "t": ["joint eigenstructure"]}, {"label": "USED-FOR", "tokens": "Joint matrix triangularization is often used for estimating the [[ joint eigenstructure ]] of a set M of matrices , with applications in << signal processing >> and machine learning .", "h": ["joint eigenstructure"], "t": ["signal processing"]}, {"label": "USED-FOR", "tokens": "Joint matrix triangularization is often used for estimating the [[ joint eigenstructure ]] of a set M of matrices , with applications in signal processing and << machine learning >> .", "h": ["joint eigenstructure"], "t": ["machine learning"]}, {"label": "CONJUNCTION", "tokens": "Joint matrix triangularization is often used for estimating the joint eigenstructure of a set M of matrices , with applications in [[ signal processing ]] and << machine learning >> .", "h": ["signal processing"], "t": ["machine learning"]}, {"label": "CONJUNCTION", "tokens": "Our main result is a first-order upper bound on the distance between any [[ approximate joint triangularizer ]] of the matrices in M ' and any << exact joint triangularizer >> of the matrices in M .", "h": ["approximate joint triangularizer"], "t": ["exact joint triangularizer"]}, {"label": "USED-FOR", "tokens": "To our knowledge , this is the first a [[ posteriori bound ]] for << joint matrix decomposition >> .", "h": ["posteriori bound"], "t": ["joint matrix decomposition"]}, {"label": "USED-FOR", "tokens": "The [[ psycholinguistic literature ]] provides evidence for << syntactic priming >> , i.e. , the tendency to repeat structures .", "h": ["psycholinguistic literature"], "t": ["syntactic priming"]}, {"label": "USED-FOR", "tokens": "This paper describes a [[ method ]] for incorporating << priming >> into an incremental probabilistic parser .", "h": ["method"], "t": ["priming"]}, {"label": "USED-FOR", "tokens": "This paper describes a method for incorporating [[ priming ]] into an << incremental probabilistic parser >> .", "h": ["priming"], "t": ["incremental probabilistic parser"]}, {"label": "PART-OF", "tokens": "These models simulate the reading time advantage for [[ parallel structures ]] found in << human data >> , and also yield a small increase in overall parsing accuracy .", "h": ["parallel structures"], "t": ["human data"]}, {"label": "USED-FOR", "tokens": "[[ Learned confidence measures ]] gain increasing importance for << outlier removal >> and quality improvement in stereo vision .", "h": ["Learned confidence measures"], "t": ["outlier removal"]}, {"label": "USED-FOR", "tokens": "[[ Learned confidence measures ]] gain increasing importance for outlier removal and << quality improvement >> in stereo vision .", "h": ["Learned confidence measures"], "t": ["quality improvement"]}, {"label": "CONJUNCTION", "tokens": "Learned confidence measures gain increasing importance for [[ outlier removal ]] and << quality improvement >> in stereo vision .", "h": ["outlier removal"], "t": ["quality improvement"]}, {"label": "PART-OF", "tokens": "Learned confidence measures gain increasing importance for [[ outlier removal ]] and quality improvement in << stereo vision >> .", "h": ["outlier removal"], "t": ["stereo vision"]}, {"label": "PART-OF", "tokens": "Learned confidence measures gain increasing importance for outlier removal and [[ quality improvement ]] in << stereo vision >> .", "h": ["quality improvement"], "t": ["stereo vision"]}, {"label": "USED-FOR", "tokens": "However , acquiring the necessary training data is typically a tedious and time consuming << task >> that involves [[ manual interaction ]] , active sensing devices and/or synthetic scenes .", "h": ["manual interaction"], "t": ["task"]}, {"label": "CONJUNCTION", "tokens": "However , acquiring the necessary training data is typically a tedious and time consuming task that involves [[ manual interaction ]] , << active sensing devices >> and/or synthetic scenes .", "h": ["manual interaction"], "t": ["active sensing devices"]}, {"label": "USED-FOR", "tokens": "However , acquiring the necessary training data is typically a tedious and time consuming << task >> that involves manual interaction , [[ active sensing devices ]] and/or synthetic scenes .", "h": ["active sensing devices"], "t": ["task"]}, {"label": "CONJUNCTION", "tokens": "However , acquiring the necessary training data is typically a tedious and time consuming task that involves manual interaction , [[ active sensing devices ]] and/or << synthetic scenes >> .", "h": ["active sensing devices"], "t": ["synthetic scenes"]}, {"label": "USED-FOR", "tokens": "However , acquiring the necessary training data is typically a tedious and time consuming << task >> that involves manual interaction , active sensing devices and/or [[ synthetic scenes ]] .", "h": ["synthetic scenes"], "t": ["task"]}, {"label": "USED-FOR", "tokens": "The key idea of our << approach >> is to use different [[ view points ]] for reasoning about contradictions and consistencies between multiple depth maps generated with the same stereo algorithm .", "h": ["view points"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "Among other experiments , we demonstrate the potential of our [[ approach ]] by boosting the performance of three << learned confidence measures >> on the KITTI2012 dataset by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data .", "h": ["approach"], "t": ["learned confidence measures"]}, {"label": "EVALUATE-FOR", "tokens": "Among other experiments , we demonstrate the potential of our approach by boosting the performance of three << learned confidence measures >> on the [[ KITTI2012 dataset ]] by simply training them on a vast amount of automatically generated training data rather than a limited amount of laser ground truth data .", "h": ["KITTI2012 dataset"], "t": ["learned confidence measures"]}, {"label": "USED-FOR", "tokens": "Among other experiments , we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the KITTI2012 dataset by simply training << them >> on a vast amount of [[ automatically generated training data ]] rather than a limited amount of laser ground truth data .", "h": ["automatically generated training data"], "t": ["them"]}, {"label": "COMPARE", "tokens": "Among other experiments , we demonstrate the potential of our approach by boosting the performance of three learned confidence measures on the KITTI2012 dataset by simply training them on a vast amount of << automatically generated training data >> rather than a limited amount of [[ laser ground truth data ]] .", "h": ["laser ground truth data"], "t": ["automatically generated training data"]}, {"label": "USED-FOR", "tokens": "An important area of [[ learning in autonomous agents ]] is the ability to learn << domain-speciic models of actions >> to be used by planning systems .", "h": ["learning in autonomous agents"], "t": ["domain-speciic models of actions"]}, {"label": "USED-FOR", "tokens": "An important area of learning in autonomous agents is the ability to learn << domain-speciic models of actions >> to be used by [[ planning systems ]] .", "h": ["planning systems"], "t": ["domain-speciic models of actions"]}, {"label": "USED-FOR", "tokens": "These << methods >> diier from previous work in the area in two ways : the use of an [[ action model formalism ]] which is better suited to the needs of a re-active agent , and successful implementation of noise-handling mechanisms .", "h": ["action model formalism"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "These methods diier from previous work in the area in two ways : the use of an [[ action model formalism ]] which is better suited to the needs of a << re-active agent >> , and successful implementation of noise-handling mechanisms .", "h": ["action model formalism"], "t": ["re-active agent"]}, {"label": "USED-FOR", "tokens": "These << methods >> diier from previous work in the area in two ways : the use of an action model formalism which is better suited to the needs of a re-active agent , and successful implementation of [[ noise-handling mechanisms ]] .", "h": ["noise-handling mechanisms"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "Training instances are generated from experience and observation , and a variant of [[ GOLEM ]] is used to learn << action models >> from these instances .", "h": ["GOLEM"], "t": ["action models"]}, {"label": "EVALUATE-FOR", "tokens": "The << integrated learning system >> has been experimentally validated in [[ simulated construction ]] and ooce domains .", "h": ["simulated construction"], "t": ["integrated learning system"]}, {"label": "CONJUNCTION", "tokens": "The integrated learning system has been experimentally validated in [[ simulated construction ]] and << ooce domains >> .", "h": ["simulated construction"], "t": ["ooce domains"]}, {"label": "EVALUATE-FOR", "tokens": "The << integrated learning system >> has been experimentally validated in simulated construction and [[ ooce domains ]] .", "h": ["ooce domains"], "t": ["integrated learning system"]}, {"label": "HYPONYM-OF", "tokens": "This paper describes [[ FERRET ]] , an << interactive question-answering -LRB- Q/A -RRB- system >> designed to address the challenges of integrating automatic Q/A applications into real-world environments .", "h": ["FERRET"], "t": ["interactive question-answering -LRB- Q/A -RRB- system"]}, {"label": "USED-FOR", "tokens": "This paper describes [[ FERRET ]] , an interactive question-answering -LRB- Q/A -RRB- system designed to address the challenges of << integrating automatic Q/A applications into real-world environments >> .", "h": ["FERRET"], "t": ["integrating automatic Q/A applications into real-world environments"]}, {"label": "USED-FOR", "tokens": "<< FERRET >> utilizes a novel [[ approach ]] to Q/A known as predictive questioning which attempts to identify the questions -LRB- and answers -RRB- that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario .", "h": ["approach"], "t": ["FERRET"]}, {"label": "USED-FOR", "tokens": "FERRET utilizes a novel [[ approach ]] to << Q/A >> known as predictive questioning which attempts to identify the questions -LRB- and answers -RRB- that users need by analyzing how a user interacts with a system while gathering information related to a particular scenario .", "h": ["approach"], "t": ["Q/A"]}, {"label": "USED-FOR", "tokens": "In order to build robust << automatic abstracting systems >> , there is a need for better [[ training resources ]] than are currently available .", "h": ["training resources"], "t": ["automatic abstracting systems"]}, {"label": "USED-FOR", "tokens": "In this paper , we introduce an [[ annotation scheme ]] for << scientific articles >> which can be used to build such a resource in a consistent way .", "h": ["annotation scheme"], "t": ["scientific articles"]}, {"label": "USED-FOR", "tokens": "In this paper , we introduce an [[ annotation scheme ]] for scientific articles which can be used to build such a << resource >> in a consistent way .", "h": ["annotation scheme"], "t": ["resource"]}, {"label": "USED-FOR", "tokens": "The seven categories of the << scheme >> are based on [[ rhetorical moves of argumentation ]] .", "h": ["rhetorical moves of argumentation"], "t": ["scheme"]}, {"label": "USED-FOR", "tokens": "The << automated segmentation >> of [[ images ]] into semantically meaningful parts requires shape information since low-level feature analysis alone often fails to reach this goal .", "h": ["images"], "t": ["automated segmentation"]}, {"label": "USED-FOR", "tokens": "We introduce a novel [[ method ]] of << shape constrained image segmentation >> which is based on mixtures of feature distributions for color and texture as well as probabilistic shape knowledge .", "h": ["method"], "t": ["shape constrained image segmentation"]}, {"label": "USED-FOR", "tokens": "We introduce a novel << method >> of shape constrained image segmentation which is based on [[ mixtures of feature distributions ]] for color and texture as well as probabilistic shape knowledge .", "h": ["mixtures of feature distributions"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "We introduce a novel method of shape constrained image segmentation which is based on [[ mixtures of feature distributions ]] for << color >> and texture as well as probabilistic shape knowledge .", "h": ["mixtures of feature distributions"], "t": ["color"]}, {"label": "USED-FOR", "tokens": "We introduce a novel method of shape constrained image segmentation which is based on [[ mixtures of feature distributions ]] for color and << texture >> as well as probabilistic shape knowledge .", "h": ["mixtures of feature distributions"], "t": ["texture"]}, {"label": "USED-FOR", "tokens": "We introduce a novel method of shape constrained image segmentation which is based on [[ mixtures of feature distributions ]] for color and texture as well as << probabilistic shape knowledge >> .", "h": ["mixtures of feature distributions"], "t": ["probabilistic shape knowledge"]}, {"label": "CONJUNCTION", "tokens": "We introduce a novel method of shape constrained image segmentation which is based on mixtures of feature distributions for [[ color ]] and << texture >> as well as probabilistic shape knowledge .", "h": ["color"], "t": ["texture"]}, {"label": "CONJUNCTION", "tokens": "We introduce a novel method of shape constrained image segmentation which is based on mixtures of feature distributions for color and [[ texture ]] as well as << probabilistic shape knowledge >> .", "h": ["texture"], "t": ["probabilistic shape knowledge"]}, {"label": "USED-FOR", "tokens": "The combined [[ approach ]] is formulated in the framework of Bayesian statistics to account for the << robust-ness requirement in image understanding >> .", "h": ["approach"], "t": ["robust-ness requirement in image understanding"]}, {"label": "USED-FOR", "tokens": "The combined << approach >> is formulated in the framework of [[ Bayesian statistics ]] to account for the robust-ness requirement in image understanding .", "h": ["Bayesian statistics"], "t": ["approach"]}, {"label": "FEATURE-OF", "tokens": "The goal of this work is the enrichment of << human-machine interactions >> in a [[ natural language environment ]] .", "h": ["natural language environment"], "t": ["human-machine interactions"]}, {"label": "HYPONYM-OF", "tokens": "This paper highlights a particular class of << miscommunication >> -- [[ reference problems ]] -- by describing a case study and techniques for avoiding failures of reference .", "h": ["reference problems"], "t": ["miscommunication"]}, {"label": "USED-FOR", "tokens": "This paper highlights a particular class of miscommunication -- reference problems -- by describing a case study and [[ techniques ]] for avoiding << failures of reference >> .", "h": ["techniques"], "t": ["failures of reference"]}, {"label": "USED-FOR", "tokens": "This paper examines the benefits of [[ system combination ]] for << unsupervised WSD >> .", "h": ["system combination"], "t": ["unsupervised WSD"]}, {"label": "USED-FOR", "tokens": "We investigate several [[ voting - and arbiter-based combination strategies ]] over a diverse pool of << unsupervised WSD systems >> .", "h": ["voting - and arbiter-based combination strategies"], "t": ["unsupervised WSD systems"]}, {"label": "USED-FOR", "tokens": "Our << combination methods >> rely on [[ predominant senses ]] which are derived automatically from raw text .", "h": ["predominant senses"], "t": ["combination methods"]}, {"label": "USED-FOR", "tokens": "Our combination methods rely on << predominant senses >> which are derived automatically from [[ raw text ]] .", "h": ["raw text"], "t": ["predominant senses"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments using the [[ SemCor and Senseval-3 data sets ]] demonstrate that our << ensembles >> yield significantly better results when compared with state-of-the-art .", "h": ["SemCor and Senseval-3 data sets"], "t": ["ensembles"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments using the [[ SemCor and Senseval-3 data sets ]] demonstrate that our ensembles yield significantly better results when compared with << state-of-the-art >> .", "h": ["SemCor and Senseval-3 data sets"], "t": ["state-of-the-art"]}, {"label": "USED-FOR", "tokens": "The applicability of many current << information extraction techniques >> is severely limited by the need for [[ supervised training data ]] .", "h": ["supervised training data"], "t": ["information extraction techniques"]}, {"label": "HYPONYM-OF", "tokens": "We demonstrate that for certain << field structured extraction tasks >> , such as [[ classified advertisements ]] and bibliographic citations , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .", "h": ["classified advertisements"], "t": ["field structured extraction tasks"]}, {"label": "CONJUNCTION", "tokens": "We demonstrate that for certain field structured extraction tasks , such as [[ classified advertisements ]] and << bibliographic citations >> , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .", "h": ["classified advertisements"], "t": ["bibliographic citations"]}, {"label": "HYPONYM-OF", "tokens": "We demonstrate that for certain << field structured extraction tasks >> , such as classified advertisements and [[ bibliographic citations ]] , small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion .", "h": ["bibliographic citations"], "t": ["field structured extraction tasks"]}, {"label": "USED-FOR", "tokens": "We demonstrate that for certain << field structured extraction tasks >> , such as classified advertisements and bibliographic citations , small amounts of [[ prior knowledge ]] can be used to learn effective models in a primarily unsupervised fashion .", "h": ["prior knowledge"], "t": ["field structured extraction tasks"]}, {"label": "USED-FOR", "tokens": "Although [[ hidden Markov models -LRB- HMMs -RRB- ]] provide a suitable << generative model >> for field structured text , general unsupervised HMM learning fails to learn useful structure in either of our domains .", "h": ["hidden Markov models -LRB- HMMs -RRB-"], "t": ["generative model"]}, {"label": "USED-FOR", "tokens": "Although hidden Markov models -LRB- HMMs -RRB- provide a suitable [[ generative model ]] for << field structured text >> , general unsupervised HMM learning fails to learn useful structure in either of our domains .", "h": ["generative model"], "t": ["field structured text"]}, {"label": "COMPARE", "tokens": "In both domains , we found that [[ unsupervised methods ]] can attain accuracies with 400 unlabeled examples comparable to those attained by << supervised methods >> on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .", "h": ["unsupervised methods"], "t": ["supervised methods"]}, {"label": "EVALUATE-FOR", "tokens": "In both domains , we found that << unsupervised methods >> can attain [[ accuracies ]] with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .", "h": ["accuracies"], "t": ["unsupervised methods"]}, {"label": "EVALUATE-FOR", "tokens": "In both domains , we found that unsupervised methods can attain [[ accuracies ]] with 400 unlabeled examples comparable to those attained by << supervised methods >> on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .", "h": ["accuracies"], "t": ["supervised methods"]}, {"label": "USED-FOR", "tokens": "In both domains , we found that << unsupervised methods >> can attain accuracies with 400 [[ unlabeled examples ]] comparable to those attained by supervised methods on 50 labeled examples , and that semi-supervised methods can make good use of small amounts of labeled data .", "h": ["unlabeled examples"], "t": ["unsupervised methods"]}, {"label": "USED-FOR", "tokens": "In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by << supervised methods >> on 50 [[ labeled examples ]] , and that semi-supervised methods can make good use of small amounts of labeled data .", "h": ["labeled examples"], "t": ["supervised methods"]}, {"label": "USED-FOR", "tokens": "In both domains , we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples , and that << semi-supervised methods >> can make good use of small amounts of [[ labeled data ]] .", "h": ["labeled data"], "t": ["semi-supervised methods"]}, {"label": "HYPONYM-OF", "tokens": "This paper gives an overall account of a prototype << natural language question answering system >> , called [[ Chat-80 ]] .", "h": ["Chat-80"], "t": ["natural language question answering system"]}, {"label": "USED-FOR", "tokens": "The << system >> is implemented entirely in [[ Prolog ]] , a programming language based on logic .", "h": ["Prolog"], "t": ["system"]}, {"label": "HYPONYM-OF", "tokens": "The system is implemented entirely in [[ Prolog ]] , a << programming language >> based on logic .", "h": ["Prolog"], "t": ["programming language"]}, {"label": "USED-FOR", "tokens": "The system is implemented entirely in Prolog , a << programming language >> based on [[ logic ]] .", "h": ["logic"], "t": ["programming language"]}, {"label": "HYPONYM-OF", "tokens": "With the aid of a << logic-based grammar formalism >> called [[ extraposition grammars ]] , Chat-80 translates English questions into the Prolog subset of logic .", "h": ["extraposition grammars"], "t": ["logic-based grammar formalism"]}, {"label": "USED-FOR", "tokens": "With the aid of a logic-based grammar formalism called [[ extraposition grammars ]] , << Chat-80 >> translates English questions into the Prolog subset of logic .", "h": ["extraposition grammars"], "t": ["Chat-80"]}, {"label": "USED-FOR", "tokens": "The resulting << logical expression >> is then transformed by a [[ planning algorithm ]] into efficient Prolog , cf. query optimisation in a relational database .", "h": ["planning algorithm"], "t": ["logical expression"]}, {"label": "USED-FOR", "tokens": "The resulting logical expression is then transformed by a planning algorithm into efficient Prolog , cf. << query optimisation >> in a [[ relational database ]] .", "h": ["relational database"], "t": ["query optimisation"]}, {"label": "USED-FOR", "tokens": "<< Human action recognition >> from [[ well-segmented 3D skeleton data ]] has been intensively studied and attracting an increasing attention .", "h": ["well-segmented 3D skeleton data"], "t": ["Human action recognition"]}, {"label": "USED-FOR", "tokens": "[[ Online action detection ]] goes one step further and is more challenging , which identifies the << action type >> and localizes the action positions on the fly from the untrimmed stream .", "h": ["Online action detection"], "t": ["action type"]}, {"label": "USED-FOR", "tokens": "[[ Online action detection ]] goes one step further and is more challenging , which identifies the action type and localizes the << action positions >> on the fly from the untrimmed stream .", "h": ["Online action detection"], "t": ["action positions"]}, {"label": "CONJUNCTION", "tokens": "Online action detection goes one step further and is more challenging , which identifies the [[ action type ]] and localizes the << action positions >> on the fly from the untrimmed stream .", "h": ["action type"], "t": ["action positions"]}, {"label": "USED-FOR", "tokens": "<< Online action detection >> goes one step further and is more challenging , which identifies the action type and localizes the action positions on the fly from the [[ untrimmed stream ]] .", "h": ["untrimmed stream"], "t": ["Online action detection"]}, {"label": "USED-FOR", "tokens": "In this paper , we study the problem of << online action detection >> from the [[ streaming skeleton data ]] .", "h": ["streaming skeleton data"], "t": ["online action detection"]}, {"label": "USED-FOR", "tokens": "We propose a [[ multi-task end-to-end Joint Classification-Regression Recurrent Neural Network ]] to better explore the << action type >> and temporal localiza-tion information .", "h": ["multi-task end-to-end Joint Classification-Regression Recurrent Neural Network"], "t": ["action type"]}, {"label": "USED-FOR", "tokens": "We propose a [[ multi-task end-to-end Joint Classification-Regression Recurrent Neural Network ]] to better explore the action type and << temporal localiza-tion information >> .", "h": ["multi-task end-to-end Joint Classification-Regression Recurrent Neural Network"], "t": ["temporal localiza-tion information"]}, {"label": "CONJUNCTION", "tokens": "We propose a multi-task end-to-end Joint Classification-Regression Recurrent Neural Network to better explore the [[ action type ]] and << temporal localiza-tion information >> .", "h": ["action type"], "t": ["temporal localiza-tion information"]}, {"label": "USED-FOR", "tokens": "By employing a [[ joint classification and regression optimization objective ]] , this << network >> is capable of automatically localizing the start and end points of actions more accurately .", "h": ["joint classification and regression optimization objective"], "t": ["network"]}, {"label": "USED-FOR", "tokens": "Specifically , by leveraging the merits of the [[ deep Long Short-Term Memory -LRB- LSTM -RRB- subnetwork ]] , the proposed << model >> automatically captures the complex long-range temporal dynamics , which naturally avoids the typical sliding window design and thus ensures high computational efficiency .", "h": ["deep Long Short-Term Memory -LRB- LSTM -RRB- subnetwork"], "t": ["model"]}, {"label": "FEATURE-OF", "tokens": "Specifically , by leveraging the merits of the deep Long Short-Term Memory -LRB- LSTM -RRB- subnetwork , the proposed << model >> automatically captures the complex [[ long-range temporal dynamics ]] , which naturally avoids the typical sliding window design and thus ensures high computational efficiency .", "h": ["long-range temporal dynamics"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "To evaluate our proposed << model >> , we build a large [[ streaming video dataset ]] with annotations .", "h": ["streaming video dataset"], "t": ["model"]}, {"label": "CONJUNCTION", "tokens": "Experimental results on our [[ dataset ]] and the public << G3D dataset >> both demonstrate very promising performance of our scheme .", "h": ["dataset"], "t": ["G3D dataset"]}, {"label": "CONJUNCTION", "tokens": "The task of [[ machine translation -LRB- MT -RRB- evaluation ]] is closely related to the task of << sentence-level semantic equivalence classification >> .", "h": ["machine translation -LRB- MT -RRB- evaluation"], "t": ["sentence-level semantic equivalence classification"]}, {"label": "USED-FOR", "tokens": "This paper investigates the utility of applying standard [[ MT evaluation methods ]] -LRB- BLEU , NIST , WER and PER -RRB- to building << classifiers >> to predict semantic equivalence and entailment .", "h": ["MT evaluation methods"], "t": ["classifiers"]}, {"label": "HYPONYM-OF", "tokens": "This paper investigates the utility of applying standard << MT evaluation methods >> -LRB- [[ BLEU ]] , NIST , WER and PER -RRB- to building classifiers to predict semantic equivalence and entailment .", "h": ["BLEU"], "t": ["MT evaluation methods"]}, {"label": "CONJUNCTION", "tokens": "This paper investigates the utility of applying standard MT evaluation methods -LRB- [[ BLEU ]] , << NIST >> , WER and PER -RRB- to building classifiers to predict semantic equivalence and entailment .", "h": ["BLEU"], "t": ["NIST"]}, {"label": "HYPONYM-OF", "tokens": "This paper investigates the utility of applying standard << MT evaluation methods >> -LRB- BLEU , [[ NIST ]] , WER and PER -RRB- to building classifiers to predict semantic equivalence and entailment .", "h": ["NIST"], "t": ["MT evaluation methods"]}, {"label": "CONJUNCTION", "tokens": "This paper investigates the utility of applying standard MT evaluation methods -LRB- BLEU , [[ NIST ]] , << WER >> and PER -RRB- to building classifiers to predict semantic equivalence and entailment .", "h": ["NIST"], "t": ["WER"]}, {"label": "HYPONYM-OF", "tokens": "This paper investigates the utility of applying standard << MT evaluation methods >> -LRB- BLEU , NIST , [[ WER ]] and PER -RRB- to building classifiers to predict semantic equivalence and entailment .", "h": ["WER"], "t": ["MT evaluation methods"]}, {"label": "CONJUNCTION", "tokens": "This paper investigates the utility of applying standard MT evaluation methods -LRB- BLEU , NIST , [[ WER ]] and << PER >> -RRB- to building classifiers to predict semantic equivalence and entailment .", "h": ["WER"], "t": ["PER"]}, {"label": "HYPONYM-OF", "tokens": "This paper investigates the utility of applying standard << MT evaluation methods >> -LRB- BLEU , NIST , WER and [[ PER ]] -RRB- to building classifiers to predict semantic equivalence and entailment .", "h": ["PER"], "t": ["MT evaluation methods"]}, {"label": "USED-FOR", "tokens": "This paper investigates the utility of applying standard MT evaluation methods -LRB- BLEU , NIST , WER and PER -RRB- to building [[ classifiers ]] to predict << semantic equivalence >> and entailment .", "h": ["classifiers"], "t": ["semantic equivalence"]}, {"label": "USED-FOR", "tokens": "This paper investigates the utility of applying standard MT evaluation methods -LRB- BLEU , NIST , WER and PER -RRB- to building [[ classifiers ]] to predict semantic equivalence and << entailment >> .", "h": ["classifiers"], "t": ["entailment"]}, {"label": "CONJUNCTION", "tokens": "This paper investigates the utility of applying standard MT evaluation methods -LRB- BLEU , NIST , WER and PER -RRB- to building classifiers to predict [[ semantic equivalence ]] and << entailment >> .", "h": ["semantic equivalence"], "t": ["entailment"]}, {"label": "USED-FOR", "tokens": "We also introduce a novel << classification method >> based on [[ PER ]] which leverages part of speech information of the words contributing to the word matches and non-matches in the sentence .", "h": ["PER"], "t": ["classification method"]}, {"label": "USED-FOR", "tokens": "We also introduce a novel classification method based on [[ PER ]] which leverages << part of speech information >> of the words contributing to the word matches and non-matches in the sentence .", "h": ["PER"], "t": ["part of speech information"]}, {"label": "USED-FOR", "tokens": "We also introduce a novel classification method based on PER which leverages [[ part of speech information ]] of the words contributing to the << word matches and non-matches >> in the sentence .", "h": ["part of speech information"], "t": ["word matches and non-matches"]}, {"label": "USED-FOR", "tokens": "Our results show that [[ MT evaluation techniques ]] are able to produce useful << features >> for paraphrase classification and to a lesser extent entailment .", "h": ["MT evaluation techniques"], "t": ["features"]}, {"label": "USED-FOR", "tokens": "Our results show that [[ MT evaluation techniques ]] are able to produce useful features for << paraphrase classification >> and to a lesser extent entailment .", "h": ["MT evaluation techniques"], "t": ["paraphrase classification"]}, {"label": "USED-FOR", "tokens": "Our results show that [[ MT evaluation techniques ]] are able to produce useful features for paraphrase classification and to a lesser extent << entailment >> .", "h": ["MT evaluation techniques"], "t": ["entailment"]}, {"label": "CONJUNCTION", "tokens": "Our results show that MT evaluation techniques are able to produce useful features for [[ paraphrase classification ]] and to a lesser extent << entailment >> .", "h": ["paraphrase classification"], "t": ["entailment"]}, {"label": "COMPARE", "tokens": "Our [[ technique ]] gives a substantial improvement in paraphrase classification accuracy over all of the other << models >> used in the experiments .", "h": ["technique"], "t": ["models"]}, {"label": "EVALUATE-FOR", "tokens": "Our << technique >> gives a substantial improvement in [[ paraphrase classification accuracy ]] over all of the other models used in the experiments .", "h": ["paraphrase classification accuracy"], "t": ["technique"]}, {"label": "EVALUATE-FOR", "tokens": "Our technique gives a substantial improvement in [[ paraphrase classification accuracy ]] over all of the other << models >> used in the experiments .", "h": ["paraphrase classification accuracy"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "Given an object model and a black-box measure of similarity between the model and candidate targets , we consider << visual object tracking >> as a [[ numerical optimization problem ]] .", "h": ["numerical optimization problem"], "t": ["visual object tracking"]}, {"label": "USED-FOR", "tokens": "During normal tracking conditions when the object is visible from frame to frame , [[ local optimization ]] is used to track the << local mode of the similarity measure >> in a parameter space of translation , rotation and scale .", "h": ["local optimization"], "t": ["local mode of the similarity measure"]}, {"label": "USED-FOR", "tokens": "During normal tracking conditions when the object is visible from frame to frame , local optimization is used to track the << local mode of the similarity measure >> in a [[ parameter space of translation , rotation and scale ]] .", "h": ["parameter space of translation , rotation and scale"], "t": ["local mode of the similarity measure"]}, {"label": "PART-OF", "tokens": "However , when the object becomes partially or totally occluded , such local tracking is prone to failure , especially when common << prediction techniques >> like the [[ Kalman filter ]] do not provide a good estimate of object parameters in future frames .", "h": ["Kalman filter"], "t": ["prediction techniques"]}, {"label": "USED-FOR", "tokens": "To recover from these inevitable tracking failures , we consider << object detection >> as a [[ global optimization problem ]] and solve it via Adaptive Simulated Annealing -LRB- ASA -RRB- , a method that avoids becoming trapped at local modes and is much faster than exhaustive search .", "h": ["global optimization problem"], "t": ["object detection"]}, {"label": "USED-FOR", "tokens": "To recover from these inevitable tracking failures , we consider object detection as a global optimization problem and solve << it >> via [[ Adaptive Simulated Annealing -LRB- ASA -RRB- ]] , a method that avoids becoming trapped at local modes and is much faster than exhaustive search .", "h": ["Adaptive Simulated Annealing -LRB- ASA -RRB-"], "t": ["it"]}, {"label": "COMPARE", "tokens": "To recover from these inevitable tracking failures , we consider object detection as a global optimization problem and solve it via Adaptive Simulated Annealing -LRB- ASA -RRB- , a [[ method ]] that avoids becoming trapped at local modes and is much faster than << exhaustive search >> .", "h": ["method"], "t": ["exhaustive search"]}, {"label": "HYPONYM-OF", "tokens": "As a << Monte Carlo approach >> , [[ ASA ]] stochastically samples the parameter space , in contrast to local deterministic search .", "h": ["ASA"], "t": ["Monte Carlo approach"]}, {"label": "COMPARE", "tokens": "As a Monte Carlo approach , [[ ASA ]] stochastically samples the parameter space , in contrast to << local deterministic search >> .", "h": ["ASA"], "t": ["local deterministic search"]}, {"label": "USED-FOR", "tokens": "We apply [[ cluster analysis ]] on the << sampled parameter space >> to redetect the object and renew the local tracker .", "h": ["cluster analysis"], "t": ["sampled parameter space"]}, {"label": "USED-FOR", "tokens": "We apply [[ cluster analysis ]] on the sampled parameter space to redetect the object and renew the << local tracker >> .", "h": ["cluster analysis"], "t": ["local tracker"]}, {"label": "EVALUATE-FOR", "tokens": "Our << numerical hybrid local and global mode-seeking tracker >> is validated on challenging [[ airborne videos ]] with heavy occlusion and large camera motions .", "h": ["airborne videos"], "t": ["numerical hybrid local and global mode-seeking tracker"]}, {"label": "FEATURE-OF", "tokens": "Our numerical hybrid local and global mode-seeking tracker is validated on challenging << airborne videos >> with [[ heavy occlusion ]] and large camera motions .", "h": ["heavy occlusion"], "t": ["airborne videos"]}, {"label": "CONJUNCTION", "tokens": "Our numerical hybrid local and global mode-seeking tracker is validated on challenging airborne videos with [[ heavy occlusion ]] and large << camera motions >> .", "h": ["heavy occlusion"], "t": ["camera motions"]}, {"label": "FEATURE-OF", "tokens": "Our numerical hybrid local and global mode-seeking tracker is validated on challenging << airborne videos >> with heavy occlusion and large [[ camera motions ]] .", "h": ["camera motions"], "t": ["airborne videos"]}, {"label": "COMPARE", "tokens": "Our << approach >> outperforms [[ state-of-the-art trackers ]] on the VIVID benchmark datasets .", "h": ["state-of-the-art trackers"], "t": ["approach"]}, {"label": "EVALUATE-FOR", "tokens": "Our << approach >> outperforms state-of-the-art trackers on the [[ VIVID benchmark datasets ]] .", "h": ["VIVID benchmark datasets"], "t": ["approach"]}, {"label": "EVALUATE-FOR", "tokens": "Our approach outperforms << state-of-the-art trackers >> on the [[ VIVID benchmark datasets ]] .", "h": ["VIVID benchmark datasets"], "t": ["state-of-the-art trackers"]}, {"label": "USED-FOR", "tokens": "[[ Techniques ]] for << automatically training modules >> of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .", "h": ["Techniques"], "t": ["automatically training modules"]}, {"label": "PART-OF", "tokens": "Techniques for [[ automatically training modules ]] of a << natural language generator >> have recently been proposed , but a fundamental concern is whether the quality of utterances produced with trainable components can compete with hand-crafted template-based or rule-based approaches .", "h": ["automatically training modules"], "t": ["natural language generator"]}, {"label": "EVALUATE-FOR", "tokens": "Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of [[ utterances ]] produced with << trainable components >> can compete with hand-crafted template-based or rule-based approaches .", "h": ["utterances"], "t": ["trainable components"]}, {"label": "EVALUATE-FOR", "tokens": "Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of [[ utterances ]] produced with trainable components can compete with << hand-crafted template-based or rule-based approaches >> .", "h": ["utterances"], "t": ["hand-crafted template-based or rule-based approaches"]}, {"label": "COMPARE", "tokens": "Techniques for automatically training modules of a natural language generator have recently been proposed , but a fundamental concern is whether the quality of utterances produced with [[ trainable components ]] can compete with << hand-crafted template-based or rule-based approaches >> .", "h": ["trainable components"], "t": ["hand-crafted template-based or rule-based approaches"]}, {"label": "USED-FOR", "tokens": "In this paper We experimentally evaluate a [[ trainable sentence planner ]] for a << spoken dialogue system >> by eliciting subjective human judgments .", "h": ["trainable sentence planner"], "t": ["spoken dialogue system"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper We experimentally evaluate a << trainable sentence planner >> for a spoken dialogue system by eliciting [[ subjective human judgments ]] .", "h": ["subjective human judgments"], "t": ["trainable sentence planner"]}, {"label": "CONJUNCTION", "tokens": "In order to perform an exhaustive comparison , we also evaluate a [[ hand-crafted template-based generation component ]] , two << rule-based sentence planners >> , and two baseline sentence planners .", "h": ["hand-crafted template-based generation component"], "t": ["rule-based sentence planners"]}, {"label": "CONJUNCTION", "tokens": "In order to perform an exhaustive comparison , we also evaluate a hand-crafted template-based generation component , two [[ rule-based sentence planners ]] , and two << baseline sentence planners >> .", "h": ["rule-based sentence planners"], "t": ["baseline sentence planners"]}, {"label": "COMPARE", "tokens": "We show that the [[ trainable sentence planner ]] performs better than the << rule-based systems >> and the baselines , and as well as the hand-crafted system .", "h": ["trainable sentence planner"], "t": ["rule-based systems"]}, {"label": "COMPARE", "tokens": "We show that the [[ trainable sentence planner ]] performs better than the rule-based systems and the << baselines >> , and as well as the hand-crafted system .", "h": ["trainable sentence planner"], "t": ["baselines"]}, {"label": "COMPARE", "tokens": "We show that the [[ trainable sentence planner ]] performs better than the rule-based systems and the baselines , and as well as the << hand-crafted system >> .", "h": ["trainable sentence planner"], "t": ["hand-crafted system"]}, {"label": "CONJUNCTION", "tokens": "We show that the trainable sentence planner performs better than the [[ rule-based systems ]] and the << baselines >> , and as well as the hand-crafted system .", "h": ["rule-based systems"], "t": ["baselines"]}, {"label": "CONJUNCTION", "tokens": "We show that the trainable sentence planner performs better than the rule-based systems and the [[ baselines ]] , and as well as the << hand-crafted system >> .", "h": ["baselines"], "t": ["hand-crafted system"]}, {"label": "USED-FOR", "tokens": "A new [[ algorithm ]] is proposed for << novel view generation >> in one-to-one teleconferencing applications .", "h": ["algorithm"], "t": ["novel view generation"]}, {"label": "USED-FOR", "tokens": "A new algorithm is proposed for [[ novel view generation ]] in << one-to-one teleconferencing applications >> .", "h": ["novel view generation"], "t": ["one-to-one teleconferencing applications"]}, {"label": "USED-FOR", "tokens": "Given the << video streams >> acquired by two [[ cameras ]] placed on either side of a computer monitor , the proposed algorithm synthesises images from a virtual camera in arbitrary position -LRB- typically located within the monitor -RRB- to facilitate eye contact .", "h": ["cameras"], "t": ["video streams"]}, {"label": "USED-FOR", "tokens": "Given the video streams acquired by two cameras placed on either side of a computer monitor , the proposed [[ algorithm ]] synthesises images from a virtual camera in arbitrary position -LRB- typically located within the monitor -RRB- to facilitate << eye contact >> .", "h": ["algorithm"], "t": ["eye contact"]}, {"label": "USED-FOR", "tokens": "Given the video streams acquired by two cameras placed on either side of a computer monitor , the proposed algorithm synthesises << images >> from a [[ virtual camera ]] in arbitrary position -LRB- typically located within the monitor -RRB- to facilitate eye contact .", "h": ["virtual camera"], "t": ["images"]}, {"label": "FEATURE-OF", "tokens": "Given the video streams acquired by two cameras placed on either side of a computer monitor , the proposed algorithm synthesises images from a << virtual camera >> in [[ arbitrary position ]] -LRB- typically located within the monitor -RRB- to facilitate eye contact .", "h": ["arbitrary position"], "t": ["virtual camera"]}, {"label": "USED-FOR", "tokens": "Our [[ technique ]] is based on an improved , dynamic-programming , stereo algorithm for efficient << novel-view generation >> .", "h": ["technique"], "t": ["novel-view generation"]}, {"label": "USED-FOR", "tokens": "Our << technique >> is based on an improved , [[ dynamic-programming , stereo algorithm ]] for efficient novel-view generation .", "h": ["dynamic-programming , stereo algorithm"], "t": ["technique"]}, {"label": "USED-FOR", "tokens": "The two main contributions of this paper are : i -RRB- a new type of [[ three-plane graph ]] for << dense-stereo dynamic-programming >> , that encourages correct occlusion labeling ; ii -RRB- a compact geometric derivation for novel-view synthesis by direct projection of the minimum-cost surface .", "h": ["three-plane graph"], "t": ["dense-stereo dynamic-programming"]}, {"label": "USED-FOR", "tokens": "The two main contributions of this paper are : i -RRB- a new type of three-plane graph for [[ dense-stereo dynamic-programming ]] , that encourages correct << occlusion labeling >> ; ii -RRB- a compact geometric derivation for novel-view synthesis by direct projection of the minimum-cost surface .", "h": ["dense-stereo dynamic-programming"], "t": ["occlusion labeling"]}, {"label": "USED-FOR", "tokens": "The two main contributions of this paper are : i -RRB- a new type of three-plane graph for dense-stereo dynamic-programming , that encourages correct occlusion labeling ; ii -RRB- a [[ compact geometric derivation ]] for << novel-view synthesis >> by direct projection of the minimum-cost surface .", "h": ["compact geometric derivation"], "t": ["novel-view synthesis"]}, {"label": "USED-FOR", "tokens": "The two main contributions of this paper are : i -RRB- a new type of three-plane graph for dense-stereo dynamic-programming , that encourages correct occlusion labeling ; ii -RRB- a << compact geometric derivation >> for novel-view synthesis by [[ direct projection of the minimum-cost surface ]] .", "h": ["direct projection of the minimum-cost surface"], "t": ["compact geometric derivation"]}, {"label": "USED-FOR", "tokens": "Furthermore , this paper presents a novel [[ algorithm ]] for the << temporal maintenance of a background model >> to enhance the rendering of occlusions and reduce temporal artefacts -LRB- flicker -RRB- ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .", "h": ["algorithm"], "t": ["temporal maintenance of a background model"]}, {"label": "USED-FOR", "tokens": "Furthermore , this paper presents a novel [[ algorithm ]] for the temporal maintenance of a background model to enhance the << rendering of occlusions >> and reduce temporal artefacts -LRB- flicker -RRB- ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .", "h": ["algorithm"], "t": ["rendering of occlusions"]}, {"label": "USED-FOR", "tokens": "Furthermore , this paper presents a novel [[ algorithm ]] for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce << temporal artefacts -LRB- flicker -RRB- >> ; and a cost aggregation algorithm that acts directly on our three-dimensional matching cost space .", "h": ["algorithm"], "t": ["temporal artefacts -LRB- flicker -RRB-"]}, {"label": "CONJUNCTION", "tokens": "Furthermore , this paper presents a novel << algorithm >> for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts -LRB- flicker -RRB- ; and a [[ cost aggregation algorithm ]] that acts directly on our three-dimensional matching cost space .", "h": ["cost aggregation algorithm"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "Furthermore , this paper presents a novel algorithm for the temporal maintenance of a background model to enhance the rendering of occlusions and reduce temporal artefacts -LRB- flicker -RRB- ; and a [[ cost aggregation algorithm ]] that acts directly on our << three-dimensional matching cost space >> .", "h": ["cost aggregation algorithm"], "t": ["three-dimensional matching cost space"]}, {"label": "EVALUATE-FOR", "tokens": "Examples are given that demonstrate the [[ robustness ]] of the new << algorithm >> to spatial and temporal artefacts for long stereo video streams .", "h": ["robustness"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "Examples are given that demonstrate the robustness of the new [[ algorithm ]] to << spatial and temporal artefacts >> for long stereo video streams .", "h": ["algorithm"], "t": ["spatial and temporal artefacts"]}, {"label": "USED-FOR", "tokens": "Examples are given that demonstrate the robustness of the new algorithm to [[ spatial and temporal artefacts ]] for << long stereo video streams >> .", "h": ["spatial and temporal artefacts"], "t": ["long stereo video streams"]}, {"label": "USED-FOR", "tokens": "We further demonstrate << synthesis >> from a freely [[ translating virtual camera ]] .", "h": ["translating virtual camera"], "t": ["synthesis"]}, {"label": "USED-FOR", "tokens": "To a large extent , these statistics reflect [[ semantic constraints ]] and thus are used to disambiguate << anaphora references >> and syntactic ambiguities .", "h": ["semantic constraints"], "t": ["anaphora references"]}, {"label": "USED-FOR", "tokens": "To a large extent , these statistics reflect [[ semantic constraints ]] and thus are used to disambiguate anaphora references and << syntactic ambiguities >> .", "h": ["semantic constraints"], "t": ["syntactic ambiguities"]}, {"label": "CONJUNCTION", "tokens": "To a large extent , these statistics reflect semantic constraints and thus are used to disambiguate [[ anaphora references ]] and << syntactic ambiguities >> .", "h": ["anaphora references"], "t": ["syntactic ambiguities"]}, {"label": "USED-FOR", "tokens": "The results of the experiment show that in most of the cases the [[ cooccurrence statistics ]] indeed reflect the semantic constraints and thus provide a basis for a useful << disambiguation tool >> .", "h": ["cooccurrence statistics"], "t": ["disambiguation tool"]}, {"label": "USED-FOR", "tokens": "We present a novel [[ method ]] for << discovering parallel sentences >> in comparable , non-parallel corpora .", "h": ["method"], "t": ["discovering parallel sentences"]}, {"label": "USED-FOR", "tokens": "We present a novel method for << discovering parallel sentences >> in [[ comparable , non-parallel corpora ]] .", "h": ["comparable , non-parallel corpora"], "t": ["discovering parallel sentences"]}, {"label": "USED-FOR", "tokens": "Using this [[ approach ]] , we extract << parallel data >> from large Chinese , Arabic , and English non-parallel newspaper corpora .", "h": ["approach"], "t": ["parallel data"]}, {"label": "PART-OF", "tokens": "Using this approach , we extract [[ parallel data ]] from large << Chinese , Arabic , and English non-parallel newspaper corpora >> .", "h": ["parallel data"], "t": ["Chinese , Arabic , and English non-parallel newspaper corpora"]}, {"label": "USED-FOR", "tokens": "We evaluate the quality of the extracted data by showing that [[ it ]] improves the performance of a state-of-the-art << statistical machine translation system >> .", "h": ["it"], "t": ["statistical machine translation system"]}, {"label": "USED-FOR", "tokens": "We also show that a good-quality << MT system >> can be built from scratch by starting with a very small [[ parallel corpus ]] -LRB- 100,000 words -RRB- and exploiting a large non-parallel corpus .", "h": ["parallel corpus"], "t": ["MT system"]}, {"label": "CONJUNCTION", "tokens": "We also show that a good-quality MT system can be built from scratch by starting with a very small [[ parallel corpus ]] -LRB- 100,000 words -RRB- and exploiting a large << non-parallel corpus >> .", "h": ["parallel corpus"], "t": ["non-parallel corpus"]}, {"label": "USED-FOR", "tokens": "We also show that a good-quality << MT system >> can be built from scratch by starting with a very small parallel corpus -LRB- 100,000 words -RRB- and exploiting a large [[ non-parallel corpus ]] .", "h": ["non-parallel corpus"], "t": ["MT system"]}, {"label": "USED-FOR", "tokens": "Thus , our << method >> can be applied with great benefit to language pairs for which only [[ scarce resources ]] are available .", "h": ["scarce resources"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe a [[ search procedure ]] for << statistical machine translation -LRB- MT -RRB- >> based on dynamic programming -LRB- DP -RRB- .", "h": ["search procedure"], "t": ["statistical machine translation -LRB- MT -RRB-"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe a search procedure for << statistical machine translation -LRB- MT -RRB- >> based on [[ dynamic programming -LRB- DP -RRB- ]] .", "h": ["dynamic programming -LRB- DP -RRB-"], "t": ["statistical machine translation -LRB- MT -RRB-"]}, {"label": "USED-FOR", "tokens": "Starting from a DP-based solution to the traveling salesman problem , we present a novel [[ technique ]] to restrict the possible word reordering between source and target language in order to achieve an efficient << search algorithm >> .", "h": ["technique"], "t": ["search algorithm"]}, {"label": "HYPONYM-OF", "tokens": "The experimental tests are carried out on the [[ Verbmobil task ]] -LRB- German-English , 8000-word vocabulary -RRB- , which is a << limited-domain spoken-language task >> .", "h": ["Verbmobil task"], "t": ["limited-domain spoken-language task"]}, {"label": "CONJUNCTION", "tokens": "A purely functional implementation of << LR-parsers >> is given , together with a simple [[ correctness proof ]] .", "h": ["correctness proof"], "t": ["LR-parsers"]}, {"label": "USED-FOR", "tokens": "<< It >> is presented as a generalization of the [[ recursive descent parser ]] .", "h": ["recursive descent parser"], "t": ["It"]}, {"label": "EVALUATE-FOR", "tokens": "For non-LR grammars the [[ time-complexity ]] of our << parser >> is cubic if the functions that constitute the parser are implemented as memo-functions , i.e. functions that memorize the results of previous invocations .", "h": ["time-complexity"], "t": ["parser"]}, {"label": "USED-FOR", "tokens": "For << non-LR grammars >> the time-complexity of our [[ parser ]] is cubic if the functions that constitute the parser are implemented as memo-functions , i.e. functions that memorize the results of previous invocations .", "h": ["parser"], "t": ["non-LR grammars"]}, {"label": "USED-FOR", "tokens": "For non-LR grammars the time-complexity of our parser is cubic if the functions that constitute the << parser >> are implemented as [[ memo-functions ]] , i.e. functions that memorize the results of previous invocations .", "h": ["memo-functions"], "t": ["parser"]}, {"label": "USED-FOR", "tokens": "[[ Memo-functions ]] also facilitate a simple way to construct a very compact representation of the << parse forest >> .", "h": ["Memo-functions"], "t": ["parse forest"]}, {"label": "USED-FOR", "tokens": "For << LR -LRB- 0 -RRB- grammars >> , our [[ algorithm ]] is closely related to the recursive ascent parsers recently discovered by Kruse-man Aretz -LSB- 1 -RSB- and Roberts -LSB- 2 -RSB- .", "h": ["algorithm"], "t": ["LR -LRB- 0 -RRB- grammars"]}, {"label": "CONJUNCTION", "tokens": "For LR -LRB- 0 -RRB- grammars , our [[ algorithm ]] is closely related to the << recursive ascent parsers >> recently discovered by Kruse-man Aretz -LSB- 1 -RSB- and Roberts -LSB- 2 -RSB- .", "h": ["algorithm"], "t": ["recursive ascent parsers"]}, {"label": "FEATURE-OF", "tokens": "Extended CF grammars -LRB- << grammars >> with [[ regular expressions ]] at the right hand side -RRB- can be parsed with a simple modification of the LR-parser for normal CF grammars .", "h": ["regular expressions"], "t": ["grammars"]}, {"label": "USED-FOR", "tokens": "<< Extended CF grammars >> -LRB- grammars with regular expressions at the right hand side -RRB- can be parsed with a simple modification of the [[ LR-parser ]] for normal CF grammars .", "h": ["LR-parser"], "t": ["Extended CF grammars"]}, {"label": "USED-FOR", "tokens": "Extended CF grammars -LRB- grammars with regular expressions at the right hand side -RRB- can be parsed with a simple modification of the [[ LR-parser ]] for normal << CF grammars >> .", "h": ["LR-parser"], "t": ["CF grammars"]}, {"label": "PART-OF", "tokens": "In this theory , << discourse structure >> is composed of three separate but interrelated [[ components ]] : the structure of the sequence of utterances -LRB- called the linguistic structure -RRB- , a structure of purposes -LRB- called the intentional structure -RRB- , and the state of focus of attention -LRB- called the attentional state -RRB- .", "h": ["components"], "t": ["discourse structure"]}, {"label": "PART-OF", "tokens": "In this theory , discourse structure is composed of three separate but interrelated << components >> : the structure of the sequence of utterances -LRB- called the [[ linguistic structure ]] -RRB- , a structure of purposes -LRB- called the intentional structure -RRB- , and the state of focus of attention -LRB- called the attentional state -RRB- .", "h": ["linguistic structure"], "t": ["components"]}, {"label": "CONJUNCTION", "tokens": "In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances -LRB- called the [[ linguistic structure ]] -RRB- , a structure of purposes -LRB- called the << intentional structure >> -RRB- , and the state of focus of attention -LRB- called the attentional state -RRB- .", "h": ["linguistic structure"], "t": ["intentional structure"]}, {"label": "PART-OF", "tokens": "In this theory , discourse structure is composed of three separate but interrelated << components >> : the structure of the sequence of utterances -LRB- called the linguistic structure -RRB- , a structure of purposes -LRB- called the [[ intentional structure ]] -RRB- , and the state of focus of attention -LRB- called the attentional state -RRB- .", "h": ["intentional structure"], "t": ["components"]}, {"label": "CONJUNCTION", "tokens": "In this theory , discourse structure is composed of three separate but interrelated components : the structure of the sequence of utterances -LRB- called the linguistic structure -RRB- , a structure of purposes -LRB- called the [[ intentional structure ]] -RRB- , and the state of focus of attention -LRB- called the << attentional state >> -RRB- .", "h": ["intentional structure"], "t": ["attentional state"]}, {"label": "PART-OF", "tokens": "In this theory , discourse structure is composed of three separate but interrelated << components >> : the structure of the sequence of utterances -LRB- called the linguistic structure -RRB- , a structure of purposes -LRB- called the intentional structure -RRB- , and the state of focus of attention -LRB- called the [[ attentional state ]] -RRB- .", "h": ["attentional state"], "t": ["components"]}, {"label": "USED-FOR", "tokens": "The [[ intentional structure ]] captures the << discourse-relevant purposes >> , expressed in each of the linguistic segments as well as relationships among them .", "h": ["intentional structure"], "t": ["discourse-relevant purposes"]}, {"label": "HYPONYM-OF", "tokens": "The distinction among these components is essential to provide an adequate explanation of such << discourse phenomena >> as [[ cue phrases ]] , referring expressions , and interruptions .", "h": ["cue phrases"], "t": ["discourse phenomena"]}, {"label": "CONJUNCTION", "tokens": "The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as [[ cue phrases ]] , << referring expressions >> , and interruptions .", "h": ["cue phrases"], "t": ["referring expressions"]}, {"label": "HYPONYM-OF", "tokens": "The distinction among these components is essential to provide an adequate explanation of such << discourse phenomena >> as cue phrases , [[ referring expressions ]] , and interruptions .", "h": ["referring expressions"], "t": ["discourse phenomena"]}, {"label": "CONJUNCTION", "tokens": "The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases , [[ referring expressions ]] , and << interruptions >> .", "h": ["referring expressions"], "t": ["interruptions"]}, {"label": "HYPONYM-OF", "tokens": "The distinction among these components is essential to provide an adequate explanation of such << discourse phenomena >> as cue phrases , referring expressions , and [[ interruptions ]] .", "h": ["interruptions"], "t": ["discourse phenomena"]}, {"label": "HYPONYM-OF", "tokens": "We examine the relationship between the two << grammatical formalisms >> : [[ Tree Adjoining Grammars ]] and Head Grammars .", "h": ["Tree Adjoining Grammars"], "t": ["grammatical formalisms"]}, {"label": "COMPARE", "tokens": "We examine the relationship between the two grammatical formalisms : [[ Tree Adjoining Grammars ]] and << Head Grammars >> .", "h": ["Tree Adjoining Grammars"], "t": ["Head Grammars"]}, {"label": "HYPONYM-OF", "tokens": "We examine the relationship between the two << grammatical formalisms >> : Tree Adjoining Grammars and [[ Head Grammars ]] .", "h": ["Head Grammars"], "t": ["grammatical formalisms"]}, {"label": "FEATURE-OF", "tokens": "We then turn to a discussion comparing the [[ linguistic expressiveness ]] of the two << formalisms >> .", "h": ["linguistic expressiveness"], "t": ["formalisms"]}, {"label": "USED-FOR", "tokens": "We provide a unified account of << sentence-level and text-level anaphora >> within the framework of a [[ dependency-based grammar model ]] .", "h": ["dependency-based grammar model"], "t": ["sentence-level and text-level anaphora"]}, {"label": "USED-FOR", "tokens": "[[ Criteria ]] for << anaphora resolution within sentence boundaries >> rephrase major concepts from GB 's binding theory , while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model .", "h": ["Criteria"], "t": ["anaphora resolution within sentence boundaries"]}, {"label": "USED-FOR", "tokens": "<< Criteria >> for anaphora resolution within sentence boundaries rephrase major concepts from [[ GB 's binding theory ]] , while those for text-level anaphora incorporate an adapted version of a Grosz-Sidner-style focus model .", "h": ["GB 's binding theory"], "t": ["Criteria"]}, {"label": "USED-FOR", "tokens": "Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB 's binding theory , while [[ those ]] for << text-level anaphora >> incorporate an adapted version of a Grosz-Sidner-style focus model .", "h": ["those"], "t": ["text-level anaphora"]}, {"label": "PART-OF", "tokens": "Criteria for anaphora resolution within sentence boundaries rephrase major concepts from GB 's binding theory , while << those >> for text-level anaphora incorporate an adapted version of a [[ Grosz-Sidner-style focus model ]] .", "h": ["Grosz-Sidner-style focus model"], "t": ["those"]}, {"label": "USED-FOR", "tokens": "[[ Coedition ]] of a natural language text and its representation in some interlingual form seems the best and simplest way to share << text revision >> across languages .", "h": ["Coedition"], "t": ["text revision"]}, {"label": "USED-FOR", "tokens": "<< Coedition >> of a [[ natural language text ]] and its representation in some interlingual form seems the best and simplest way to share text revision across languages .", "h": ["natural language text"], "t": ["Coedition"]}, {"label": "USED-FOR", "tokens": "The modified [[ graph ]] is then sent to the << UNL-L0 deconverter >> and the result shown .", "h": ["graph"], "t": ["UNL-L0 deconverter"]}, {"label": "USED-FOR", "tokens": "On the internal side , << liaisons >> are established between elements of the text and the graph by using broadly available [[ resources ]] such as a LO-English or better a L0-UNL dictionary , a morphosyntactic parser of L0 , and a canonical graph2tree transformation .", "h": ["resources"], "t": ["liaisons"]}, {"label": "HYPONYM-OF", "tokens": "On the internal side , liaisons are established between elements of the text and the graph by using broadly available << resources >> such as a [[ LO-English or better a L0-UNL dictionary ]] , a morphosyntactic parser of L0 , and a canonical graph2tree transformation .", "h": ["LO-English or better a L0-UNL dictionary"], "t": ["resources"]}, {"label": "CONJUNCTION", "tokens": "On the internal side , liaisons are established between elements of the text and the graph by using broadly available resources such as a [[ LO-English or better a L0-UNL dictionary ]] , a << morphosyntactic parser of L0 >> , and a canonical graph2tree transformation .", "h": ["LO-English or better a L0-UNL dictionary"], "t": ["morphosyntactic parser of L0"]}, {"label": "HYPONYM-OF", "tokens": "On the internal side , liaisons are established between elements of the text and the graph by using broadly available << resources >> such as a LO-English or better a L0-UNL dictionary , a [[ morphosyntactic parser of L0 ]] , and a canonical graph2tree transformation .", "h": ["morphosyntactic parser of L0"], "t": ["resources"]}, {"label": "CONJUNCTION", "tokens": "On the internal side , liaisons are established between elements of the text and the graph by using broadly available resources such as a LO-English or better a L0-UNL dictionary , a [[ morphosyntactic parser of L0 ]] , and a << canonical graph2tree transformation >> .", "h": ["morphosyntactic parser of L0"], "t": ["canonical graph2tree transformation"]}, {"label": "HYPONYM-OF", "tokens": "On the internal side , liaisons are established between elements of the text and the graph by using broadly available << resources >> such as a LO-English or better a L0-UNL dictionary , a morphosyntactic parser of L0 , and a [[ canonical graph2tree transformation ]] .", "h": ["canonical graph2tree transformation"], "t": ["resources"]}, {"label": "CONJUNCTION", "tokens": "Establishing a `` best '' correspondence between the '' [[ UNL-tree + L0 ]] '' and the '' << MS-L0 structure >> '' , a lattice , may be done using the dictionary and trying to align the tree and the selected trajectory with as few crossing liaisons as possible .", "h": ["UNL-tree + L0"], "t": ["MS-L0 structure"]}, {"label": "USED-FOR", "tokens": "Establishing a `` best '' correspondence between the '' UNL-tree + L0 '' and the '' MS-L0 structure '' , a << lattice >> , may be done using the [[ dictionary ]] and trying to align the tree and the selected trajectory with as few crossing liaisons as possible .", "h": ["dictionary"], "t": ["lattice"]}, {"label": "CONJUNCTION", "tokens": "A central goal of this research is to merge approaches from [[ pivot MT ]] , << interactive MT >> , and multilingual text authoring .", "h": ["pivot MT"], "t": ["interactive MT"]}, {"label": "CONJUNCTION", "tokens": "A central goal of this research is to merge approaches from pivot MT , [[ interactive MT ]] , and << multilingual text authoring >> .", "h": ["interactive MT"], "t": ["multilingual text authoring"]}, {"label": "EVALUATE-FOR", "tokens": "We report experiments conducted on a [[ multilingual corpus ]] to estimate the number of << analogies >> among the sentences that it contains .", "h": ["multilingual corpus"], "t": ["analogies"]}, {"label": "USED-FOR", "tokens": "Our goal is to learn a << Mahalanobis distance >> by minimizing a [[ loss ]] defined on the weighted sum of the precision at different ranks .", "h": ["loss"], "t": ["Mahalanobis distance"]}, {"label": "FEATURE-OF", "tokens": "Our goal is to learn a Mahalanobis distance by minimizing a loss defined on the [[ weighted sum ]] of the << precision >> at different ranks .", "h": ["weighted sum"], "t": ["precision"]}, {"label": "USED-FOR", "tokens": "Our core motivation is that minimizing a [[ weighted rank loss ]] is a natural criterion for many problems in << computer vision >> such as person re-identification .", "h": ["weighted rank loss"], "t": ["computer vision"]}, {"label": "USED-FOR", "tokens": "Our core motivation is that minimizing a [[ weighted rank loss ]] is a natural criterion for many problems in computer vision such as << person re-identification >> .", "h": ["weighted rank loss"], "t": ["person re-identification"]}, {"label": "HYPONYM-OF", "tokens": "Our core motivation is that minimizing a weighted rank loss is a natural criterion for many problems in << computer vision >> such as [[ person re-identification ]] .", "h": ["person re-identification"], "t": ["computer vision"]}, {"label": "HYPONYM-OF", "tokens": "We propose a novel << metric learning formulation >> called [[ Weighted Approximate Rank Component Analysis -LRB- WARCA -RRB- ]] .", "h": ["Weighted Approximate Rank Component Analysis -LRB- WARCA -RRB-"], "t": ["metric learning formulation"]}, {"label": "USED-FOR", "tokens": "We then derive a scalable [[ stochastic gradient descent algorithm ]] for the resulting << learning problem >> .", "h": ["stochastic gradient descent algorithm"], "t": ["learning problem"]}, {"label": "USED-FOR", "tokens": "We also derive an efficient << non-linear extension of WARCA >> by using the [[ kernel trick ]] .", "h": ["kernel trick"], "t": ["non-linear extension of WARCA"]}, {"label": "USED-FOR", "tokens": "[[ Kernel space embedding ]] decouples the training and prediction costs from the data dimension and enables us to plug << inarbitrary distance measures >> which are more natural for the features .", "h": ["Kernel space embedding"], "t": ["inarbitrary distance measures"]}, {"label": "CONJUNCTION", "tokens": "We also address a more general problem of [[ matrix rank degeneration ]] & << non-isolated minima >> in the low-rank matrix optimization by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently .", "h": ["matrix rank degeneration"], "t": ["non-isolated minima"]}, {"label": "FEATURE-OF", "tokens": "We also address a more general problem of [[ matrix rank degeneration ]] & non-isolated minima in the << low-rank matrix optimization >> by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently .", "h": ["matrix rank degeneration"], "t": ["low-rank matrix optimization"]}, {"label": "FEATURE-OF", "tokens": "We also address a more general problem of matrix rank degeneration & [[ non-isolated minima ]] in the << low-rank matrix optimization >> by using new type of regularizer which approximately enforces the or-thonormality of the learned matrix very efficiently .", "h": ["non-isolated minima"], "t": ["low-rank matrix optimization"]}, {"label": "USED-FOR", "tokens": "We also address a more general problem of matrix rank degeneration & non-isolated minima in the << low-rank matrix optimization >> by using new type of [[ regularizer ]] which approximately enforces the or-thonormality of the learned matrix very efficiently .", "h": ["regularizer"], "t": ["low-rank matrix optimization"]}, {"label": "USED-FOR", "tokens": "We also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of [[ regularizer ]] which approximately enforces the << or-thonormality >> of the learned matrix very efficiently .", "h": ["regularizer"], "t": ["or-thonormality"]}, {"label": "FEATURE-OF", "tokens": "We also address a more general problem of matrix rank degeneration & non-isolated minima in the low-rank matrix optimization by using new type of regularizer which approximately enforces the [[ or-thonormality ]] of the << learned matrix >> very efficiently .", "h": ["or-thonormality"], "t": ["learned matrix"]}, {"label": "EVALUATE-FOR", "tokens": "We validate this new << method >> on nine standard [[ person re-identification datasets ]] including two large scale Market-1501 and CUHK03 datasets and show that we improve upon the current state-of-the-art methods on all of them .", "h": ["person re-identification datasets"], "t": ["method"]}, {"label": "HYPONYM-OF", "tokens": "We validate this new method on nine standard << person re-identification datasets >> including two large [[ scale Market-1501 ]] and CUHK03 datasets and show that we improve upon the current state-of-the-art methods on all of them .", "h": ["scale Market-1501"], "t": ["person re-identification datasets"]}, {"label": "HYPONYM-OF", "tokens": "We validate this new method on nine standard << person re-identification datasets >> including two large scale Market-1501 and [[ CUHK03 datasets ]] and show that we improve upon the current state-of-the-art methods on all of them .", "h": ["CUHK03 datasets"], "t": ["person re-identification datasets"]}, {"label": "CONJUNCTION", "tokens": "We validate this new method on nine standard person re-identification datasets including two large << scale Market-1501 >> and [[ CUHK03 datasets ]] and show that we improve upon the current state-of-the-art methods on all of them .", "h": ["CUHK03 datasets"], "t": ["scale Market-1501"]}, {"label": "USED-FOR", "tokens": "In this paper , we discuss << language model adaptation methods >> given a [[ word list ]] and a raw corpus .", "h": ["word list"], "t": ["language model adaptation methods"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we discuss language model adaptation methods given a [[ word list ]] and a << raw corpus >> .", "h": ["word list"], "t": ["raw corpus"]}, {"label": "USED-FOR", "tokens": "In this paper , we discuss << language model adaptation methods >> given a word list and a [[ raw corpus ]] .", "h": ["raw corpus"], "t": ["language model adaptation methods"]}, {"label": "USED-FOR", "tokens": "In this situation , the general [[ method ]] is to segment the << raw corpus >> automatically using a word list , correct the output sentences by hand , and build a model from the segmented corpus .", "h": ["method"], "t": ["raw corpus"]}, {"label": "USED-FOR", "tokens": "In this situation , the general << method >> is to segment the raw corpus automatically using a [[ word list ]] , correct the output sentences by hand , and build a model from the segmented corpus .", "h": ["word list"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "In this situation , the general method is to segment the raw corpus automatically using a word list , correct the output sentences by hand , and build a << model >> from the [[ segmented corpus ]] .", "h": ["segmented corpus"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "In the experiments , we used a variety of [[ methods ]] for << preparing a segmented corpus >> and compared the language models by their speech recognition accuracies .", "h": ["methods"], "t": ["preparing a segmented corpus"]}, {"label": "EVALUATE-FOR", "tokens": "In the experiments , we used a variety of methods for preparing a segmented corpus and compared the << language models >> by their [[ speech recognition accuracies ]] .", "h": ["speech recognition accuracies"], "t": ["language models"]}, {"label": "USED-FOR", "tokens": "Many practical << modeling problems >> involve [[ discrete data ]] that are best represented as draws from multinomial or categorical distributions .", "h": ["discrete data"], "t": ["modeling problems"]}, {"label": "USED-FOR", "tokens": "Many practical << modeling problems >> involve discrete data that are best represented as draws from [[ multinomial or categorical distributions ]] .", "h": ["multinomial or categorical distributions"], "t": ["modeling problems"]}, {"label": "USED-FOR", "tokens": "For example , << nucleotides in a DNA sequence >> , children 's names in a given state and year , and text documents are all commonly modeled with [[ multinomial distributions ]] .", "h": ["multinomial distributions"], "t": ["nucleotides in a DNA sequence"]}, {"label": "USED-FOR", "tokens": "For example , nucleotides in a DNA sequence , children 's names in a given state and year , and << text documents >> are all commonly modeled with [[ multinomial distributions ]] .", "h": ["multinomial distributions"], "t": ["text documents"]}, {"label": "USED-FOR", "tokens": "Here , we leverage a [[ logistic stick-breaking representation ]] and recent innovations in P\u00f3lya-gamma augmentation to reformu-late the << multinomial distribution >> in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .", "h": ["logistic stick-breaking representation"], "t": ["multinomial distribution"]}, {"label": "USED-FOR", "tokens": "Here , we leverage a logistic stick-breaking representation and recent innovations in [[ P\u00f3lya-gamma augmentation ]] to reformu-late the << multinomial distribution >> in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .", "h": ["P\u00f3lya-gamma augmentation"], "t": ["multinomial distribution"]}, {"label": "PART-OF", "tokens": "Here , we leverage a logistic stick-breaking representation and recent innovations in P\u00f3lya-gamma augmentation to reformu-late the << multinomial distribution >> in terms of [[ latent variables ]] with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .", "h": ["latent variables"], "t": ["multinomial distribution"]}, {"label": "FEATURE-OF", "tokens": "Here , we leverage a logistic stick-breaking representation and recent innovations in P\u00f3lya-gamma augmentation to reformu-late the multinomial distribution in terms of << latent variables >> with [[ jointly Gaussian likelihoods ]] , enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead .", "h": ["jointly Gaussian likelihoods"], "t": ["latent variables"]}, {"label": "USED-FOR", "tokens": "Here , we leverage a logistic stick-breaking representation and recent innovations in P\u00f3lya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of [[ Bayesian inference techniques ]] for << Gaussian models >> with minimal overhead .", "h": ["Bayesian inference techniques"], "t": ["Gaussian models"]}, {"label": "FEATURE-OF", "tokens": "Here , we leverage a logistic stick-breaking representation and recent innovations in P\u00f3lya-gamma augmentation to reformu-late the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods , enabling us to take advantage of a host of Bayesian inference techniques for << Gaussian models >> with [[ minimal overhead ]] .", "h": ["minimal overhead"], "t": ["Gaussian models"]}, {"label": "HYPONYM-OF", "tokens": "[[ MINPRAN ]] , a new << robust operator >> , nds good ts in data sets where more than 50 % of the points are outliers .", "h": ["MINPRAN"], "t": ["robust operator"]}, {"label": "USED-FOR", "tokens": "Unlike other [[ techniques ]] that handle << large outlier percentages >> , MINPRAN does not rely on a known error bound for the good data .", "h": ["techniques"], "t": ["large outlier percentages"]}, {"label": "COMPARE", "tokens": "Unlike other [[ techniques ]] that handle large outlier percentages , << MINPRAN >> does not rely on a known error bound for the good data .", "h": ["techniques"], "t": ["MINPRAN"]}, {"label": "USED-FOR", "tokens": "Based on this , << MINPRAN >> uses [[ random sampling ]] to search for the t and the number of inliers to the t that are least likely to have occurred randomly .", "h": ["random sampling"], "t": ["MINPRAN"]}, {"label": "EVALUATE-FOR", "tokens": "<< MINPRAN >> 's properties are connrmed experimentally on [[ synthetic data ]] and compare favorably to least median of squares .", "h": ["synthetic data"], "t": ["MINPRAN"]}, {"label": "COMPARE", "tokens": "<< MINPRAN >> 's properties are connrmed experimentally on synthetic data and compare favorably to [[ least median of squares ]] .", "h": ["least median of squares"], "t": ["MINPRAN"]}, {"label": "USED-FOR", "tokens": "Related work applies [[ MINPRAN ]] to << complex range >> and intensity data 23 -RSB- .", "h": ["MINPRAN"], "t": ["complex range"]}, {"label": "USED-FOR", "tokens": "Related work applies [[ MINPRAN ]] to complex range and << intensity data >> 23 -RSB- .", "h": ["MINPRAN"], "t": ["intensity data"]}, {"label": "PART-OF", "tokens": "<< Metagrammatical formalisms >> that combine [[ context-free phrase structure rules ]] and metarules -LRB- MPS grammars -RRB- allow concise statement of generalizations about the syntax of natural languages .", "h": ["context-free phrase structure rules"], "t": ["Metagrammatical formalisms"]}, {"label": "CONJUNCTION", "tokens": "Metagrammatical formalisms that combine [[ context-free phrase structure rules ]] and << metarules -LRB- MPS grammars -RRB- >> allow concise statement of generalizations about the syntax of natural languages .", "h": ["context-free phrase structure rules"], "t": ["metarules -LRB- MPS grammars -RRB-"]}, {"label": "PART-OF", "tokens": "<< Metagrammatical formalisms >> that combine context-free phrase structure rules and [[ metarules -LRB- MPS grammars -RRB- ]] allow concise statement of generalizations about the syntax of natural languages .", "h": ["metarules -LRB- MPS grammars -RRB-"], "t": ["Metagrammatical formalisms"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate several proposals for constraining << them >> , basing our assessment on [[ computational tractability and explanatory adequacy ]] .", "h": ["computational tractability and explanatory adequacy"], "t": ["them"]}, {"label": "USED-FOR", "tokens": "The unique properties of tree-adjoining grammars -LRB- TAG -RRB- present a challenge for the application of [[ TAGs ]] beyond the limited confines of syntax , for instance , to the task of << semantic interpretation >> or automatic translation of natural language .", "h": ["TAGs"], "t": ["semantic interpretation"]}, {"label": "USED-FOR", "tokens": "The unique properties of tree-adjoining grammars -LRB- TAG -RRB- present a challenge for the application of [[ TAGs ]] beyond the limited confines of syntax , for instance , to the task of semantic interpretation or << automatic translation of natural language >> .", "h": ["TAGs"], "t": ["automatic translation of natural language"]}, {"label": "CONJUNCTION", "tokens": "The unique properties of tree-adjoining grammars -LRB- TAG -RRB- present a challenge for the application of TAGs beyond the limited confines of syntax , for instance , to the task of [[ semantic interpretation ]] or << automatic translation of natural language >> .", "h": ["semantic interpretation"], "t": ["automatic translation of natural language"]}, {"label": "USED-FOR", "tokens": "The formalism 's intended usage is to relate expressions of natural languages to their associated << semantics >> represented in a [[ logical form language ]] , or to their translates in another natural language ; in summary , we intend it to allow TAGs to be used beyond their role in syntax proper .", "h": ["logical form language"], "t": ["semantics"]}, {"label": "USED-FOR", "tokens": "The formalism 's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language , or to their translates in another natural language ; in summary , we intend it to allow [[ TAGs ]] to be used beyond their role in << syntax proper >> .", "h": ["TAGs"], "t": ["syntax proper"]}, {"label": "USED-FOR", "tokens": "A [[ model-based approach ]] to << on-line cursive handwriting analysis and recognition >> is presented and evaluated .", "h": ["model-based approach"], "t": ["on-line cursive handwriting analysis and recognition"]}, {"label": "USED-FOR", "tokens": "In this [[ model ]] , << on-line handwriting >> is considered as a modulation of a simple cycloidal pen motion , described by two coupled oscillations with a constant linear drift along the line of the writing .", "h": ["model"], "t": ["on-line handwriting"]}, {"label": "PART-OF", "tokens": "In this model , [[ on-line handwriting ]] is considered as a modulation of a simple << cycloidal pen motion >> , described by two coupled oscillations with a constant linear drift along the line of the writing .", "h": ["on-line handwriting"], "t": ["cycloidal pen motion"]}, {"label": "USED-FOR", "tokens": "A general procedure for the estimation and quantization of these [[ cycloidal motion parameters ]] for << arbitrary handwriting >> is presented .", "h": ["cycloidal motion parameters"], "t": ["arbitrary handwriting"]}, {"label": "USED-FOR", "tokens": "The result is a [[ discrete motor control representation ]] of the << continuous pen motion >> , via the quantized levels of the model parameters .", "h": ["discrete motor control representation"], "t": ["continuous pen motion"]}, {"label": "USED-FOR", "tokens": "This [[ motor control representation ]] enables successful << word spotting >> and matching of cursive scripts .", "h": ["motor control representation"], "t": ["word spotting"]}, {"label": "USED-FOR", "tokens": "This [[ motor control representation ]] enables successful word spotting and << matching of cursive scripts >> .", "h": ["motor control representation"], "t": ["matching of cursive scripts"]}, {"label": "CONJUNCTION", "tokens": "This motor control representation enables successful [[ word spotting ]] and << matching of cursive scripts >> .", "h": ["word spotting"], "t": ["matching of cursive scripts"]}, {"label": "USED-FOR", "tokens": "Our experiments clearly indicate the potential of this [[ dynamic representation ]] for complete << cursive handwriting recognition >> .", "h": ["dynamic representation"], "t": ["cursive handwriting recognition"]}, {"label": "PART-OF", "tokens": "In the << Object Recognition task >> , there exists a di-chotomy between the [[ categorization of objects ]] and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .", "h": ["categorization of objects"], "t": ["Object Recognition task"]}, {"label": "CONJUNCTION", "tokens": "In the Object Recognition task , there exists a di-chotomy between the [[ categorization of objects ]] and << estimating object pose >> , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .", "h": ["categorization of objects"], "t": ["estimating object pose"]}, {"label": "PART-OF", "tokens": "In the << Object Recognition task >> , there exists a di-chotomy between the categorization of objects and [[ estimating object pose ]] , where the former necessitates a view-invariant representation , while the latter requires a representation capable of capturing pose information over different categories of objects .", "h": ["estimating object pose"], "t": ["Object Recognition task"]}, {"label": "USED-FOR", "tokens": "In the Object Recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the << former >> necessitates a [[ view-invariant representation ]] , while the latter requires a representation capable of capturing pose information over different categories of objects .", "h": ["view-invariant representation"], "t": ["former"]}, {"label": "USED-FOR", "tokens": "In the Object Recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the << latter >> requires a [[ representation ]] capable of capturing pose information over different categories of objects .", "h": ["representation"], "t": ["latter"]}, {"label": "USED-FOR", "tokens": "In the Object Recognition task , there exists a di-chotomy between the categorization of objects and estimating object pose , where the former necessitates a view-invariant representation , while the latter requires a [[ representation ]] capable of capturing << pose information >> over different categories of objects .", "h": ["representation"], "t": ["pose information"]}, {"label": "USED-FOR", "tokens": "With the rise of [[ deep archi-tectures ]] , the prime focus has been on << object category recognition >> .", "h": ["deep archi-tectures"], "t": ["object category recognition"]}, {"label": "USED-FOR", "tokens": "In contrast , << object pose estimation >> using these [[ approaches ]] has received relatively less attention .", "h": ["approaches"], "t": ["object pose estimation"]}, {"label": "USED-FOR", "tokens": "In this work , we study how [[ Convolutional Neural Networks -LRB- CNN -RRB- architectures ]] can be adapted to the task of simultaneous << object recognition >> and pose estimation .", "h": ["Convolutional Neural Networks -LRB- CNN -RRB- architectures"], "t": ["object recognition"]}, {"label": "USED-FOR", "tokens": "In this work , we study how [[ Convolutional Neural Networks -LRB- CNN -RRB- architectures ]] can be adapted to the task of simultaneous object recognition and << pose estimation >> .", "h": ["Convolutional Neural Networks -LRB- CNN -RRB- architectures"], "t": ["pose estimation"]}, {"label": "CONJUNCTION", "tokens": "In this work , we study how Convolutional Neural Networks -LRB- CNN -RRB- architectures can be adapted to the task of simultaneous [[ object recognition ]] and << pose estimation >> .", "h": ["object recognition"], "t": ["pose estimation"]}, {"label": "PART-OF", "tokens": "We investigate and analyze the [[ layers ]] of various << CNN models >> and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how this contradicts with object category representations .", "h": ["layers"], "t": ["CNN models"]}, {"label": "PART-OF", "tokens": "We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the [[ layers of distributed representations ]] within << CNNs >> represent object pose information and how this contradicts with object category representations .", "h": ["layers of distributed representations"], "t": ["CNNs"]}, {"label": "USED-FOR", "tokens": "We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the [[ layers of distributed representations ]] within CNNs represent << object pose information >> and how this contradicts with object category representations .", "h": ["layers of distributed representations"], "t": ["object pose information"]}, {"label": "COMPARE", "tokens": "We investigate and analyze the layers of various CNN models and extensively compare between them with the goal of discovering how the layers of distributed representations within CNNs represent object pose information and how [[ this ]] contradicts with << object category representations >> .", "h": ["this"], "t": ["object category representations"]}, {"label": "USED-FOR", "tokens": "[[ It ]] is particularly valuable to << empirical MT research >> .", "h": ["It"], "t": ["empirical MT research"]}, {"label": "USED-FOR", "tokens": "In this paper , we explore [[ geometric structures of 3D lines ]] in ray space for improving << light field triangulation >> and stereo matching .", "h": ["geometric structures of 3D lines"], "t": ["light field triangulation"]}, {"label": "USED-FOR", "tokens": "In this paper , we explore [[ geometric structures of 3D lines ]] in ray space for improving light field triangulation and << stereo matching >> .", "h": ["geometric structures of 3D lines"], "t": ["stereo matching"]}, {"label": "FEATURE-OF", "tokens": "In this paper , we explore << geometric structures of 3D lines >> in [[ ray space ]] for improving light field triangulation and stereo matching .", "h": ["ray space"], "t": ["geometric structures of 3D lines"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we explore geometric structures of 3D lines in ray space for improving [[ light field triangulation ]] and << stereo matching >> .", "h": ["light field triangulation"], "t": ["stereo matching"]}, {"label": "USED-FOR", "tokens": "Such a [[ triangulation ]] provides a << piecewise-linear interpolant >> useful for light field super-resolution .", "h": ["triangulation"], "t": ["piecewise-linear interpolant"]}, {"label": "USED-FOR", "tokens": "Such a triangulation provides a [[ piecewise-linear interpolant ]] useful for << light field super-resolution >> .", "h": ["piecewise-linear interpolant"], "t": ["light field super-resolution"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on [[ synthetic and real data ]] show that both our << triangulation and LAGC algorithms >> outperform state-of-the-art solutions in accuracy and visual quality .", "h": ["synthetic and real data"], "t": ["triangulation and LAGC algorithms"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on [[ synthetic and real data ]] show that both our triangulation and LAGC algorithms outperform << state-of-the-art solutions >> in accuracy and visual quality .", "h": ["synthetic and real data"], "t": ["state-of-the-art solutions"]}, {"label": "COMPARE", "tokens": "Experiments on synthetic and real data show that both our [[ triangulation and LAGC algorithms ]] outperform << state-of-the-art solutions >> in accuracy and visual quality .", "h": ["triangulation and LAGC algorithms"], "t": ["state-of-the-art solutions"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on synthetic and real data show that both our << triangulation and LAGC algorithms >> outperform state-of-the-art solutions in [[ accuracy ]] and visual quality .", "h": ["accuracy"], "t": ["triangulation and LAGC algorithms"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform << state-of-the-art solutions >> in [[ accuracy ]] and visual quality .", "h": ["accuracy"], "t": ["state-of-the-art solutions"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on synthetic and real data show that both our << triangulation and LAGC algorithms >> outperform state-of-the-art solutions in accuracy and [[ visual quality ]] .", "h": ["visual quality"], "t": ["triangulation and LAGC algorithms"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on synthetic and real data show that both our triangulation and LAGC algorithms outperform << state-of-the-art solutions >> in accuracy and [[ visual quality ]] .", "h": ["visual quality"], "t": ["state-of-the-art solutions"]}, {"label": "USED-FOR", "tokens": "This paper presents a << phrase-based statistical machine translation method >> , based on [[ non-contiguous phrases ]] , i.e. phrases with gaps .", "h": ["non-contiguous phrases"], "t": ["phrase-based statistical machine translation method"]}, {"label": "USED-FOR", "tokens": "A [[ method ]] for producing such << phrases >> from a word-aligned corpora is proposed .", "h": ["method"], "t": ["phrases"]}, {"label": "EVALUATE-FOR", "tokens": "A << method >> for producing such phrases from a [[ word-aligned corpora ]] is proposed .", "h": ["word-aligned corpora"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "A [[ statistical translation model ]] is also presented that deals such << phrases >> , as well as a training method based on the maximization of translation accuracy , as measured with the NIST evaluation metric .", "h": ["statistical translation model"], "t": ["phrases"]}, {"label": "USED-FOR", "tokens": "A statistical translation model is also presented that deals such phrases , as well as a << training method >> based on the [[ maximization of translation accuracy ]] , as measured with the NIST evaluation metric .", "h": ["maximization of translation accuracy"], "t": ["training method"]}, {"label": "EVALUATE-FOR", "tokens": "A << statistical translation model >> is also presented that deals such phrases , as well as a training method based on the maximization of translation accuracy , as measured with the [[ NIST evaluation metric ]] .", "h": ["NIST evaluation metric"], "t": ["statistical translation model"]}, {"label": "USED-FOR", "tokens": "<< Translations >> are produced by means of a [[ beam-search decoder ]] .", "h": ["beam-search decoder"], "t": ["Translations"]}, {"label": "USED-FOR", "tokens": "[[ GLOSSER ]] is designed to support << reading and learning >> to read in a foreign language .", "h": ["GLOSSER"], "t": ["reading and learning"]}, {"label": "USED-FOR", "tokens": "There are four [[ language pairs ]] currently supported by << GLOSSER >> : English-Bulgarian , English-Estonian , English-Hungarian and French-Dutch .", "h": ["language pairs"], "t": ["GLOSSER"]}, {"label": "HYPONYM-OF", "tokens": "There are four << language pairs >> currently supported by GLOSSER : [[ English-Bulgarian ]] , English-Estonian , English-Hungarian and French-Dutch .", "h": ["English-Bulgarian"], "t": ["language pairs"]}, {"label": "CONJUNCTION", "tokens": "There are four language pairs currently supported by GLOSSER : [[ English-Bulgarian ]] , << English-Estonian >> , English-Hungarian and French-Dutch .", "h": ["English-Bulgarian"], "t": ["English-Estonian"]}, {"label": "HYPONYM-OF", "tokens": "There are four << language pairs >> currently supported by GLOSSER : English-Bulgarian , [[ English-Estonian ]] , English-Hungarian and French-Dutch .", "h": ["English-Estonian"], "t": ["language pairs"]}, {"label": "CONJUNCTION", "tokens": "There are four language pairs currently supported by GLOSSER : English-Bulgarian , [[ English-Estonian ]] , << English-Hungarian >> and French-Dutch .", "h": ["English-Estonian"], "t": ["English-Hungarian"]}, {"label": "HYPONYM-OF", "tokens": "There are four << language pairs >> currently supported by GLOSSER : English-Bulgarian , English-Estonian , [[ English-Hungarian ]] and French-Dutch .", "h": ["English-Hungarian"], "t": ["language pairs"]}, {"label": "CONJUNCTION", "tokens": "There are four language pairs currently supported by GLOSSER : English-Bulgarian , English-Estonian , [[ English-Hungarian ]] and << French-Dutch >> .", "h": ["English-Hungarian"], "t": ["French-Dutch"]}, {"label": "HYPONYM-OF", "tokens": "There are four << language pairs >> currently supported by GLOSSER : English-Bulgarian , English-Estonian , English-Hungarian and [[ French-Dutch ]] .", "h": ["French-Dutch"], "t": ["language pairs"]}, {"label": "USED-FOR", "tokens": "A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes [[ components ]] put to novel technical uses in << intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- >> , including disambiguated morphological analysis and lemmatized indexing for an aligned bilingual corpus of word examples .", "h": ["components"], "t": ["intelligent computer-assisted morphological analysis -LRB- ICALL -RRB-"]}, {"label": "HYPONYM-OF", "tokens": "A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes << components >> put to novel technical uses in intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- , including [[ disambiguated morphological analysis ]] and lemmatized indexing for an aligned bilingual corpus of word examples .", "h": ["disambiguated morphological analysis"], "t": ["components"]}, {"label": "CONJUNCTION", "tokens": "A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- , including [[ disambiguated morphological analysis ]] and << lemmatized indexing >> for an aligned bilingual corpus of word examples .", "h": ["disambiguated morphological analysis"], "t": ["lemmatized indexing"]}, {"label": "USED-FOR", "tokens": "A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- , including [[ disambiguated morphological analysis ]] and lemmatized indexing for an << aligned bilingual corpus >> of word examples .", "h": ["disambiguated morphological analysis"], "t": ["aligned bilingual corpus"]}, {"label": "HYPONYM-OF", "tokens": "A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes << components >> put to novel technical uses in intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- , including disambiguated morphological analysis and [[ lemmatized indexing ]] for an aligned bilingual corpus of word examples .", "h": ["lemmatized indexing"], "t": ["components"]}, {"label": "USED-FOR", "tokens": "A demonstration -LRB- in UNIX -RRB- for Applied Natural Language Processing emphasizes components put to novel technical uses in intelligent computer-assisted morphological analysis -LRB- ICALL -RRB- , including disambiguated morphological analysis and [[ lemmatized indexing ]] for an << aligned bilingual corpus >> of word examples .", "h": ["lemmatized indexing"], "t": ["aligned bilingual corpus"]}, {"label": "USED-FOR", "tokens": "We present a new << part-of-speech tagger >> that demonstrates the following ideas : -LRB- i -RRB- explicit use of both preceding and following [[ tag contexts ]] via a dependency network representation , -LRB- ii -RRB- broad use of lexical features , including jointly conditioning on multiple consecutive words , -LRB- iii -RRB- effective use of priors in conditional loglinear models , and -LRB- iv -RRB- fine-grained modeling of unknown word features .", "h": ["tag contexts"], "t": ["part-of-speech tagger"]}, {"label": "USED-FOR", "tokens": "We present a new part-of-speech tagger that demonstrates the following ideas : -LRB- i -RRB- explicit use of both preceding and following << tag contexts >> via a [[ dependency network representation ]] , -LRB- ii -RRB- broad use of lexical features , including jointly conditioning on multiple consecutive words , -LRB- iii -RRB- effective use of priors in conditional loglinear models , and -LRB- iv -RRB- fine-grained modeling of unknown word features .", "h": ["dependency network representation"], "t": ["tag contexts"]}, {"label": "USED-FOR", "tokens": "We present a new << part-of-speech tagger >> that demonstrates the following ideas : -LRB- i -RRB- explicit use of both preceding and following tag contexts via a dependency network representation , -LRB- ii -RRB- broad use of [[ lexical features ]] , including jointly conditioning on multiple consecutive words , -LRB- iii -RRB- effective use of priors in conditional loglinear models , and -LRB- iv -RRB- fine-grained modeling of unknown word features .", "h": ["lexical features"], "t": ["part-of-speech tagger"]}, {"label": "USED-FOR", "tokens": "We present a new << part-of-speech tagger >> that demonstrates the following ideas : -LRB- i -RRB- explicit use of both preceding and following tag contexts via a dependency network representation , -LRB- ii -RRB- broad use of lexical features , including jointly conditioning on multiple consecutive words , -LRB- iii -RRB- effective use of [[ priors in conditional loglinear models ]] , and -LRB- iv -RRB- fine-grained modeling of unknown word features .", "h": ["priors in conditional loglinear models"], "t": ["part-of-speech tagger"]}, {"label": "USED-FOR", "tokens": "We present a new << part-of-speech tagger >> that demonstrates the following ideas : -LRB- i -RRB- explicit use of both preceding and following tag contexts via a dependency network representation , -LRB- ii -RRB- broad use of lexical features , including jointly conditioning on multiple consecutive words , -LRB- iii -RRB- effective use of priors in conditional loglinear models , and -LRB- iv -RRB- [[ fine-grained modeling of unknown word features ]] .", "h": ["fine-grained modeling of unknown word features"], "t": ["part-of-speech tagger"]}, {"label": "EVALUATE-FOR", "tokens": "Using these ideas together , the resulting << tagger >> gives a 97.24 % [[ accuracy ]] on the Penn Treebank WSJ , an error reduction of 4.4 % on the best previous single automatically learned tagging result .", "h": ["accuracy"], "t": ["tagger"]}, {"label": "EVALUATE-FOR", "tokens": "Using these ideas together , the resulting << tagger >> gives a 97.24 % accuracy on the [[ Penn Treebank WSJ ]] , an error reduction of 4.4 % on the best previous single automatically learned tagging result .", "h": ["Penn Treebank WSJ"], "t": ["tagger"]}, {"label": "EVALUATE-FOR", "tokens": "Using these ideas together , the resulting << tagger >> gives a 97.24 % accuracy on the Penn Treebank WSJ , an [[ error ]] reduction of 4.4 % on the best previous single automatically learned tagging result .", "h": ["error"], "t": ["tagger"]}, {"label": "USED-FOR", "tokens": "Owing to these variations , the << pedestrian data >> is distributed as [[ highly-curved manifolds ]] in the feature space , despite the current convolutional neural networks -LRB- CNN -RRB- 's capability of feature extraction .", "h": ["highly-curved manifolds"], "t": ["pedestrian data"]}, {"label": "FEATURE-OF", "tokens": "Owing to these variations , the pedestrian data is distributed as << highly-curved manifolds >> in the [[ feature space ]] , despite the current convolutional neural networks -LRB- CNN -RRB- 's capability of feature extraction .", "h": ["feature space"], "t": ["highly-curved manifolds"]}, {"label": "USED-FOR", "tokens": "Owing to these variations , the pedestrian data is distributed as highly-curved manifolds in the feature space , despite the current [[ convolutional neural networks -LRB- CNN -RRB- ]] 's capability of << feature extraction >> .", "h": ["convolutional neural networks -LRB- CNN -RRB-"], "t": ["feature extraction"]}, {"label": "USED-FOR", "tokens": "In practice , the current << deep embedding methods >> use the [[ Euclidean distance ]] for the training and test .", "h": ["Euclidean distance"], "t": ["deep embedding methods"]}, {"label": "USED-FOR", "tokens": "On the other hand , the << manifold learning methods >> suggest to use the [[ Euclidean distance ]] in the local range , combining with the graphical relationship between samples , for approximating the geodesic distance .", "h": ["Euclidean distance"], "t": ["manifold learning methods"]}, {"label": "CONJUNCTION", "tokens": "On the other hand , the manifold learning methods suggest to use the [[ Euclidean distance ]] in the local range , combining with the << graphical relationship >> between samples , for approximating the geodesic distance .", "h": ["Euclidean distance"], "t": ["graphical relationship"]}, {"label": "USED-FOR", "tokens": "On the other hand , the manifold learning methods suggest to use the [[ Euclidean distance ]] in the local range , combining with the graphical relationship between samples , for approximating the << geodesic distance >> .", "h": ["Euclidean distance"], "t": ["geodesic distance"]}, {"label": "FEATURE-OF", "tokens": "On the other hand , the manifold learning methods suggest to use the << Euclidean distance >> in the [[ local range ]] , combining with the graphical relationship between samples , for approximating the geodesic distance .", "h": ["local range"], "t": ["Euclidean distance"]}, {"label": "USED-FOR", "tokens": "On the other hand , the manifold learning methods suggest to use the Euclidean distance in the local range , combining with the [[ graphical relationship ]] between samples , for approximating the << geodesic distance >> .", "h": ["graphical relationship"], "t": ["geodesic distance"]}, {"label": "FEATURE-OF", "tokens": "From this point of view , selecting suitable positive -LRB- i.e. intra-class -RRB- training samples within a local range is critical for training the CNN embedding , especially when the << data >> has large [[ intra-class variations ]] .", "h": ["intra-class variations"], "t": ["data"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a novel [[ moderate positive sample mining method ]] to train << robust CNN >> for person re-identification , dealing with the problem of large variation .", "h": ["moderate positive sample mining method"], "t": ["robust CNN"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a novel moderate positive sample mining method to train [[ robust CNN ]] for << person re-identification >> , dealing with the problem of large variation .", "h": ["robust CNN"], "t": ["person re-identification"]}, {"label": "USED-FOR", "tokens": "In addition , we improve the << learning >> by a [[ metric weight constraint ]] , so that the learned metric has a better generalization ability .", "h": ["metric weight constraint"], "t": ["learning"]}, {"label": "FEATURE-OF", "tokens": "In addition , we improve the learning by a metric weight constraint , so that the << learned metric >> has a better [[ generalization ability ]] .", "h": ["generalization ability"], "t": ["learned metric"]}, {"label": "USED-FOR", "tokens": "Experiments show that these two strategies are effective in learning [[ robust deep metrics ]] for << person re-identification >> , and accordingly our deep model significantly outperforms the state-of-the-art methods on several benchmarks of person re-identification .", "h": ["robust deep metrics"], "t": ["person re-identification"]}, {"label": "COMPARE", "tokens": "Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification , and accordingly our [[ deep model ]] significantly outperforms the << state-of-the-art methods >> on several benchmarks of person re-identification .", "h": ["deep model"], "t": ["state-of-the-art methods"]}, {"label": "USED-FOR", "tokens": "Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification , and accordingly our [[ deep model ]] significantly outperforms the state-of-the-art methods on several benchmarks of << person re-identification >> .", "h": ["deep model"], "t": ["person re-identification"]}, {"label": "USED-FOR", "tokens": "Experiments show that these two strategies are effective in learning robust deep metrics for person re-identification , and accordingly our deep model significantly outperforms the [[ state-of-the-art methods ]] on several benchmarks of << person re-identification >> .", "h": ["state-of-the-art methods"], "t": ["person re-identification"]}, {"label": "USED-FOR", "tokens": "Therefore , the study presented in this paper may be useful in inspiring new designs of [[ deep models ]] for << person re-identification >> .", "h": ["deep models"], "t": ["person re-identification"]}, {"label": "HYPONYM-OF", "tokens": "[[ Utterance Verification -LRB- UV -RRB- ]] is a critical function of an << Automatic Speech Recognition -LRB- ASR -RRB- System >> working on real applications where spontaneous speech , out-of-vocabulary -LRB- OOV -RRB- words and acoustic noises are present .", "h": ["Utterance Verification -LRB- UV -RRB-"], "t": ["Automatic Speech Recognition -LRB- ASR -RRB- System"]}, {"label": "USED-FOR", "tokens": "In this paper we present a new UV procedure with two major features : a -RRB- [[ Confidence tests ]] are applied to << decoded string hypotheses >> obtained from using word and garbage models that represent OOV words and noises .", "h": ["Confidence tests"], "t": ["decoded string hypotheses"]}, {"label": "CONJUNCTION", "tokens": "In this paper we present a new UV procedure with two major features : a -RRB- Confidence tests are applied to decoded string hypotheses obtained from using word and garbage models that represent << OOV words >> and [[ noises ]] .", "h": ["noises"], "t": ["OOV words"]}, {"label": "USED-FOR", "tokens": "Thus the [[ ASR system ]] is designed to deal with what we refer to as << Word Spotting >> and Noise Spotting capabilities .", "h": ["ASR system"], "t": ["Word Spotting"]}, {"label": "USED-FOR", "tokens": "Thus the [[ ASR system ]] is designed to deal with what we refer to as Word Spotting and << Noise Spotting capabilities >> .", "h": ["ASR system"], "t": ["Noise Spotting capabilities"]}, {"label": "USED-FOR", "tokens": "b -RRB- The << UV procedure >> is based on three different [[ confidence tests ]] , two based on acoustic measures and one founded on linguistic information , applied in a hierarchical structure .", "h": ["confidence tests"], "t": ["UV procedure"]}, {"label": "USED-FOR", "tokens": "b -RRB- The UV procedure is based on three different [[ confidence tests ]] , two based on acoustic measures and one founded on linguistic information , applied in a << hierarchical structure >> .", "h": ["confidence tests"], "t": ["hierarchical structure"]}, {"label": "HYPONYM-OF", "tokens": "b -RRB- The UV procedure is based on three different << confidence tests >> , [[ two ]] based on acoustic measures and one founded on linguistic information , applied in a hierarchical structure .", "h": ["two"], "t": ["confidence tests"]}, {"label": "USED-FOR", "tokens": "b -RRB- The UV procedure is based on three different confidence tests , << two >> based on [[ acoustic measures ]] and one founded on linguistic information , applied in a hierarchical structure .", "h": ["acoustic measures"], "t": ["two"]}, {"label": "HYPONYM-OF", "tokens": "b -RRB- The UV procedure is based on three different << confidence tests >> , two based on acoustic measures and [[ one ]] founded on linguistic information , applied in a hierarchical structure .", "h": ["one"], "t": ["confidence tests"]}, {"label": "USED-FOR", "tokens": "b -RRB- The UV procedure is based on three different confidence tests , two based on acoustic measures and << one >> founded on [[ linguistic information ]] , applied in a hierarchical structure .", "h": ["linguistic information"], "t": ["one"]}, {"label": "FEATURE-OF", "tokens": "Experimental results from a real << telephone application >> on a [[ natural number recognition task ]] show an 50 % reduction in recognition errors with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .", "h": ["natural number recognition task"], "t": ["telephone application"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results from a real telephone application on a << natural number recognition task >> show an 50 % reduction in [[ recognition errors ]] with a moderate 12 % rejection rate of correct utterances and a low 1.5 % rate of false acceptance .", "h": ["recognition errors"], "t": ["natural number recognition task"]}, {"label": "USED-FOR", "tokens": "A critical step in [[ encoding sound ]] for << neuronal processing >> occurs when the analog pressure wave is coded into discrete nerve-action potentials .", "h": ["encoding sound"], "t": ["neuronal processing"]}, {"label": "USED-FOR", "tokens": "A critical step in encoding sound for neuronal processing occurs when the << analog pressure wave >> is coded into [[ discrete nerve-action potentials ]] .", "h": ["discrete nerve-action potentials"], "t": ["analog pressure wave"]}, {"label": "USED-FOR", "tokens": "Recent [[ pool models ]] of the << inner hair cell synapse >> do not reproduce the dead time period after an intense stimulus , so we used visual inspection and automatic speech recognition -LRB- ASR -RRB- to investigate an offset adaptation -LRB- OA -RRB- model proposed by Zhang et al. -LSB- 1 -RSB- .", "h": ["pool models"], "t": ["inner hair cell synapse"]}, {"label": "CONJUNCTION", "tokens": "Recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used [[ visual inspection ]] and << automatic speech recognition -LRB- ASR -RRB- >> to investigate an offset adaptation -LRB- OA -RRB- model proposed by Zhang et al. -LSB- 1 -RSB- .", "h": ["visual inspection"], "t": ["automatic speech recognition -LRB- ASR -RRB-"]}, {"label": "USED-FOR", "tokens": "Recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used [[ visual inspection ]] and automatic speech recognition -LRB- ASR -RRB- to investigate an << offset adaptation -LRB- OA -RRB- model >> proposed by Zhang et al. -LSB- 1 -RSB- .", "h": ["visual inspection"], "t": ["offset adaptation -LRB- OA -RRB- model"]}, {"label": "USED-FOR", "tokens": "Recent pool models of the inner hair cell synapse do not reproduce the dead time period after an intense stimulus , so we used visual inspection and [[ automatic speech recognition -LRB- ASR -RRB- ]] to investigate an << offset adaptation -LRB- OA -RRB- model >> proposed by Zhang et al. -LSB- 1 -RSB- .", "h": ["automatic speech recognition -LRB- ASR -RRB-"], "t": ["offset adaptation -LRB- OA -RRB- model"]}, {"label": "USED-FOR", "tokens": "[[ OA ]] improved << phase locking in the auditory nerve -LRB- AN -RRB- >> and raised ASR accuracy for features derived from AN fibers -LRB- ANFs -RRB- .", "h": ["OA"], "t": ["phase locking in the auditory nerve -LRB- AN -RRB-"]}, {"label": "USED-FOR", "tokens": "[[ OA ]] improved phase locking in the auditory nerve -LRB- AN -RRB- and raised ASR accuracy for << features >> derived from AN fibers -LRB- ANFs -RRB- .", "h": ["OA"], "t": ["features"]}, {"label": "EVALUATE-FOR", "tokens": "OA improved phase locking in the auditory nerve -LRB- AN -RRB- and raised [[ ASR accuracy ]] for << features >> derived from AN fibers -LRB- ANFs -RRB- .", "h": ["ASR accuracy"], "t": ["features"]}, {"label": "USED-FOR", "tokens": "OA improved phase locking in the auditory nerve -LRB- AN -RRB- and raised ASR accuracy for << features >> derived from [[ AN fibers -LRB- ANFs -RRB- ]] .", "h": ["AN fibers -LRB- ANFs -RRB-"], "t": ["features"]}, {"label": "USED-FOR", "tokens": "We also found that [[ OA ]] is crucial for << auditory processing >> by onset neurons -LRB- ONs -RRB- in the next neuronal stage , the auditory brainstem .", "h": ["OA"], "t": ["auditory processing"]}, {"label": "USED-FOR", "tokens": "We also found that << OA >> is crucial for auditory processing by [[ onset neurons -LRB- ONs -RRB- ]] in the next neuronal stage , the auditory brainstem .", "h": ["onset neurons -LRB- ONs -RRB-"], "t": ["OA"]}, {"label": "COMPARE", "tokens": "[[ Multi-layer perceptrons -LRB- MLPs -RRB- ]] performed much better than standard << Gaussian mixture models -LRB- GMMs -RRB- >> for both our ANF-based and ON-based auditory features .", "h": ["Multi-layer perceptrons -LRB- MLPs -RRB-"], "t": ["Gaussian mixture models -LRB- GMMs -RRB-"]}, {"label": "USED-FOR", "tokens": "[[ Multi-layer perceptrons -LRB- MLPs -RRB- ]] performed much better than standard Gaussian mixture models -LRB- GMMs -RRB- for both our << ANF-based and ON-based auditory features >> .", "h": ["Multi-layer perceptrons -LRB- MLPs -RRB-"], "t": ["ANF-based and ON-based auditory features"]}, {"label": "USED-FOR", "tokens": "Multi-layer perceptrons -LRB- MLPs -RRB- performed much better than standard [[ Gaussian mixture models -LRB- GMMs -RRB- ]] for both our << ANF-based and ON-based auditory features >> .", "h": ["Gaussian mixture models -LRB- GMMs -RRB-"], "t": ["ANF-based and ON-based auditory features"]}, {"label": "USED-FOR", "tokens": "Recent progress in << computer vision >> has been driven by [[ high-capacity models ]] trained on large datasets .", "h": ["high-capacity models"], "t": ["computer vision"]}, {"label": "USED-FOR", "tokens": "Recent progress in computer vision has been driven by << high-capacity models >> trained on [[ large datasets ]] .", "h": ["large datasets"], "t": ["high-capacity models"]}, {"label": "FEATURE-OF", "tokens": "Unfortunately , creating << large datasets >> with [[ pixel-level labels ]] has been extremely costly due to the amount of human effort required .", "h": ["pixel-level labels"], "t": ["large datasets"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an [[ approach ]] to rapidly creating << pixel-accurate semantic label maps >> for images extracted from modern computer games .", "h": ["approach"], "t": ["pixel-accurate semantic label maps"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an approach to rapidly creating [[ pixel-accurate semantic label maps ]] for << images >> extracted from modern computer games .", "h": ["pixel-accurate semantic label maps"], "t": ["images"]}, {"label": "PART-OF", "tokens": "In this paper , we present an approach to rapidly creating pixel-accurate semantic label maps for [[ images ]] extracted from << modern computer games >> .", "h": ["images"], "t": ["modern computer games"]}, {"label": "USED-FOR", "tokens": "We propose a novel step toward the << unsupervised seg-mentation of whole objects >> by combining '' hints '' of [[ partial scene segmentation ]] offered by multiple soft , binary mattes .", "h": ["partial scene segmentation"], "t": ["unsupervised seg-mentation of whole objects"]}, {"label": "USED-FOR", "tokens": "We propose a novel step toward the unsupervised seg-mentation of whole objects by combining '' hints '' of << partial scene segmentation >> offered by multiple [[ soft , binary mattes ]] .", "h": ["soft , binary mattes"], "t": ["partial scene segmentation"]}, {"label": "USED-FOR", "tokens": "These << mattes >> are implied by a set of [[ hypothesized object boundary fragments ]] in the scene .", "h": ["hypothesized object boundary fragments"], "t": ["mattes"]}, {"label": "USED-FOR", "tokens": "This reflects [[ contemporary methods ]] for << unsupervised object discovery >> from groups of images , and it allows us to define intuitive evaluation met-rics for our sets of segmentations based on the accurate and parsimonious delineation of scene objects .", "h": ["contemporary methods"], "t": ["unsupervised object discovery"]}, {"label": "USED-FOR", "tokens": "Our proposed << approach >> builds on recent advances in [[ spectral clustering ]] , image matting , and boundary detection .", "h": ["spectral clustering"], "t": ["approach"]}, {"label": "CONJUNCTION", "tokens": "Our proposed approach builds on recent advances in [[ spectral clustering ]] , << image matting >> , and boundary detection .", "h": ["spectral clustering"], "t": ["image matting"]}, {"label": "USED-FOR", "tokens": "Our proposed << approach >> builds on recent advances in spectral clustering , [[ image matting ]] , and boundary detection .", "h": ["image matting"], "t": ["approach"]}, {"label": "CONJUNCTION", "tokens": "Our proposed approach builds on recent advances in spectral clustering , [[ image matting ]] , and << boundary detection >> .", "h": ["image matting"], "t": ["boundary detection"]}, {"label": "USED-FOR", "tokens": "Our proposed << approach >> builds on recent advances in spectral clustering , image matting , and [[ boundary detection ]] .", "h": ["boundary detection"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "[[ It ]] is demonstrated qualitatively and quantitatively on a dataset of scenes and is suitable for current work in << unsupervised object discovery >> without top-down knowledge .", "h": ["It"], "t": ["unsupervised object discovery"]}, {"label": "EVALUATE-FOR", "tokens": "<< It >> is demonstrated qualitatively and quantitatively on a [[ dataset of scenes ]] and is suitable for current work in unsupervised object discovery without top-down knowledge .", "h": ["dataset of scenes"], "t": ["It"]}, {"label": "FEATURE-OF", "tokens": "[[ Language resource quality ]] is crucial in << NLP >> .", "h": ["Language resource quality"], "t": ["NLP"]}, {"label": "HYPONYM-OF", "tokens": "Many of the resources used are derived from data created by human beings out of an << NLP >> context , especially regarding [[ MT ]] and reference translations .", "h": ["MT"], "t": ["NLP"]}, {"label": "CONJUNCTION", "tokens": "Many of the resources used are derived from data created by human beings out of an NLP context , especially regarding [[ MT ]] and << reference translations >> .", "h": ["MT"], "t": ["reference translations"]}, {"label": "HYPONYM-OF", "tokens": "Many of the resources used are derived from data created by human beings out of an << NLP >> context , especially regarding MT and [[ reference translations ]] .", "h": ["reference translations"], "t": ["NLP"]}, {"label": "EVALUATE-FOR", "tokens": "Indeed , << automatic evaluations >> need [[ high-quality data ]] that allow the comparison of both automatic and human translations .", "h": ["high-quality data"], "t": ["automatic evaluations"]}, {"label": "USED-FOR", "tokens": "This paper describes the impact of using [[ different-quality references ]] on << evaluation >> .", "h": ["different-quality references"], "t": ["evaluation"]}, {"label": "EVALUATE-FOR", "tokens": "Thus , the limitations of the [[ automatic metrics ]] used within << MT >> are also discussed in this regard .", "h": ["automatic metrics"], "t": ["MT"]}, {"label": "USED-FOR", "tokens": "This poster paper describes a [[ full scale two-level morphological description ]] -LRB- Karttunen , 1983 ; Koskenniemi , 1983 -RRB- of << Turkish word structures >> .", "h": ["full scale two-level morphological description"], "t": ["Turkish word structures"]}, {"label": "USED-FOR", "tokens": "The << description >> has been implemented using the [[ PC-KIMMO environment ]] -LRB- Antworth , 1990 -RRB- and is based on a root word lexicon of about 23,000 roots words .", "h": ["PC-KIMMO environment"], "t": ["description"]}, {"label": "USED-FOR", "tokens": "The << description >> has been implemented using the PC-KIMMO environment -LRB- Antworth , 1990 -RRB- and is based on a [[ root word lexicon ]] of about 23,000 roots words .", "h": ["root word lexicon"], "t": ["description"]}, {"label": "HYPONYM-OF", "tokens": "[[ Turkish ]] is an << agglutinative language >> with word structures formed by productive affixations of derivational and inflectional suffixes to root words .", "h": ["Turkish"], "t": ["agglutinative language"]}, {"label": "FEATURE-OF", "tokens": "Turkish is an << agglutinative language >> with [[ word structures ]] formed by productive affixations of derivational and inflectional suffixes to root words .", "h": ["word structures"], "t": ["agglutinative language"]}, {"label": "PART-OF", "tokens": "Turkish is an agglutinative language with << word structures >> formed by [[ productive affixations of derivational and inflectional suffixes ]] to root words .", "h": ["productive affixations of derivational and inflectional suffixes"], "t": ["word structures"]}, {"label": "USED-FOR", "tokens": "The << surface realizations of morphological constructions >> are constrained and modified by a number of [[ phonetic rules ]] such as vowel harmony .", "h": ["phonetic rules"], "t": ["surface realizations of morphological constructions"]}, {"label": "HYPONYM-OF", "tokens": "The surface realizations of morphological constructions are constrained and modified by a number of << phonetic rules >> such as [[ vowel harmony ]] .", "h": ["vowel harmony"], "t": ["phonetic rules"]}, {"label": "USED-FOR", "tokens": "This paper deals with the problem of generating the [[ fundamental frequency -LRB- F0 -RRB- contour of speech ]] from a text input for << text-to-speech synthesis >> .", "h": ["fundamental frequency -LRB- F0 -RRB- contour of speech"], "t": ["text-to-speech synthesis"]}, {"label": "USED-FOR", "tokens": "This paper deals with the problem of generating the << fundamental frequency -LRB- F0 -RRB- contour of speech >> from a [[ text input ]] for text-to-speech synthesis .", "h": ["text input"], "t": ["fundamental frequency -LRB- F0 -RRB- contour of speech"]}, {"label": "USED-FOR", "tokens": "We have previously introduced a [[ statistical model ]] describing the generating process of << speech F0 contours >> , based on the discrete-time version of the Fujisaki model .", "h": ["statistical model"], "t": ["speech F0 contours"]}, {"label": "USED-FOR", "tokens": "We have previously introduced a << statistical model >> describing the generating process of speech F0 contours , based on the discrete-time version of the [[ Fujisaki model ]] .", "h": ["Fujisaki model"], "t": ["statistical model"]}, {"label": "FEATURE-OF", "tokens": "One [[ remarkable feature ]] of this << model >> is that it has allowed us to derive an efficient algorithm based on powerful statistical methods for estimating the Fujisaki-model parameters from raw F0 contours .", "h": ["remarkable feature"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "One remarkable feature of this model is that it has allowed us to derive an efficient [[ algorithm ]] based on powerful statistical methods for estimating the << Fujisaki-model parameters >> from raw F0 contours .", "h": ["algorithm"], "t": ["Fujisaki-model parameters"]}, {"label": "USED-FOR", "tokens": "One remarkable feature of this model is that it has allowed us to derive an efficient << algorithm >> based on powerful [[ statistical methods ]] for estimating the Fujisaki-model parameters from raw F0 contours .", "h": ["statistical methods"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "One remarkable feature of this model is that it has allowed us to derive an efficient algorithm based on powerful statistical methods for estimating the << Fujisaki-model parameters >> from [[ raw F0 contours ]] .", "h": ["raw F0 contours"], "t": ["Fujisaki-model parameters"]}, {"label": "USED-FOR", "tokens": "To associate a sequence of the << Fujisaki-model parameters >> with a [[ text input ]] based on statistical learning , this paper proposes extending this model to a context-dependent one .", "h": ["text input"], "t": ["Fujisaki-model parameters"]}, {"label": "USED-FOR", "tokens": "To associate a sequence of the << Fujisaki-model parameters >> with a text input based on [[ statistical learning ]] , this paper proposes extending this model to a context-dependent one .", "h": ["statistical learning"], "t": ["Fujisaki-model parameters"]}, {"label": "USED-FOR", "tokens": "We further propose a [[ parameter training algorithm ]] for the present << model >> based on a decision tree-based context clustering .", "h": ["parameter training algorithm"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "We further propose a << parameter training algorithm >> for the present model based on a [[ decision tree-based context clustering ]] .", "h": ["decision tree-based context clustering"], "t": ["parameter training algorithm"]}, {"label": "USED-FOR", "tokens": "We introduce a [[ method ]] to accelerate the << evaluation of object detection cascades >> with the help of a divide-and-conquer procedure in the space of candidate regions .", "h": ["method"], "t": ["evaluation of object detection cascades"]}, {"label": "USED-FOR", "tokens": "We introduce a << method >> to accelerate the evaluation of object detection cascades with the help of a [[ divide-and-conquer procedure ]] in the space of candidate regions .", "h": ["divide-and-conquer procedure"], "t": ["method"]}, {"label": "FEATURE-OF", "tokens": "We introduce a method to accelerate the evaluation of object detection cascades with the help of a << divide-and-conquer procedure >> in the [[ space of candidate regions ]] .", "h": ["space of candidate regions"], "t": ["divide-and-conquer procedure"]}, {"label": "USED-FOR", "tokens": "Compared to the [[ exhaustive procedure ]] that thus far is the state-of-the-art for << cascade evaluation >> , the proposed method requires fewer evaluations of the classifier functions , thereby speeding up the search .", "h": ["exhaustive procedure"], "t": ["cascade evaluation"]}, {"label": "COMPARE", "tokens": "Compared to the [[ exhaustive procedure ]] that thus far is the state-of-the-art for cascade evaluation , the proposed << method >> requires fewer evaluations of the classifier functions , thereby speeding up the search .", "h": ["exhaustive procedure"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "Compared to the exhaustive procedure that thus far is the state-of-the-art for cascade evaluation , the proposed [[ method ]] requires fewer evaluations of the classifier functions , thereby speeding up the << search >> .", "h": ["method"], "t": ["search"]}, {"label": "PART-OF", "tokens": "Furthermore , we show how the recently developed efficient [[ subwindow search -LRB- ESS -RRB- procedure ]] -LSB- 11 -RSB- can be integrated into the last stage of our << method >> .", "h": ["subwindow search -LRB- ESS -RRB- procedure"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "This allows us to use our [[ method ]] to act not only as a faster procedure for << cascade evaluation >> , but also as a tool to perform efficient branch-and-bound object detection with nonlinear quality functions , in particular kernel-ized support vector machines .", "h": ["method"], "t": ["cascade evaluation"]}, {"label": "USED-FOR", "tokens": "This allows us to use our [[ method ]] to act not only as a faster procedure for cascade evaluation , but also as a tool to perform efficient << branch-and-bound object detection >> with nonlinear quality functions , in particular kernel-ized support vector machines .", "h": ["method"], "t": ["branch-and-bound object detection"]}, {"label": "USED-FOR", "tokens": "This allows us to use our method to act not only as a faster procedure for cascade evaluation , but also as a tool to perform efficient << branch-and-bound object detection >> with [[ nonlinear quality functions ]] , in particular kernel-ized support vector machines .", "h": ["nonlinear quality functions"], "t": ["branch-and-bound object detection"]}, {"label": "HYPONYM-OF", "tokens": "This allows us to use our method to act not only as a faster procedure for cascade evaluation , but also as a tool to perform efficient branch-and-bound object detection with << nonlinear quality functions >> , in particular [[ kernel-ized support vector machines ]] .", "h": ["kernel-ized support vector machines"], "t": ["nonlinear quality functions"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on the [[ PASCAL VOC 2006 dataset ]] show an acceleration of more than 50 % by our << method >> compared to standard cascade evaluation .", "h": ["PASCAL VOC 2006 dataset"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on the [[ PASCAL VOC 2006 dataset ]] show an acceleration of more than 50 % by our method compared to standard << cascade evaluation >> .", "h": ["PASCAL VOC 2006 dataset"], "t": ["cascade evaluation"]}, {"label": "COMPARE", "tokens": "Experiments on the PASCAL VOC 2006 dataset show an acceleration of more than 50 % by our << method >> compared to standard [[ cascade evaluation ]] .", "h": ["cascade evaluation"], "t": ["method"]}, {"label": "PART-OF", "tokens": "[[ Background modeling ]] is an important component of many << vision systems >> .", "h": ["Background modeling"], "t": ["vision systems"]}, {"label": "FEATURE-OF", "tokens": "When the << scene >> exhibits a [[ persistent dynamic behavior ]] in time , such an assumption is violated and detection performance deteriorates .", "h": ["persistent dynamic behavior"], "t": ["scene"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a new [[ method ]] for the << modeling and subtraction of such scenes >> .", "h": ["method"], "t": ["modeling and subtraction of such scenes"]}, {"label": "USED-FOR", "tokens": "Towards the << modeling of the dynamic characteristics >> , [[ optical flow ]] is computed and utilized as a feature in a higher dimensional space .", "h": ["optical flow"], "t": ["modeling of the dynamic characteristics"]}, {"label": "USED-FOR", "tokens": "Towards the modeling of the dynamic characteristics , [[ optical flow ]] is computed and utilized as a << feature >> in a higher dimensional space .", "h": ["optical flow"], "t": ["feature"]}, {"label": "USED-FOR", "tokens": "Towards the << modeling of the dynamic characteristics >> , optical flow is computed and utilized as a [[ feature ]] in a higher dimensional space .", "h": ["feature"], "t": ["modeling of the dynamic characteristics"]}, {"label": "FEATURE-OF", "tokens": "Towards the modeling of the dynamic characteristics , optical flow is computed and utilized as a << feature >> in a [[ higher dimensional space ]] .", "h": ["higher dimensional space"], "t": ["feature"]}, {"label": "FEATURE-OF", "tokens": "Inherent [[ ambiguities ]] in the << computation of features >> are addressed by using a data-dependent bandwidth for density estimation using kernels .", "h": ["ambiguities"], "t": ["computation of features"]}, {"label": "USED-FOR", "tokens": "Inherent << ambiguities >> in the computation of features are addressed by using a [[ data-dependent bandwidth ]] for density estimation using kernels .", "h": ["data-dependent bandwidth"], "t": ["ambiguities"]}, {"label": "USED-FOR", "tokens": "Inherent ambiguities in the computation of features are addressed by using a [[ data-dependent bandwidth ]] for << density estimation >> using kernels .", "h": ["data-dependent bandwidth"], "t": ["density estimation"]}, {"label": "USED-FOR", "tokens": "Inherent ambiguities in the computation of features are addressed by using a data-dependent bandwidth for << density estimation >> using [[ kernels ]] .", "h": ["kernels"], "t": ["density estimation"]}, {"label": "USED-FOR", "tokens": "In this paper , we present our approach for using [[ information extraction annotations ]] to augment << document retrieval for distillation >> .", "h": ["information extraction annotations"], "t": ["document retrieval for distillation"]}, {"label": "USED-FOR", "tokens": "This paper presents a novel [[ representation ]] for << three-dimensional objects >> in terms of affine-invariant image patches and their spatial relationships .", "h": ["representation"], "t": ["three-dimensional objects"]}, {"label": "FEATURE-OF", "tokens": "This paper presents a novel representation for << three-dimensional objects >> in terms of [[ affine-invariant image patches ]] and their spatial relationships .", "h": ["affine-invariant image patches"], "t": ["three-dimensional objects"]}, {"label": "FEATURE-OF", "tokens": "This paper presents a novel representation for three-dimensional objects in terms of << affine-invariant image patches >> and their [[ spatial relationships ]] .", "h": ["spatial relationships"], "t": ["affine-invariant image patches"]}, {"label": "CONJUNCTION", "tokens": "[[ Multi-view constraints ]] associated with groups of patches are combined with a << normalized representation >> of their appearance to guide matching and reconstruction , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .", "h": ["Multi-view constraints"], "t": ["normalized representation"]}, {"label": "USED-FOR", "tokens": "[[ Multi-view constraints ]] associated with groups of patches are combined with a normalized representation of their appearance to guide << matching >> and reconstruction , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .", "h": ["Multi-view constraints"], "t": ["matching"]}, {"label": "USED-FOR", "tokens": "[[ Multi-view constraints ]] associated with groups of patches are combined with a normalized representation of their appearance to guide matching and << reconstruction >> , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .", "h": ["Multi-view constraints"], "t": ["reconstruction"]}, {"label": "USED-FOR", "tokens": "Multi-view constraints associated with groups of patches are combined with a [[ normalized representation ]] of their appearance to guide << matching >> and reconstruction , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .", "h": ["normalized representation"], "t": ["matching"]}, {"label": "USED-FOR", "tokens": "Multi-view constraints associated with groups of patches are combined with a [[ normalized representation ]] of their appearance to guide matching and << reconstruction >> , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .", "h": ["normalized representation"], "t": ["reconstruction"]}, {"label": "CONJUNCTION", "tokens": "Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide [[ matching ]] and << reconstruction >> , allowing the acquisition of true three-dimensional affine and Euclidean models from multiple images and their recognition in a single photograph taken from an arbitrary viewpoint .", "h": ["matching"], "t": ["reconstruction"]}, {"label": "USED-FOR", "tokens": "Multi-view constraints associated with groups of patches are combined with a normalized representation of their appearance to guide matching and reconstruction , allowing the << acquisition of true three-dimensional affine and Euclidean models >> from multiple [[ images ]] and their recognition in a single photograph taken from an arbitrary viewpoint .", "h": ["images"], "t": ["acquisition of true three-dimensional affine and Euclidean models"]}, {"label": "USED-FOR", "tokens": "The proposed [[ approach ]] does not require a separate segmentation stage and is applicable to << cluttered scenes >> .", "h": ["approach"], "t": ["cluttered scenes"]}, {"label": "USED-FOR", "tokens": "[[ Fast algorithms ]] for << nearest neighbor -LRB- NN -RRB- search >> have in large part focused on 2 distance .", "h": ["Fast algorithms"], "t": ["nearest neighbor -LRB- NN -RRB- search"]}, {"label": "USED-FOR", "tokens": "Here we develop an [[ approach ]] for << 1 distance >> that begins with an explicit and exactly distance-preserving embedding of the points into 2 2 .", "h": ["approach"], "t": ["1 distance"]}, {"label": "CONJUNCTION", "tokens": "We show how [[ this ]] can efficiently be combined with << random-projection based methods >> for 2 NN search , such as locality-sensitive hashing -LRB- LSH -RRB- or random projection trees .", "h": ["this"], "t": ["random-projection based methods"]}, {"label": "USED-FOR", "tokens": "We show how this can efficiently be combined with [[ random-projection based methods ]] for 2 << NN search >> , such as locality-sensitive hashing -LRB- LSH -RRB- or random projection trees .", "h": ["random-projection based methods"], "t": ["NN search"]}, {"label": "HYPONYM-OF", "tokens": "We show how this can efficiently be combined with << random-projection based methods >> for 2 NN search , such as [[ locality-sensitive hashing -LRB- LSH -RRB- ]] or random projection trees .", "h": ["locality-sensitive hashing -LRB- LSH -RRB-"], "t": ["random-projection based methods"]}, {"label": "CONJUNCTION", "tokens": "We show how this can efficiently be combined with random-projection based methods for 2 NN search , such as [[ locality-sensitive hashing -LRB- LSH -RRB- ]] or << random projection trees >> .", "h": ["locality-sensitive hashing -LRB- LSH -RRB-"], "t": ["random projection trees"]}, {"label": "HYPONYM-OF", "tokens": "We show how this can efficiently be combined with << random-projection based methods >> for 2 NN search , such as locality-sensitive hashing -LRB- LSH -RRB- or [[ random projection trees ]] .", "h": ["random projection trees"], "t": ["random-projection based methods"]}, {"label": "COMPARE", "tokens": "We rigorously establish the correctness of the methodology and show by experimentation using LSH that [[ it ]] is competitive in practice with available << alternatives >> .", "h": ["it"], "t": ["alternatives"]}]