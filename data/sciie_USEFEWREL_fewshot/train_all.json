[{"label": "CONJUNCTION", "tokens": "The agreement in question involves number in [[ nouns ]] and << reflexive pronouns >> and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .", "h": ["nouns"], "t": ["reflexive pronouns"]}, {"label": "FEATURE-OF", "tokens": "The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English , like [[ grammatical gender ]] in << languages >> such as French , is partly arbitrary .", "h": ["grammatical gender"], "t": ["languages"]}, {"label": "HYPONYM-OF", "tokens": "The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in << languages >> such as [[ French ]] , is partly arbitrary .", "h": ["French"], "t": ["languages"]}, {"label": "USED-FOR", "tokens": "In this paper , a novel [[ method ]] to learn the << intrinsic object structure >> for robust visual tracking is proposed .", "h": ["method"], "t": ["intrinsic object structure"]}, {"label": "USED-FOR", "tokens": "In this paper , a novel method to learn the [[ intrinsic object structure ]] for << robust visual tracking >> is proposed .", "h": ["intrinsic object structure"], "t": ["robust visual tracking"]}, {"label": "FEATURE-OF", "tokens": "The basic assumption is that the << parameterized object state >> lies on a [[ low dimensional manifold ]] and can be learned from training data .", "h": ["low dimensional manifold"], "t": ["parameterized object state"]}, {"label": "USED-FOR", "tokens": "Based on this assumption , firstly we derived the [[ dimensionality reduction and density estimation algorithm ]] for << unsupervised learning of object intrinsic representation >> , the obtained non-rigid part of object state reduces even to 2 dimensions .", "h": ["dimensionality reduction and density estimation algorithm"], "t": ["unsupervised learning of object intrinsic representation"]}, {"label": "USED-FOR", "tokens": "Secondly the << dynamical model >> is derived and trained based on this [[ intrinsic representation ]] .", "h": ["intrinsic representation"], "t": ["dynamical model"]}, {"label": "PART-OF", "tokens": "Thirdly the learned [[ intrinsic object structure ]] is integrated into a << particle-filter style tracker >> .", "h": ["intrinsic object structure"], "t": ["particle-filter style tracker"]}, {"label": "USED-FOR", "tokens": "We will show that this intrinsic object representation has some interesting properties and based on which the newly derived [[ dynamical model ]] makes << particle-filter style tracker >> more robust and reliable .", "h": ["dynamical model"], "t": ["particle-filter style tracker"]}, {"label": "COMPARE", "tokens": "Experiments show that the learned [[ tracker ]] performs much better than existing << trackers >> on the tracking of complex non-rigid motions such as fish twisting with self-occlusion and large inter-frame lip motion .", "h": ["tracker"], "t": ["trackers"]}, {"label": "USED-FOR", "tokens": "Experiments show that the learned [[ tracker ]] performs much better than existing trackers on the << tracking of complex non-rigid motions >> such as fish twisting with self-occlusion and large inter-frame lip motion .", "h": ["tracker"], "t": ["tracking of complex non-rigid motions"]}, {"label": "USED-FOR", "tokens": "Experiments show that the learned tracker performs much better than existing [[ trackers ]] on the << tracking of complex non-rigid motions >> such as fish twisting with self-occlusion and large inter-frame lip motion .", "h": ["trackers"], "t": ["tracking of complex non-rigid motions"]}, {"label": "HYPONYM-OF", "tokens": "Experiments show that the learned tracker performs much better than existing trackers on the tracking of << complex non-rigid motions >> such as [[ fish twisting ]] with self-occlusion and large inter-frame lip motion .", "h": ["fish twisting"], "t": ["complex non-rigid motions"]}, {"label": "FEATURE-OF", "tokens": "Experiments show that the learned tracker performs much better than existing trackers on the tracking of complex non-rigid motions such as << fish twisting >> with [[ self-occlusion ]] and large inter-frame lip motion .", "h": ["self-occlusion"], "t": ["fish twisting"]}, {"label": "CONJUNCTION", "tokens": "Experiments show that the learned tracker performs much better than existing trackers on the tracking of complex non-rigid motions such as fish twisting with [[ self-occlusion ]] and large << inter-frame lip motion >> .", "h": ["self-occlusion"], "t": ["inter-frame lip motion"]}, {"label": "FEATURE-OF", "tokens": "Experiments show that the learned tracker performs much better than existing trackers on the tracking of complex non-rigid motions such as << fish twisting >> with self-occlusion and large [[ inter-frame lip motion ]] .", "h": ["inter-frame lip motion"], "t": ["fish twisting"]}, {"label": "USED-FOR", "tokens": "The proposed [[ method ]] also has the potential to solve other type of << tracking problems >> .", "h": ["method"], "t": ["tracking problems"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a [[ digital signal processor -LRB- DSP -RRB- implementation ]] of << real-time statistical voice conversion -LRB- VC -RRB- >> for silent speech enhancement and electrolaryngeal speech enhancement .", "h": ["digital signal processor -LRB- DSP -RRB- implementation"], "t": ["real-time statistical voice conversion -LRB- VC -RRB-"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a digital signal processor -LRB- DSP -RRB- implementation of [[ real-time statistical voice conversion -LRB- VC -RRB- ]] for << silent speech enhancement >> and electrolaryngeal speech enhancement .", "h": ["real-time statistical voice conversion -LRB- VC -RRB-"], "t": ["silent speech enhancement"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a digital signal processor -LRB- DSP -RRB- implementation of [[ real-time statistical voice conversion -LRB- VC -RRB- ]] for silent speech enhancement and << electrolaryngeal speech enhancement >> .", "h": ["real-time statistical voice conversion -LRB- VC -RRB-"], "t": ["electrolaryngeal speech enhancement"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we present a digital signal processor -LRB- DSP -RRB- implementation of real-time statistical voice conversion -LRB- VC -RRB- for [[ silent speech enhancement ]] and << electrolaryngeal speech enhancement >> .", "h": ["silent speech enhancement"], "t": ["electrolaryngeal speech enhancement"]}, {"label": "HYPONYM-OF", "tokens": "[[ Electrolaryngeal speech ]] is one of the typical types of << alaryngeal speech >> produced by an alternative speaking method for laryngectomees .", "h": ["Electrolaryngeal speech"], "t": ["alaryngeal speech"]}, {"label": "USED-FOR", "tokens": "Electrolaryngeal speech is one of the typical types of << alaryngeal speech >> produced by an alternative [[ speaking method ]] for laryngectomees .", "h": ["speaking method"], "t": ["alaryngeal speech"]}, {"label": "USED-FOR", "tokens": "Electrolaryngeal speech is one of the typical types of alaryngeal speech produced by an alternative [[ speaking method ]] for << laryngectomees >> .", "h": ["speaking method"], "t": ["laryngectomees"]}, {"label": "EVALUATE-FOR", "tokens": "However , the [[ sound quality ]] of << NAM and electrolaryngeal speech >> suffers from lack of naturalness .", "h": ["sound quality"], "t": ["NAM and electrolaryngeal speech"]}, {"label": "USED-FOR", "tokens": "VC has proven to be one of the promising approaches to address this problem , and << it >> has been successfully implemented on [[ devices ]] with sufficient computational resources .", "h": ["devices"], "t": ["it"]}, {"label": "FEATURE-OF", "tokens": "VC has proven to be one of the promising approaches to address this problem , and it has been successfully implemented on << devices >> with [[ sufficient computational resources ]] .", "h": ["sufficient computational resources"], "t": ["devices"]}, {"label": "FEATURE-OF", "tokens": "An implementation on << devices >> that are highly portable but have [[ limited computational resources ]] would greatly contribute to its practical use .", "h": ["limited computational resources"], "t": ["devices"]}, {"label": "USED-FOR", "tokens": "In this paper we further implement << real-time VC >> on a [[ DSP ]] .", "h": ["DSP"], "t": ["real-time VC"]}, {"label": "USED-FOR", "tokens": "To implement the two << speech enhancement systems >> based on [[ real-time VC ]] , one from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several methods for reducing computational cost while preserving conversion accuracy .", "h": ["real-time VC"], "t": ["speech enhancement systems"]}, {"label": "HYPONYM-OF", "tokens": "To implement the two << speech enhancement systems >> based on real-time VC , [[ one ]] from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several methods for reducing computational cost while preserving conversion accuracy .", "h": ["one"], "t": ["speech enhancement systems"]}, {"label": "CONJUNCTION", "tokens": "To implement the two speech enhancement systems based on real-time VC , [[ one ]] from NAM to a whispered voice and the << other >> from electrolaryngeal speech to a natural voice , we propose several methods for reducing computational cost while preserving conversion accuracy .", "h": ["one"], "t": ["other"]}, {"label": "HYPONYM-OF", "tokens": "To implement the two << speech enhancement systems >> based on real-time VC , one from NAM to a whispered voice and the [[ other ]] from electrolaryngeal speech to a natural voice , we propose several methods for reducing computational cost while preserving conversion accuracy .", "h": ["other"], "t": ["speech enhancement systems"]}, {"label": "EVALUATE-FOR", "tokens": "To implement the two speech enhancement systems based on real-time VC , one from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several << methods >> for reducing [[ computational cost ]] while preserving conversion accuracy .", "h": ["computational cost"], "t": ["methods"]}, {"label": "CONJUNCTION", "tokens": "To implement the two speech enhancement systems based on real-time VC , one from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several methods for reducing [[ computational cost ]] while preserving << conversion accuracy >> .", "h": ["computational cost"], "t": ["conversion accuracy"]}, {"label": "EVALUATE-FOR", "tokens": "To implement the two speech enhancement systems based on real-time VC , one from NAM to a whispered voice and the other from electrolaryngeal speech to a natural voice , we propose several << methods >> for reducing computational cost while preserving [[ conversion accuracy ]] .", "h": ["conversion accuracy"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "We conduct experimental evaluations and show that << real-time VC >> is capable of running on a [[ DSP ]] with little degradation .", "h": ["DSP"], "t": ["real-time VC"]}, {"label": "USED-FOR", "tokens": "We propose a [[ method ]] that automatically generates << paraphrase >> sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like BLEU and NIST .", "h": ["method"], "t": ["paraphrase"]}, {"label": "USED-FOR", "tokens": "We propose a method that automatically generates [[ paraphrase ]] sets from seed sentences to be used as reference sets in objective << machine translation evaluation measures >> like BLEU and NIST .", "h": ["paraphrase"], "t": ["machine translation evaluation measures"]}, {"label": "HYPONYM-OF", "tokens": "We propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective << machine translation evaluation measures >> like [[ BLEU ]] and NIST .", "h": ["BLEU"], "t": ["machine translation evaluation measures"]}, {"label": "CONJUNCTION", "tokens": "We propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective machine translation evaluation measures like [[ BLEU ]] and << NIST >> .", "h": ["BLEU"], "t": ["NIST"]}, {"label": "HYPONYM-OF", "tokens": "We propose a method that automatically generates paraphrase sets from seed sentences to be used as reference sets in objective << machine translation evaluation measures >> like BLEU and [[ NIST ]] .", "h": ["NIST"], "t": ["machine translation evaluation measures"]}, {"label": "CONJUNCTION", "tokens": "We measured the quality of the paraphrases produced in an experiment , i.e. , -LRB- i -RRB- their << grammaticality >> : at least 99 % correct sentences ; -LRB- ii -RRB- their [[ equivalence in meaning ]] : at least 96 % correct paraphrases either by meaning equivalence or entailment ; and , -LRB- iii -RRB- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .", "h": ["equivalence in meaning"], "t": ["grammaticality"]}, {"label": "USED-FOR", "tokens": "We measured the quality of the paraphrases produced in an experiment , i.e. , -LRB- i -RRB- their grammaticality : at least 99 % correct sentences ; -LRB- ii -RRB- their equivalence in meaning : at least 96 % correct << paraphrases >> either by [[ meaning equivalence ]] or entailment ; and , -LRB- iii -RRB- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .", "h": ["meaning equivalence"], "t": ["paraphrases"]}, {"label": "CONJUNCTION", "tokens": "We measured the quality of the paraphrases produced in an experiment , i.e. , -LRB- i -RRB- their grammaticality : at least 99 % correct sentences ; -LRB- ii -RRB- their equivalence in meaning : at least 96 % correct paraphrases either by [[ meaning equivalence ]] or << entailment >> ; and , -LRB- iii -RRB- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .", "h": ["meaning equivalence"], "t": ["entailment"]}, {"label": "USED-FOR", "tokens": "We measured the quality of the paraphrases produced in an experiment , i.e. , -LRB- i -RRB- their grammaticality : at least 99 % correct sentences ; -LRB- ii -RRB- their equivalence in meaning : at least 96 % correct << paraphrases >> either by meaning equivalence or [[ entailment ]] ; and , -LRB- iii -RRB- the amount of internal lexical and syntactical variation in a set of paraphrases : slightly superior to that of hand-produced sets .", "h": ["entailment"], "t": ["paraphrases"]}, {"label": "CONJUNCTION", "tokens": "We measured the quality of the paraphrases produced in an experiment , i.e. , -LRB- i -RRB- their grammaticality : at least 99 % correct sentences ; -LRB- ii -RRB- their << equivalence in meaning >> : at least 96 % correct paraphrases either by meaning equivalence or entailment ; and , -LRB- iii -RRB- the amount of [[ internal lexical and syntactical variation ]] in a set of paraphrases : slightly superior to that of hand-produced sets .", "h": ["internal lexical and syntactical variation"], "t": ["equivalence in meaning"]}, {"label": "COMPARE", "tokens": "We measured the quality of the paraphrases produced in an experiment , i.e. , -LRB- i -RRB- their grammaticality : at least 99 % correct sentences ; -LRB- ii -RRB- their equivalence in meaning : at least 96 % correct paraphrases either by meaning equivalence or entailment ; and , -LRB- iii -RRB- the amount of internal lexical and syntactical variation in a set of [[ paraphrases ]] : slightly superior to that of << hand-produced sets >> .", "h": ["paraphrases"], "t": ["hand-produced sets"]}, {"label": "USED-FOR", "tokens": "The << paraphrase >> sets produced by this [[ method ]] thus seem adequate as reference sets to be used for MT evaluation .", "h": ["method"], "t": ["paraphrase"]}, {"label": "PART-OF", "tokens": "[[ Graph unification ]] remains the most expensive part of << unification-based grammar parsing >> .", "h": ["Graph unification"], "t": ["unification-based grammar parsing"]}, {"label": "PART-OF", "tokens": "We focus on one [[ speed-up element ]] in the design of << unification algorithms >> : avoidance of copying of unmodified subgraphs .", "h": ["speed-up element"], "t": ["unification algorithms"]}, {"label": "USED-FOR", "tokens": "We propose a << method >> of attaining such a design through a method of [[ structure-sharing ]] which avoids log -LRB- d -RRB- overheads often associated with structure-sharing of graphs without any use of costly dependency pointers .", "h": ["structure-sharing"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The proposed [[ scheme ]] eliminates redundant copying while maintaining the quasi-destructive scheme 's ability to avoid over copying and early copying combined with its ability to handle << cyclic structures >> without algorithmic additions .", "h": ["scheme"], "t": ["cyclic structures"]}, {"label": "FEATURE-OF", "tokens": "The proposed << scheme >> eliminates redundant copying while maintaining the [[ quasi-destructive scheme 's ability ]] to avoid over copying and early copying combined with its ability to handle cyclic structures without algorithmic additions .", "h": ["quasi-destructive scheme 's ability"], "t": ["scheme"]}, {"label": "CONJUNCTION", "tokens": "The proposed scheme eliminates redundant copying while maintaining the quasi-destructive scheme 's ability to avoid [[ over copying ]] and << early copying >> combined with its ability to handle cyclic structures without algorithmic additions .", "h": ["over copying"], "t": ["early copying"]}, {"label": "USED-FOR", "tokens": "We describe a novel technique and implemented [[ system ]] for constructing a << subcategorization dictionary >> from textual corpora .", "h": ["system"], "t": ["subcategorization dictionary"]}, {"label": "USED-FOR", "tokens": "We describe a novel technique and implemented << system >> for constructing a subcategorization dictionary from [[ textual corpora ]] .", "h": ["textual corpora"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "We also demonstrate that a << subcategorization dictionary >> built with the [[ system ]] improves the accuracy of a parser by an appreciable amount", "h": ["system"], "t": ["subcategorization dictionary"]}, {"label": "EVALUATE-FOR", "tokens": "We also demonstrate that a subcategorization dictionary built with the system improves the [[ accuracy ]] of a << parser >> by an appreciable amount", "h": ["accuracy"], "t": ["parser"]}, {"label": "EVALUATE-FOR", "tokens": "We also demonstrate that a << subcategorization dictionary >> built with the system improves the accuracy of a [[ parser ]] by an appreciable amount", "h": ["parser"], "t": ["subcategorization dictionary"]}, {"label": "HYPONYM-OF", "tokens": "A number of powerful << registration criteria >> have been developed in the last decade , most prominently the criterion of [[ maximum mutual information ]] .", "h": ["maximum mutual information"], "t": ["registration criteria"]}, {"label": "FEATURE-OF", "tokens": "Although this criterion provides for good registration results in many applications , << it >> remains a purely [[ low-level criterion ]] .", "h": ["low-level criterion"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "In this paper , we will develop a [[ Bayesian framework ]] that allows to impose statistically learned prior knowledge about the joint intensity distribution into << image registration methods >> .", "h": ["Bayesian framework"], "t": ["image registration methods"]}, {"label": "USED-FOR", "tokens": "In this paper , we will develop a Bayesian framework that allows to impose [[ statistically learned prior knowledge ]] about the joint intensity distribution into << image registration methods >> .", "h": ["statistically learned prior knowledge"], "t": ["image registration methods"]}, {"label": "FEATURE-OF", "tokens": "In this paper , we will develop a Bayesian framework that allows to impose << statistically learned prior knowledge >> about the [[ joint intensity distribution ]] into image registration methods .", "h": ["joint intensity distribution"], "t": ["statistically learned prior knowledge"]}, {"label": "USED-FOR", "tokens": "The << prior >> is given by a [[ kernel density estimate ]] on the space of joint intensity distributions computed from a representative set of pre-registered image pairs .", "h": ["kernel density estimate"], "t": ["prior"]}, {"label": "USED-FOR", "tokens": "The prior is given by a [[ kernel density estimate ]] on the space of << joint intensity distributions >> computed from a representative set of pre-registered image pairs .", "h": ["kernel density estimate"], "t": ["joint intensity distributions"]}, {"label": "USED-FOR", "tokens": "The prior is given by a kernel density estimate on the space of << joint intensity distributions >> computed from a representative set of [[ pre-registered image pairs ]] .", "h": ["pre-registered image pairs"], "t": ["joint intensity distributions"]}, {"label": "USED-FOR", "tokens": "Experimental results demonstrate that the resulting [[ registration process ]] is more robust to << missing low-level information >> as it favors intensity correspondences statistically consistent with the learned intensity distributions .", "h": ["registration process"], "t": ["missing low-level information"]}, {"label": "USED-FOR", "tokens": "Experimental results demonstrate that the resulting registration process is more robust to missing low-level information as [[ it ]] favors << intensity correspondences >> statistically consistent with the learned intensity distributions .", "h": ["it"], "t": ["intensity correspondences"]}, {"label": "USED-FOR", "tokens": "We present a [[ method ]] for << synthesizing complex , photo-realistic facade images >> , from a single example .", "h": ["method"], "t": ["synthesizing complex , photo-realistic facade images"]}, {"label": "USED-FOR", "tokens": "After parsing the example image into its << semantic components >> , a [[ tiling ]] for it is generated .", "h": ["tiling"], "t": ["semantic components"]}, {"label": "FEATURE-OF", "tokens": "Novel tilings can then be created , yielding << facade textures >> with different dimensions or with [[ occluded parts inpainted ]] .", "h": ["occluded parts inpainted"], "t": ["facade textures"]}, {"label": "USED-FOR", "tokens": "A [[ genetic algorithm ]] guides the novel << facades >> as well as inpainted parts to be consistent with the example , both in terms of their overall structure and their detailed textures .", "h": ["genetic algorithm"], "t": ["facades"]}, {"label": "USED-FOR", "tokens": "A [[ genetic algorithm ]] guides the novel facades as well as << inpainted parts >> to be consistent with the example , both in terms of their overall structure and their detailed textures .", "h": ["genetic algorithm"], "t": ["inpainted parts"]}, {"label": "EVALUATE-FOR", "tokens": "Promising results for [[ multiple standard datasets ]] -- in particular for the different building styles they contain -- demonstrate the potential of the << method >> .", "h": ["multiple standard datasets"], "t": ["method"]}, {"label": "HYPONYM-OF", "tokens": "We introduce a new << interactive corpus exploration tool >> called [[ InfoMagnets ]] .", "h": ["InfoMagnets"], "t": ["interactive corpus exploration tool"]}, {"label": "USED-FOR", "tokens": "[[ InfoMagnets ]] aims at making << exploratory corpus analysis >> accessible to researchers who are not experts in text mining .", "h": ["InfoMagnets"], "t": ["exploratory corpus analysis"]}, {"label": "USED-FOR", "tokens": "As evidence of its usefulness and usability , [[ it ]] has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct << domains >> : tutorial dialogue -LRB- Kumar et al. , submitted -RRB- and on-line communities -LRB- Arguello et al. , 2006 -RRB- .", "h": ["it"], "t": ["domains"]}, {"label": "HYPONYM-OF", "tokens": "As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct << domains >> : [[ tutorial dialogue ]] -LRB- Kumar et al. , submitted -RRB- and on-line communities -LRB- Arguello et al. , 2006 -RRB- .", "h": ["tutorial dialogue"], "t": ["domains"]}, {"label": "CONJUNCTION", "tokens": "As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct domains : [[ tutorial dialogue ]] -LRB- Kumar et al. , submitted -RRB- and << on-line communities >> -LRB- Arguello et al. , 2006 -RRB- .", "h": ["tutorial dialogue"], "t": ["on-line communities"]}, {"label": "HYPONYM-OF", "tokens": "As evidence of its usefulness and usability , it has been used successfully in a research context to uncover relationships between language and behavioral patterns in two distinct << domains >> : tutorial dialogue -LRB- Kumar et al. , submitted -RRB- and [[ on-line communities ]] -LRB- Arguello et al. , 2006 -RRB- .", "h": ["on-line communities"], "t": ["domains"]}, {"label": "USED-FOR", "tokens": "As an [[ educational tool ]] , it has been used as part of a unit on << protocol analysis >> in an Educational Research Methods course .", "h": ["educational tool"], "t": ["protocol analysis"]}, {"label": "USED-FOR", "tokens": "Sources of training data suitable for << language modeling >> of [[ conversational speech ]] are limited .", "h": ["conversational speech"], "t": ["language modeling"]}, {"label": "USED-FOR", "tokens": "In this paper , we show how training data can be supplemented with text from the web filtered to match the style and/or topic of the target << recognition task >> , but also that it is possible to get bigger performance gains from the data by using [[ class-dependent interpolation of N-grams ]] .", "h": ["class-dependent interpolation of N-grams"], "t": ["recognition task"]}, {"label": "USED-FOR", "tokens": "We present a [[ method ]] for << detecting 3D objects >> using multi-modalities .", "h": ["method"], "t": ["detecting 3D objects"]}, {"label": "USED-FOR", "tokens": "We present a << method >> for detecting 3D objects using [[ multi-modalities ]] .", "h": ["multi-modalities"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "While [[ it ]] is generic , we demonstrate << it >> on the combination of an image and a dense depth map which give complementary object information .", "h": ["it"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "While it is generic , we demonstrate << it >> on the combination of an [[ image ]] and a dense depth map which give complementary object information .", "h": ["image"], "t": ["it"]}, {"label": "CONJUNCTION", "tokens": "While it is generic , we demonstrate it on the combination of an [[ image ]] and a << dense depth map >> which give complementary object information .", "h": ["image"], "t": ["dense depth map"]}, {"label": "USED-FOR", "tokens": "While it is generic , we demonstrate << it >> on the combination of an image and a [[ dense depth map ]] which give complementary object information .", "h": ["dense depth map"], "t": ["it"]}, {"label": "FEATURE-OF", "tokens": "While it is generic , we demonstrate it on the combination of an image and a << dense depth map >> which give [[ complementary object information ]] .", "h": ["complementary object information"], "t": ["dense depth map"]}, {"label": "USED-FOR", "tokens": "It is based on an efficient representation of [[ templates ]] that capture the different << modalities >> , and we show in many experiments on commodity hardware that our approach significantly outperforms state-of-the-art methods on single modalities .", "h": ["templates"], "t": ["modalities"]}, {"label": "COMPARE", "tokens": "It is based on an efficient representation of templates that capture the different modalities , and we show in many experiments on commodity hardware that our [[ approach ]] significantly outperforms << state-of-the-art methods >> on single modalities .", "h": ["approach"], "t": ["state-of-the-art methods"]}, {"label": "USED-FOR", "tokens": "It is based on an efficient representation of templates that capture the different modalities , and we show in many experiments on commodity hardware that our [[ approach ]] significantly outperforms state-of-the-art methods on << single modalities >> .", "h": ["approach"], "t": ["single modalities"]}, {"label": "USED-FOR", "tokens": "It is based on an efficient representation of templates that capture the different modalities , and we show in many experiments on commodity hardware that our approach significantly outperforms [[ state-of-the-art methods ]] on << single modalities >> .", "h": ["state-of-the-art methods"], "t": ["single modalities"]}, {"label": "USED-FOR", "tokens": "The [[ compact description of a video sequence ]] through a single image map and a dominant motion has applications in several << domains >> , including video browsing and retrieval , compression , mosaicing , and visual summarization .", "h": ["compact description of a video sequence"], "t": ["domains"]}, {"label": "USED-FOR", "tokens": "The << compact description of a video sequence >> through a single [[ image map ]] and a dominant motion has applications in several domains , including video browsing and retrieval , compression , mosaicing , and visual summarization .", "h": ["image map"], "t": ["compact description of a video sequence"]}, {"label": "CONJUNCTION", "tokens": "The compact description of a video sequence through a single [[ image map ]] and a << dominant motion >> has applications in several domains , including video browsing and retrieval , compression , mosaicing , and visual summarization .", "h": ["image map"], "t": ["dominant motion"]}, {"label": "USED-FOR", "tokens": "The << compact description of a video sequence >> through a single image map and a [[ dominant motion ]] has applications in several domains , including video browsing and retrieval , compression , mosaicing , and visual summarization .", "h": ["dominant motion"], "t": ["compact description of a video sequence"]}, {"label": "HYPONYM-OF", "tokens": "The compact description of a video sequence through a single image map and a dominant motion has applications in several << domains >> , including [[ video browsing and retrieval ]] , compression , mosaicing , and visual summarization .", "h": ["video browsing and retrieval"], "t": ["domains"]}, {"label": "CONJUNCTION", "tokens": "The compact description of a video sequence through a single image map and a dominant motion has applications in several domains , including [[ video browsing and retrieval ]] , << compression >> , mosaicing , and visual summarization .", "h": ["video browsing and retrieval"], "t": ["compression"]}, {"label": "HYPONYM-OF", "tokens": "The compact description of a video sequence through a single image map and a dominant motion has applications in several << domains >> , including video browsing and retrieval , [[ compression ]] , mosaicing , and visual summarization .", "h": ["compression"], "t": ["domains"]}, {"label": "CONJUNCTION", "tokens": "The compact description of a video sequence through a single image map and a dominant motion has applications in several domains , including video browsing and retrieval , [[ compression ]] , << mosaicing >> , and visual summarization .", "h": ["compression"], "t": ["mosaicing"]}, {"label": "HYPONYM-OF", "tokens": "The compact description of a video sequence through a single image map and a dominant motion has applications in several << domains >> , including video browsing and retrieval , compression , [[ mosaicing ]] , and visual summarization .", "h": ["mosaicing"], "t": ["domains"]}, {"label": "CONJUNCTION", "tokens": "The compact description of a video sequence through a single image map and a dominant motion has applications in several domains , including video browsing and retrieval , compression , [[ mosaicing ]] , and << visual summarization >> .", "h": ["mosaicing"], "t": ["visual summarization"]}, {"label": "USED-FOR", "tokens": "Building such a representation requires the capability to register all the frames with respect to the dominant object in the scene , a << task >> which has been , in the past , addressed through temporally [[ localized motion estimates ]] .", "h": ["localized motion estimates"], "t": ["task"]}, {"label": "USED-FOR", "tokens": "To avoid this oscillation , we augment the << motion model >> with a [[ generic temporal constraint ]] which increases the robustness against competing interpretations , leading to more meaningful content summarization .", "h": ["generic temporal constraint"], "t": ["motion model"]}, {"label": "USED-FOR", "tokens": "To avoid this oscillation , we augment the motion model with a [[ generic temporal constraint ]] which increases the robustness against competing interpretations , leading to more meaningful << content summarization >> .", "h": ["generic temporal constraint"], "t": ["content summarization"]}, {"label": "EVALUATE-FOR", "tokens": "To avoid this oscillation , we augment the motion model with a << generic temporal constraint >> which increases the [[ robustness ]] against competing interpretations , leading to more meaningful content summarization .", "h": ["robustness"], "t": ["generic temporal constraint"]}, {"label": "PART-OF", "tokens": "In cross-domain learning , there is a more challenging problem that the << domain divergence >> involves more than one [[ dominant factors ]] , e.g. , different viewpoints , various resolutions and changing illuminations .", "h": ["dominant factors"], "t": ["domain divergence"]}, {"label": "HYPONYM-OF", "tokens": "In cross-domain learning , there is a more challenging problem that the domain divergence involves more than one << dominant factors >> , e.g. , different [[ viewpoints ]] , various resolutions and changing illuminations .", "h": ["viewpoints"], "t": ["dominant factors"]}, {"label": "CONJUNCTION", "tokens": "In cross-domain learning , there is a more challenging problem that the domain divergence involves more than one dominant factors , e.g. , different [[ viewpoints ]] , various << resolutions >> and changing illuminations .", "h": ["viewpoints"], "t": ["resolutions"]}, {"label": "HYPONYM-OF", "tokens": "In cross-domain learning , there is a more challenging problem that the domain divergence involves more than one << dominant factors >> , e.g. , different viewpoints , various [[ resolutions ]] and changing illuminations .", "h": ["resolutions"], "t": ["dominant factors"]}, {"label": "CONJUNCTION", "tokens": "In cross-domain learning , there is a more challenging problem that the domain divergence involves more than one dominant factors , e.g. , different viewpoints , various [[ resolutions ]] and changing << illuminations >> .", "h": ["resolutions"], "t": ["illuminations"]}, {"label": "USED-FOR", "tokens": "Fortunately , an [[ intermediate domain ]] could often be found to build a bridge across them to facilitate the << learning problem >> .", "h": ["intermediate domain"], "t": ["learning problem"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a [[ Coupled Marginalized Denoising Auto-encoders framework ]] to address the << cross-domain problem >> .", "h": ["Coupled Marginalized Denoising Auto-encoders framework"], "t": ["cross-domain problem"]}, {"label": "HYPONYM-OF", "tokens": "Specifically , we design two << marginalized denoising auto-encoders >> , [[ one ]] for the target and the other for source as well as the intermediate one .", "h": ["one"], "t": ["marginalized denoising auto-encoders"]}, {"label": "CONJUNCTION", "tokens": "Specifically , we design two marginalized denoising auto-encoders , [[ one ]] for the target and the << other >> for source as well as the intermediate one .", "h": ["one"], "t": ["other"]}, {"label": "HYPONYM-OF", "tokens": "Specifically , we design two << marginalized denoising auto-encoders >> , one for the target and the [[ other ]] for source as well as the intermediate one .", "h": ["other"], "t": ["marginalized denoising auto-encoders"]}, {"label": "PART-OF", "tokens": "To better couple the two << denoising auto-encoders learning >> , we incorporate a [[ feature mapping ]] , which tends to transfer knowledge between the intermediate domain and the target one .", "h": ["feature mapping"], "t": ["denoising auto-encoders learning"]}, {"label": "USED-FOR", "tokens": "To better couple the two denoising auto-encoders learning , we incorporate a [[ feature mapping ]] , which tends to transfer knowledge between the << intermediate domain >> and the target one .", "h": ["feature mapping"], "t": ["intermediate domain"]}, {"label": "HYPONYM-OF", "tokens": "Furthermore , the << maximum margin criterion >> , e.g. , [[ intra-class com-pactness ]] and inter-class penalty , on the output layer is imposed to seek more discriminative features across different domains .", "h": ["intra-class com-pactness"], "t": ["maximum margin criterion"]}, {"label": "CONJUNCTION", "tokens": "Furthermore , the maximum margin criterion , e.g. , [[ intra-class com-pactness ]] and << inter-class penalty >> , on the output layer is imposed to seek more discriminative features across different domains .", "h": ["intra-class com-pactness"], "t": ["inter-class penalty"]}, {"label": "HYPONYM-OF", "tokens": "Furthermore , the << maximum margin criterion >> , e.g. , intra-class com-pactness and [[ inter-class penalty ]] , on the output layer is imposed to seek more discriminative features across different domains .", "h": ["inter-class penalty"], "t": ["maximum margin criterion"]}, {"label": "EVALUATE-FOR", "tokens": "Extensive experiments on two [[ tasks ]] have demonstrated the superiority of our << method >> over the state-of-the-art methods .", "h": ["tasks"], "t": ["method"]}, {"label": "COMPARE", "tokens": "Extensive experiments on two tasks have demonstrated the superiority of our [[ method ]] over the << state-of-the-art methods >> .", "h": ["method"], "t": ["state-of-the-art methods"]}, {"label": "PART-OF", "tokens": "Basically , a set of << age-group specific dictionaries >> are learned , where the [[ dictionary bases ]] corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups , and a linear combination of these patterns expresses a particular personalized aging process .", "h": ["dictionary bases"], "t": ["age-group specific dictionaries"]}, {"label": "USED-FOR", "tokens": "Basically , a set of age-group specific dictionaries are learned , where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups , and a [[ linear combination ]] of these patterns expresses a particular << personalized aging process >> .", "h": ["linear combination"], "t": ["personalized aging process"]}, {"label": "USED-FOR", "tokens": "Basically , a set of age-group specific dictionaries are learned , where the dictionary bases corresponding to the same index yet from different dictionaries form a particular aging process pattern cross different age groups , and a << linear combination >> of these [[ patterns ]] expresses a particular personalized aging process .", "h": ["patterns"], "t": ["linear combination"]}, {"label": "HYPONYM-OF", "tokens": "First , beyond the aging dictionaries , each subject may have extra << personalized facial characteristics >> , e.g. [[ mole ]] , which are invariant in the aging process .", "h": ["mole"], "t": ["personalized facial characteristics"]}, {"label": "USED-FOR", "tokens": "Thus a [[ personality-aware coupled reconstruction loss ]] is utilized to learn the << dictionaries >> based on face pairs from neighboring age groups .", "h": ["personality-aware coupled reconstruction loss"], "t": ["dictionaries"]}, {"label": "COMPARE", "tokens": "Extensive experiments well demonstrate the advantages of our proposed [[ solution ]] over other << state-of-the-arts >> in term of personalized aging progression , as well as the performance gain for cross-age face verification by synthesizing aging faces .", "h": ["solution"], "t": ["state-of-the-arts"]}, {"label": "USED-FOR", "tokens": "Extensive experiments well demonstrate the advantages of our proposed [[ solution ]] over other state-of-the-arts in term of << personalized aging progression >> , as well as the performance gain for cross-age face verification by synthesizing aging faces .", "h": ["solution"], "t": ["personalized aging progression"]}, {"label": "USED-FOR", "tokens": "Extensive experiments well demonstrate the advantages of our proposed solution over other [[ state-of-the-arts ]] in term of << personalized aging progression >> , as well as the performance gain for cross-age face verification by synthesizing aging faces .", "h": ["state-of-the-arts"], "t": ["personalized aging progression"]}, {"label": "USED-FOR", "tokens": "Extensive experiments well demonstrate the advantages of our proposed solution over other state-of-the-arts in term of personalized aging progression , as well as the performance gain for << cross-age face verification >> by [[ synthesizing aging faces ]] .", "h": ["synthesizing aging faces"], "t": ["cross-age face verification"]}, {"label": "USED-FOR", "tokens": "We propose a draft scheme of the [[ model ]] formalizing the << structure of communicative context >> in dialogue interaction .", "h": ["model"], "t": ["structure of communicative context"]}, {"label": "FEATURE-OF", "tokens": "We propose a draft scheme of the model formalizing the << structure of communicative context >> in [[ dialogue interaction ]] .", "h": ["dialogue interaction"], "t": ["structure of communicative context"]}, {"label": "USED-FOR", "tokens": "Visitors who browse the web from wireless PDAs , cell phones , and pagers are frequently stymied by [[ web interfaces ]] optimized for << desktop PCs >> .", "h": ["web interfaces"], "t": ["desktop PCs"]}, {"label": "USED-FOR", "tokens": "In this paper we develop an [[ algorithm ]] , MINPATH , that automatically improves << wireless web navigation >> by suggesting useful shortcut links in real time .", "h": ["algorithm"], "t": ["wireless web navigation"]}, {"label": "USED-FOR", "tokens": "In this paper we develop an [[ algorithm ]] , MINPATH , that automatically improves << wireless web navigation >> by suggesting useful shortcut links in real time .", "h": ["algorithm"], "t": ["wireless web navigation"]}, {"label": "USED-FOR", "tokens": "<< MINPATH >> finds shortcuts by using a learned [[ model ]] of web visitor behavior to estimate the savings of shortcut links , and suggests only the few best links .", "h": ["model"], "t": ["MINPATH"]}, {"label": "USED-FOR", "tokens": "MINPATH finds shortcuts by using a learned [[ model ]] of << web visitor behavior >> to estimate the savings of shortcut links , and suggests only the few best links .", "h": ["model"], "t": ["web visitor behavior"]}, {"label": "USED-FOR", "tokens": "MINPATH finds shortcuts by using a learned [[ model ]] of web visitor behavior to estimate the << savings of shortcut links >> , and suggests only the few best links .", "h": ["model"], "t": ["savings of shortcut links"]}, {"label": "HYPONYM-OF", "tokens": "We explore a variety of << predictive models >> , including [[ Na \u00a8 \u0131ve Bayes mixture models ]] and mixtures of Markov models , and report empirical evidence that MINPATH finds useful shortcuts that save substantial navigational effort .", "h": ["Na \u00a8 \u0131ve Bayes mixture models"], "t": ["predictive models"]}, {"label": "CONJUNCTION", "tokens": "We explore a variety of predictive models , including [[ Na \u00a8 \u0131ve Bayes mixture models ]] and << mixtures of Markov models >> , and report empirical evidence that MINPATH finds useful shortcuts that save substantial navigational effort .", "h": ["Na \u00a8 \u0131ve Bayes mixture models"], "t": ["mixtures of Markov models"]}, {"label": "HYPONYM-OF", "tokens": "We explore a variety of << predictive models >> , including Na \u00a8 \u0131ve Bayes mixture models and [[ mixtures of Markov models ]] , and report empirical evidence that MINPATH finds useful shortcuts that save substantial navigational effort .", "h": ["mixtures of Markov models"], "t": ["predictive models"]}, {"label": "USED-FOR", "tokens": "This paper describes a particular [[ approach ]] to << parsing >> that utilizes recent advances in unification-based parsing and in classification-based knowledge representation .", "h": ["approach"], "t": ["parsing"]}, {"label": "USED-FOR", "tokens": "This paper describes a particular << approach >> to parsing that utilizes recent advances in [[ unification-based parsing ]] and in classification-based knowledge representation .", "h": ["unification-based parsing"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "This paper describes a particular << approach >> to parsing that utilizes recent advances in unification-based parsing and in [[ classification-based knowledge representation ]] .", "h": ["classification-based knowledge representation"], "t": ["approach"]}, {"label": "CONJUNCTION", "tokens": "This paper describes a particular approach to parsing that utilizes recent advances in << unification-based parsing >> and in [[ classification-based knowledge representation ]] .", "h": ["classification-based knowledge representation"], "t": ["unification-based parsing"]}, {"label": "USED-FOR", "tokens": "As [[ unification-based grammatical frameworks ]] are extended to handle richer descriptions of << linguistic information >> , they begin to share many of the properties that have been developed in KL-ONE-like knowledge representation systems .", "h": ["unification-based grammatical frameworks"], "t": ["linguistic information"]}, {"label": "USED-FOR", "tokens": "As unification-based grammatical frameworks are extended to handle richer descriptions of linguistic information , << they >> begin to share many of the properties that have been developed in [[ KL-ONE-like knowledge representation systems ]] .", "h": ["KL-ONE-like knowledge representation systems"], "t": ["they"]}, {"label": "USED-FOR", "tokens": "This commonality suggests that some of the [[ classification-based representation techniques ]] can be applied to << unification-based linguistic descriptions >> .", "h": ["classification-based representation techniques"], "t": ["unification-based linguistic descriptions"]}, {"label": "USED-FOR", "tokens": "This merging supports the integration of [[ semantic and syntactic information ]] into the same << system >> , simultaneously subject to the same types of processes , in an efficient manner .", "h": ["semantic and syntactic information"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "The use of a [[ KL-ONE style representation ]] for << parsing >> and semantic interpretation was first explored in the PSI-KLONE system -LSB- 2 -RSB- , in which parsing is characterized as an inference process called incremental description refinement .", "h": ["KL-ONE style representation"], "t": ["parsing"]}, {"label": "USED-FOR", "tokens": "The use of a [[ KL-ONE style representation ]] for parsing and << semantic interpretation >> was first explored in the PSI-KLONE system -LSB- 2 -RSB- , in which parsing is characterized as an inference process called incremental description refinement .", "h": ["KL-ONE style representation"], "t": ["semantic interpretation"]}, {"label": "CONJUNCTION", "tokens": "The use of a KL-ONE style representation for [[ parsing ]] and << semantic interpretation >> was first explored in the PSI-KLONE system -LSB- 2 -RSB- , in which parsing is characterized as an inference process called incremental description refinement .", "h": ["parsing"], "t": ["semantic interpretation"]}, {"label": "USED-FOR", "tokens": "The use of a << KL-ONE style representation >> for parsing and semantic interpretation was first explored in the [[ PSI-KLONE system ]] -LSB- 2 -RSB- , in which parsing is characterized as an inference process called incremental description refinement .", "h": ["PSI-KLONE system"], "t": ["KL-ONE style representation"]}, {"label": "USED-FOR", "tokens": "The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system -LSB- 2 -RSB- , in which << parsing >> is characterized as an inference process called [[ incremental description refinement ]] .", "h": ["incremental description refinement"], "t": ["parsing"]}, {"label": "HYPONYM-OF", "tokens": "The use of a KL-ONE style representation for parsing and semantic interpretation was first explored in the PSI-KLONE system -LSB- 2 -RSB- , in which parsing is characterized as an << inference process >> called [[ incremental description refinement ]] .", "h": ["incremental description refinement"], "t": ["inference process"]}, {"label": "USED-FOR", "tokens": "In this paper we discuss a proposed [[ user knowledge modeling architecture ]] for the << ICICLE system >> , a language tutoring application for deaf learners of written English .", "h": ["user knowledge modeling architecture"], "t": ["ICICLE system"]}, {"label": "HYPONYM-OF", "tokens": "In this paper we discuss a proposed user knowledge modeling architecture for the [[ ICICLE system ]] , a << language tutoring application >> for deaf learners of written English .", "h": ["ICICLE system"], "t": ["language tutoring application"]}, {"label": "USED-FOR", "tokens": "In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system , a [[ language tutoring application ]] for << deaf learners >> of written English .", "h": ["language tutoring application"], "t": ["deaf learners"]}, {"label": "USED-FOR", "tokens": "In this paper we discuss a proposed user knowledge modeling architecture for the ICICLE system , a << language tutoring application >> for deaf learners of [[ written English ]] .", "h": ["written English"], "t": ["language tutoring application"]}, {"label": "USED-FOR", "tokens": "The [[ model ]] will represent the language proficiency of the user and is designed to be referenced during both << writing analysis >> and feedback production .", "h": ["model"], "t": ["writing analysis"]}, {"label": "USED-FOR", "tokens": "The [[ model ]] will represent the language proficiency of the user and is designed to be referenced during both writing analysis and << feedback production >> .", "h": ["model"], "t": ["feedback production"]}, {"label": "CONJUNCTION", "tokens": "The model will represent the language proficiency of the user and is designed to be referenced during both [[ writing analysis ]] and << feedback production >> .", "h": ["writing analysis"], "t": ["feedback production"]}, {"label": "USED-FOR", "tokens": "We motivate our << model design >> by citing relevant research on [[ second language and cognitive skill acquisition ]] , and briefly discuss preliminary empirical evidence supporting the design .", "h": ["second language and cognitive skill acquisition"], "t": ["model design"]}, {"label": "USED-FOR", "tokens": "We conclude by showing how our [[ design ]] can provide a rich and robust information base to a << language assessment / correction application >> by modeling user proficiency at a high level of granularity and specificity .", "h": ["design"], "t": ["language assessment / correction application"]}, {"label": "USED-FOR", "tokens": "We conclude by showing how our [[ design ]] can provide a rich and robust information base to a language assessment / correction application by modeling << user proficiency >> at a high level of granularity and specificity .", "h": ["design"], "t": ["user proficiency"]}, {"label": "EVALUATE-FOR", "tokens": "We conclude by showing how our design can provide a rich and robust information base to a language assessment / correction application by modeling << user proficiency >> at a high level of [[ granularity ]] and specificity .", "h": ["granularity"], "t": ["user proficiency"]}, {"label": "CONJUNCTION", "tokens": "We conclude by showing how our design can provide a rich and robust information base to a language assessment / correction application by modeling user proficiency at a high level of [[ granularity ]] and << specificity >> .", "h": ["granularity"], "t": ["specificity"]}, {"label": "EVALUATE-FOR", "tokens": "We conclude by showing how our design can provide a rich and robust information base to a language assessment / correction application by modeling << user proficiency >> at a high level of granularity and [[ specificity ]] .", "h": ["specificity"], "t": ["user proficiency"]}, {"label": "PART-OF", "tokens": "[[ Constraint propagation ]] is one of the key techniques in << constraint programming >> , and a large body of work has built up around it .", "h": ["Constraint propagation"], "t": ["constraint programming"]}, {"label": "USED-FOR", "tokens": "In this paper we present << SHORTSTR2 >> , a development of the [[ Simple Tabular Reduction algorithm STR2 + ]] .", "h": ["Simple Tabular Reduction algorithm STR2 +"], "t": ["SHORTSTR2"]}, {"label": "COMPARE", "tokens": "We show that [[ SHORTSTR2 ]] is complementary to the existing algorithms << SHORTGAC >> and HAGGISGAC that exploit short supports , while being much simpler .", "h": ["SHORTSTR2"], "t": ["SHORTGAC"]}, {"label": "COMPARE", "tokens": "We show that [[ SHORTSTR2 ]] is complementary to the existing algorithms SHORTGAC and << HAGGISGAC >> that exploit short supports , while being much simpler .", "h": ["SHORTSTR2"], "t": ["HAGGISGAC"]}, {"label": "CONJUNCTION", "tokens": "We show that SHORTSTR2 is complementary to the existing algorithms [[ SHORTGAC ]] and << HAGGISGAC >> that exploit short supports , while being much simpler .", "h": ["SHORTGAC"], "t": ["HAGGISGAC"]}, {"label": "COMPARE", "tokens": "When a constraint is amenable to short supports , the [[ short support set ]] can be exponentially smaller than the << full-length support set >> .", "h": ["short support set"], "t": ["full-length support set"]}, {"label": "USED-FOR", "tokens": "We also show that [[ SHORTSTR2 ]] can be combined with a simple algorithm to identify << short supports >> from full-length supports , to provide a superior drop-in replacement for STR2 + .", "h": ["SHORTSTR2"], "t": ["short supports"]}, {"label": "USED-FOR", "tokens": "We also show that [[ SHORTSTR2 ]] can be combined with a simple algorithm to identify short supports from full-length supports , to provide a superior << drop-in replacement >> for STR2 + .", "h": ["SHORTSTR2"], "t": ["drop-in replacement"]}, {"label": "CONJUNCTION", "tokens": "We also show that << SHORTSTR2 >> can be combined with a simple [[ algorithm ]] to identify short supports from full-length supports , to provide a superior drop-in replacement for STR2 + .", "h": ["algorithm"], "t": ["SHORTSTR2"]}, {"label": "USED-FOR", "tokens": "We also show that SHORTSTR2 can be combined with a simple [[ algorithm ]] to identify << short supports >> from full-length supports , to provide a superior drop-in replacement for STR2 + .", "h": ["algorithm"], "t": ["short supports"]}, {"label": "USED-FOR", "tokens": "We also show that << SHORTSTR2 >> can be combined with a simple algorithm to identify short supports from [[ full-length supports ]] , to provide a superior drop-in replacement for STR2 + .", "h": ["full-length supports"], "t": ["SHORTSTR2"]}, {"label": "USED-FOR", "tokens": "We also show that << SHORTSTR2 >> can be combined with a simple algorithm to identify short supports from [[ full-length supports ]] , to provide a superior drop-in replacement for STR2 + .", "h": ["full-length supports"], "t": ["SHORTSTR2"]}, {"label": "USED-FOR", "tokens": "We also show that SHORTSTR2 can be combined with a simple << algorithm >> to identify short supports from [[ full-length supports ]] , to provide a superior drop-in replacement for STR2 + .", "h": ["full-length supports"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "We also show that SHORTSTR2 can be combined with a simple << algorithm >> to identify short supports from [[ full-length supports ]] , to provide a superior drop-in replacement for STR2 + .", "h": ["full-length supports"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "We also show that SHORTSTR2 can be combined with a simple algorithm to identify short supports from full-length supports , to provide a superior [[ drop-in replacement ]] for << STR2 + >> .", "h": ["drop-in replacement"], "t": ["STR2 +"]}, {"label": "USED-FOR", "tokens": "We propose a [[ detection method ]] for << orthographic variants >> caused by transliteration in a large corpus .", "h": ["detection method"], "t": ["orthographic variants"]}, {"label": "USED-FOR", "tokens": "The << method >> employs two [[ similarities ]] .", "h": ["similarities"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "One is << string similarity >> based on [[ edit distance ]] .", "h": ["edit distance"], "t": ["string similarity"]}, {"label": "USED-FOR", "tokens": "The other is << contextual similarity >> by a [[ vector space model ]] .", "h": ["vector space model"], "t": ["contextual similarity"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results show that the << method >> performed a 0.889 [[ F-measure ]] in an open test .", "h": ["F-measure"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "[[ Uncertainty handling ]] plays an important role during << shape tracking >> .", "h": ["Uncertainty handling"], "t": ["shape tracking"]}, {"label": "USED-FOR", "tokens": "We have recently shown that the [[ fusion of measurement information with system dynamics and shape priors ]] greatly improves the << tracking >> performance for very noisy images such as ultrasound sequences -LSB- 22 -RSB- .", "h": ["fusion of measurement information with system dynamics and shape priors"], "t": ["tracking"]}, {"label": "USED-FOR", "tokens": "We have recently shown that the fusion of measurement information with system dynamics and shape priors greatly improves the [[ tracking ]] performance for very << noisy images >> such as ultrasound sequences -LSB- 22 -RSB- .", "h": ["tracking"], "t": ["noisy images"]}, {"label": "HYPONYM-OF", "tokens": "We have recently shown that the fusion of measurement information with system dynamics and shape priors greatly improves the tracking performance for very << noisy images >> such as [[ ultrasound sequences ]] -LSB- 22 -RSB- .", "h": ["ultrasound sequences"], "t": ["noisy images"]}, {"label": "USED-FOR", "tokens": "Nevertheless , this << approach >> required [[ user initialization ]] of the tracking process .", "h": ["user initialization"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "Nevertheless , this approach required [[ user initialization ]] of the << tracking process >> .", "h": ["user initialization"], "t": ["tracking process"]}, {"label": "USED-FOR", "tokens": "This paper solves the << automatic initial-ization problem >> by performing [[ boosted shape detection ]] as a generic measurement process and integrating it in our tracking framework .", "h": ["boosted shape detection"], "t": ["automatic initial-ization problem"]}, {"label": "USED-FOR", "tokens": "This paper solves the automatic initial-ization problem by performing << boosted shape detection >> as a [[ generic measurement process ]] and integrating it in our tracking framework .", "h": ["generic measurement process"], "t": ["boosted shape detection"]}, {"label": "PART-OF", "tokens": "This paper solves the automatic initial-ization problem by performing boosted shape detection as a generic measurement process and integrating [[ it ]] in our << tracking framework >> .", "h": ["it"], "t": ["tracking framework"]}, {"label": "USED-FOR", "tokens": "As a result , we treat all sources of information in a unified way and derive the << posterior shape model >> as the shape with the [[ maximum likelihood ]] .", "h": ["maximum likelihood"], "t": ["posterior shape model"]}, {"label": "USED-FOR", "tokens": "Our [[ framework ]] is applied for the << automatic tracking of endocardium >> in ultrasound sequences of the human heart .", "h": ["framework"], "t": ["automatic tracking of endocardium"]}, {"label": "PART-OF", "tokens": "Our framework is applied for the automatic tracking of [[ endocardium ]] in << ultrasound sequences of the human heart >> .", "h": ["endocardium"], "t": ["ultrasound sequences of the human heart"]}, {"label": "CONJUNCTION", "tokens": "Reliable [[ detection ]] and robust << tracking >> results are achieved when compared to existing approaches and inter-expert variations .", "h": ["detection"], "t": ["tracking"]}, {"label": "CONJUNCTION", "tokens": "Reliable detection and robust tracking results are achieved when compared to existing [[ approaches ]] and << inter-expert variations >> .", "h": ["approaches"], "t": ["inter-expert variations"]}, {"label": "USED-FOR", "tokens": "We present a [[ syntax-based constraint ]] for << word alignment >> , known as the cohesion constraint .", "h": ["syntax-based constraint"], "t": ["word alignment"]}, {"label": "HYPONYM-OF", "tokens": "We present a << syntax-based constraint >> for word alignment , known as the [[ cohesion constraint ]] .", "h": ["cohesion constraint"], "t": ["syntax-based constraint"]}, {"label": "USED-FOR", "tokens": "<< It >> requires disjoint [[ English phrases ]] to be mapped to non-overlapping intervals in the French sentence .", "h": ["English phrases"], "t": ["It"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate the utility of this << constraint >> in two different [[ algorithms ]] .", "h": ["algorithms"], "t": ["constraint"]}, {"label": "EVALUATE-FOR", "tokens": "The results show that << it >> can provide a significant improvement in [[ alignment quality ]] .", "h": ["alignment quality"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "We present a novel << entity-based representation of discourse >> which is inspired by [[ Centering Theory ]] and can be computed automatically from raw text .", "h": ["Centering Theory"], "t": ["entity-based representation of discourse"]}, {"label": "USED-FOR", "tokens": "We present a novel << entity-based representation of discourse >> which is inspired by Centering Theory and can be computed automatically from [[ raw text ]] .", "h": ["raw text"], "t": ["entity-based representation of discourse"]}, {"label": "USED-FOR", "tokens": "We view << coherence assessment >> as a [[ ranking learning problem ]] and show that the proposed discourse representation supports the effective learning of a ranking function .", "h": ["ranking learning problem"], "t": ["coherence assessment"]}, {"label": "USED-FOR", "tokens": "We view coherence assessment as a ranking learning problem and show that the proposed [[ discourse representation ]] supports the effective learning of a << ranking function >> .", "h": ["discourse representation"], "t": ["ranking function"]}, {"label": "COMPARE", "tokens": "Our experiments demonstrate that the [[ induced model ]] achieves significantly higher accuracy than a state-of-the-art << coherence model >> .", "h": ["induced model"], "t": ["coherence model"]}, {"label": "EVALUATE-FOR", "tokens": "Our experiments demonstrate that the << induced model >> achieves significantly higher [[ accuracy ]] than a state-of-the-art coherence model .", "h": ["accuracy"], "t": ["induced model"]}, {"label": "EVALUATE-FOR", "tokens": "Our experiments demonstrate that the induced model achieves significantly higher [[ accuracy ]] than a state-of-the-art << coherence model >> .", "h": ["accuracy"], "t": ["coherence model"]}, {"label": "USED-FOR", "tokens": "This paper introduces a [[ robust interactive method ]] for << speech understanding >> .", "h": ["robust interactive method"], "t": ["speech understanding"]}, {"label": "USED-FOR", "tokens": "The << generalized LR parsing >> is enhanced in this [[ approach ]] .", "h": ["approach"], "t": ["generalized LR parsing"]}, {"label": "USED-FOR", "tokens": "When a very noisy portion is detected , the << parser >> skips that portion using a fake [[ non-terminal symbol ]] .", "h": ["non-terminal symbol"], "t": ["parser"]}, {"label": "USED-FOR", "tokens": "This [[ method ]] is also capable of handling << unknown words >> , which is important in practical systems .", "h": ["method"], "t": ["unknown words"]}, {"label": "PART-OF", "tokens": "This paper shows that it is very often possible to identify the source language of [[ medium-length speeches ]] in the << EUROPARL corpus >> on the basis of frequency counts of word n-grams -LRB- 87.2 % -96.7 % accuracy depending on classification method -RRB- .", "h": ["medium-length speeches"], "t": ["EUROPARL corpus"]}, {"label": "EVALUATE-FOR", "tokens": "This paper shows that it is very often possible to identify the source language of medium-length speeches in the EUROPARL corpus on the basis of frequency counts of word n-grams -LRB- 87.2 % -96.7 % [[ accuracy ]] depending on << classification method >> -RRB- .", "h": ["accuracy"], "t": ["classification method"]}, {"label": "COMPARE", "tokens": "We investigated whether [[ automatic phonetic transcriptions -LRB- APTs -RRB- ]] can replace << manually verified phonetic transcriptions >> -LRB- MPTs -RRB- in a large corpus-based study on pronunciation variation .", "h": ["automatic phonetic transcriptions -LRB- APTs -RRB-"], "t": ["manually verified phonetic transcriptions"]}, {"label": "USED-FOR", "tokens": "We investigated whether [[ automatic phonetic transcriptions -LRB- APTs -RRB- ]] can replace manually verified phonetic transcriptions -LRB- MPTs -RRB- in a large corpus-based study on << pronunciation variation >> .", "h": ["automatic phonetic transcriptions -LRB- APTs -RRB-"], "t": ["pronunciation variation"]}, {"label": "USED-FOR", "tokens": "We investigated whether automatic phonetic transcriptions -LRB- APTs -RRB- can replace [[ manually verified phonetic transcriptions ]] -LRB- MPTs -RRB- in a large corpus-based study on << pronunciation variation >> .", "h": ["manually verified phonetic transcriptions"], "t": ["pronunciation variation"]}, {"label": "USED-FOR", "tokens": "We trained << classifiers >> on the [[ speech processes ]] extracted from the alignments of an APT and an MPT with a canonical transcription .", "h": ["speech processes"], "t": ["classifiers"]}, {"label": "USED-FOR", "tokens": "We trained classifiers on the << speech processes >> extracted from the [[ alignments ]] of an APT and an MPT with a canonical transcription .", "h": ["alignments"], "t": ["speech processes"]}, {"label": "USED-FOR", "tokens": "We trained classifiers on the speech processes extracted from the [[ alignments ]] of an << APT >> and an MPT with a canonical transcription .", "h": ["alignments"], "t": ["APT"]}, {"label": "USED-FOR", "tokens": "We trained classifiers on the speech processes extracted from the [[ alignments ]] of an APT and an << MPT >> with a canonical transcription .", "h": ["alignments"], "t": ["MPT"]}, {"label": "CONJUNCTION", "tokens": "We trained classifiers on the speech processes extracted from the alignments of an [[ APT ]] and an << MPT >> with a canonical transcription .", "h": ["APT"], "t": ["MPT"]}, {"label": "USED-FOR", "tokens": "We trained classifiers on the speech processes extracted from the << alignments >> of an APT and an MPT with a [[ canonical transcription ]] .", "h": ["canonical transcription"], "t": ["alignments"]}, {"label": "USED-FOR", "tokens": "We tested whether the [[ classifiers ]] were equally good at verifying whether << unknown transcriptions >> represent read speech or telephone dialogues , and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings .", "h": ["classifiers"], "t": ["unknown transcriptions"]}, {"label": "USED-FOR", "tokens": "We tested whether the classifiers were equally good at verifying whether [[ unknown transcriptions ]] represent << read speech >> or telephone dialogues , and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings .", "h": ["unknown transcriptions"], "t": ["read speech"]}, {"label": "USED-FOR", "tokens": "We tested whether the classifiers were equally good at verifying whether [[ unknown transcriptions ]] represent read speech or << telephone dialogues >> , and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings .", "h": ["unknown transcriptions"], "t": ["telephone dialogues"]}, {"label": "CONJUNCTION", "tokens": "We tested whether the classifiers were equally good at verifying whether unknown transcriptions represent [[ read speech ]] or << telephone dialogues >> , and whether the same speech processes were identified to distinguish between transcriptions of the two situational settings .", "h": ["read speech"], "t": ["telephone dialogues"]}, {"label": "COMPARE", "tokens": "Our results not only show that similar distinguishing speech processes were identified ; our [[ APT-based classifier ]] yielded better classification accuracy than the << MPT-based classifier >> whilst using fewer classification features .", "h": ["APT-based classifier"], "t": ["MPT-based classifier"]}, {"label": "EVALUATE-FOR", "tokens": "Our results not only show that similar distinguishing speech processes were identified ; our << APT-based classifier >> yielded better [[ classification accuracy ]] than the MPT-based classifier whilst using fewer classification features .", "h": ["classification accuracy"], "t": ["APT-based classifier"]}, {"label": "EVALUATE-FOR", "tokens": "Our results not only show that similar distinguishing speech processes were identified ; our APT-based classifier yielded better [[ classification accuracy ]] than the << MPT-based classifier >> whilst using fewer classification features .", "h": ["classification accuracy"], "t": ["MPT-based classifier"]}, {"label": "USED-FOR", "tokens": "Our results not only show that similar distinguishing speech processes were identified ; our << APT-based classifier >> yielded better classification accuracy than the MPT-based classifier whilst using fewer [[ classification features ]] .", "h": ["classification features"], "t": ["APT-based classifier"]}, {"label": "USED-FOR", "tokens": "Our results not only show that similar distinguishing speech processes were identified ; our APT-based classifier yielded better classification accuracy than the << MPT-based classifier >> whilst using fewer [[ classification features ]] .", "h": ["classification features"], "t": ["MPT-based classifier"]}, {"label": "USED-FOR", "tokens": "Machine reading is a relatively new field that features [[ computer programs ]] designed to read << flowing text >> and extract fact assertions expressed by the narrative content .", "h": ["computer programs"], "t": ["flowing text"]}, {"label": "USED-FOR", "tokens": "Machine reading is a relatively new field that features [[ computer programs ]] designed to read flowing text and extract << fact assertions >> expressed by the narrative content .", "h": ["computer programs"], "t": ["fact assertions"]}, {"label": "FEATURE-OF", "tokens": "Machine reading is a relatively new field that features computer programs designed to read flowing text and extract [[ fact assertions ]] expressed by the << narrative content >> .", "h": ["fact assertions"], "t": ["narrative content"]}, {"label": "PART-OF", "tokens": "This << task >> involves two core technologies : [[ natural language processing -LRB- NLP -RRB- ]] and information extraction -LRB- IE -RRB- .", "h": ["natural language processing -LRB- NLP -RRB-"], "t": ["task"]}, {"label": "PART-OF", "tokens": "This << task >> involves two core technologies : natural language processing -LRB- NLP -RRB- and [[ information extraction -LRB- IE -RRB- ]] .", "h": ["information extraction -LRB- IE -RRB-"], "t": ["task"]}, {"label": "FEATURE-OF", "tokens": "In this paper we describe a << machine reading system >> that we have developed within a [[ cognitive architecture ]] .", "h": ["cognitive architecture"], "t": ["machine reading system"]}, {"label": "CONJUNCTION", "tokens": "We show how we have integrated into the framework several levels of knowledge for a particular domain , ideas from [[ cognitive semantics ]] and << construction grammar >> , plus tools from prior NLP and IE research .", "h": ["cognitive semantics"], "t": ["construction grammar"]}, {"label": "CONJUNCTION", "tokens": "We show how we have integrated into the framework several levels of knowledge for a particular domain , ideas from cognitive semantics and construction grammar , plus tools from [[ prior NLP ]] and << IE research >> .", "h": ["prior NLP"], "t": ["IE research"]}, {"label": "USED-FOR", "tokens": "The result is a [[ system ]] that is capable of reading and interpreting complex and fairly << idiosyncratic texts >> in the family history domain .", "h": ["system"], "t": ["idiosyncratic texts"]}, {"label": "FEATURE-OF", "tokens": "The result is a system that is capable of reading and interpreting complex and fairly << idiosyncratic texts >> in the [[ family history domain ]] .", "h": ["family history domain"], "t": ["idiosyncratic texts"]}, {"label": "USED-FOR", "tokens": "We present two [[ methods ]] for capturing << nonstationary chaos >> , then present a few examples including biological signals , ocean waves and traffic flow .", "h": ["methods"], "t": ["nonstationary chaos"]}, {"label": "HYPONYM-OF", "tokens": "We present two methods for capturing nonstationary chaos , then present a few << examples >> including [[ biological signals ]] , ocean waves and traffic flow .", "h": ["biological signals"], "t": ["examples"]}, {"label": "CONJUNCTION", "tokens": "We present two methods for capturing nonstationary chaos , then present a few examples including [[ biological signals ]] , << ocean waves >> and traffic flow .", "h": ["biological signals"], "t": ["ocean waves"]}, {"label": "HYPONYM-OF", "tokens": "We present two methods for capturing nonstationary chaos , then present a few << examples >> including biological signals , [[ ocean waves ]] and traffic flow .", "h": ["ocean waves"], "t": ["examples"]}, {"label": "CONJUNCTION", "tokens": "We present two methods for capturing nonstationary chaos , then present a few examples including biological signals , [[ ocean waves ]] and << traffic flow >> .", "h": ["ocean waves"], "t": ["traffic flow"]}, {"label": "HYPONYM-OF", "tokens": "We present two methods for capturing nonstationary chaos , then present a few << examples >> including biological signals , ocean waves and [[ traffic flow ]] .", "h": ["traffic flow"], "t": ["examples"]}, {"label": "USED-FOR", "tokens": "This paper presents a [[ formal analysis ]] for a large class of words called << alternative markers >> , which includes other -LRB- than -RRB- , such -LRB- as -RRB- , and besides .", "h": ["formal analysis"], "t": ["alternative markers"]}, {"label": "PART-OF", "tokens": "These [[ words ]] appear frequently enough in << dialog >> to warrant serious attention , yet present natural language search engines perform poorly on queries containing them .", "h": ["words"], "t": ["dialog"]}, {"label": "PART-OF", "tokens": "I show that the performance of a << search engine >> can be improved dramatically by incorporating an [[ approximation of the formal analysis ]] that is compatible with the search engine 's operational semantics .", "h": ["approximation of the formal analysis"], "t": ["search engine"]}, {"label": "PART-OF", "tokens": "I show that the performance of a search engine can be improved dramatically by incorporating an approximation of the formal analysis that is compatible with the << search engine >> 's [[ operational semantics ]] .", "h": ["operational semantics"], "t": ["search engine"]}, {"label": "PART-OF", "tokens": "The value of this approach is that as the [[ operational semantics ]] of << natural language applications >> improve , even larger improvements are possible .", "h": ["operational semantics"], "t": ["natural language applications"]}, {"label": "HYPONYM-OF", "tokens": "We find that simple << interpolation methods >> , like [[ log-linear and linear interpolation ]] , improve the performance but fall short of the performance of an oracle .", "h": ["log-linear and linear interpolation"], "t": ["interpolation methods"]}, {"label": "FEATURE-OF", "tokens": "Actually , the oracle acts like a << dynamic combiner >> with [[ hard decisions ]] using the reference .", "h": ["hard decisions"], "t": ["dynamic combiner"]}, {"label": "USED-FOR", "tokens": "We suggest a << method >> that mimics the behavior of the oracle using a [[ neural network ]] or a decision tree .", "h": ["neural network"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "We suggest a << method >> that mimics the behavior of the oracle using a neural network or a [[ decision tree ]] .", "h": ["decision tree"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "We suggest a method that mimics the behavior of the oracle using a << neural network >> or a [[ decision tree ]] .", "h": ["decision tree"], "t": ["neural network"]}, {"label": "USED-FOR", "tokens": "The [[ method ]] amounts to tagging << LMs >> with confidence measures and picking the best hypothesis corresponding to the LM with the best confidence .", "h": ["method"], "t": ["LMs"]}, {"label": "USED-FOR", "tokens": "The << method >> amounts to tagging LMs with [[ confidence measures ]] and picking the best hypothesis corresponding to the LM with the best confidence .", "h": ["confidence measures"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "We describe a new [[ method ]] for the representation of << NLP structures >> within reranking approaches .", "h": ["method"], "t": ["NLP structures"]}, {"label": "FEATURE-OF", "tokens": "We describe a new method for the representation of << NLP structures >> within [[ reranking approaches ]] .", "h": ["reranking approaches"], "t": ["NLP structures"]}, {"label": "USED-FOR", "tokens": "We make use of a << conditional log-linear model >> , with [[ hidden variables ]] representing the assignment of lexical items to word clusters or word senses .", "h": ["hidden variables"], "t": ["conditional log-linear model"]}, {"label": "CONJUNCTION", "tokens": "We make use of a conditional log-linear model , with hidden variables representing the assignment of lexical items to [[ word clusters ]] or << word senses >> .", "h": ["word clusters"], "t": ["word senses"]}, {"label": "USED-FOR", "tokens": "The << model >> learns to automatically make these assignments based on a [[ discriminative training criterion ]] .", "h": ["discriminative training criterion"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "Training and decoding with the model requires summing over an exponential number of hidden-variable assignments : the required << summations >> can be computed efficiently and exactly using [[ dynamic programming ]] .", "h": ["dynamic programming"], "t": ["summations"]}, {"label": "USED-FOR", "tokens": "As a case study , we apply the [[ model ]] to << parse reranking >> .", "h": ["model"], "t": ["parse reranking"]}, {"label": "COMPARE", "tokens": "The [[ model ]] gives an F-measure improvement of ~ 1.25 % beyond the << base parser >> , and an ~ 0.25 % improvement beyond Collins -LRB- 2000 -RRB- reranker .", "h": ["model"], "t": ["base parser"]}, {"label": "EVALUATE-FOR", "tokens": "The << model >> gives an [[ F-measure ]] improvement of ~ 1.25 % beyond the base parser , and an ~ 0.25 % improvement beyond Collins -LRB- 2000 -RRB- reranker .", "h": ["F-measure"], "t": ["model"]}, {"label": "COMPARE", "tokens": "The model gives an F-measure improvement of ~ 1.25 % beyond the [[ base parser ]] , and an ~ 0.25 % improvement beyond << Collins -LRB- 2000 -RRB- reranker >> .", "h": ["base parser"], "t": ["Collins -LRB- 2000 -RRB- reranker"]}, {"label": "USED-FOR", "tokens": "Although our experiments are focused on << parsing >> , the [[ techniques ]] described generalize naturally to NLP structures other than parse trees .", "h": ["techniques"], "t": ["parsing"]}, {"label": "USED-FOR", "tokens": "Although our experiments are focused on parsing , the [[ techniques ]] described generalize naturally to << NLP structures >> other than parse trees .", "h": ["techniques"], "t": ["NLP structures"]}, {"label": "USED-FOR", "tokens": "Although our experiments are focused on parsing , the [[ techniques ]] described generalize naturally to NLP structures other than << parse trees >> .", "h": ["techniques"], "t": ["parse trees"]}, {"label": "CONJUNCTION", "tokens": "Although our experiments are focused on parsing , the techniques described generalize naturally to << NLP structures >> other than [[ parse trees ]] .", "h": ["parse trees"], "t": ["NLP structures"]}, {"label": "USED-FOR", "tokens": "This paper presents an [[ algorithm ]] for << learning the time-varying shape of a non-rigid 3D object >> from uncalibrated 2D tracking data .", "h": ["algorithm"], "t": ["learning the time-varying shape of a non-rigid 3D object"]}, {"label": "USED-FOR", "tokens": "We constrain the problem by assuming that the << object shape >> at each time instant is drawn from a [[ Gaussian distribution ]] .", "h": ["Gaussian distribution"], "t": ["object shape"]}, {"label": "USED-FOR", "tokens": "Based on this assumption , the [[ algorithm ]] simultaneously estimates << 3D shape and motion >> for each time frame , learns the parameters of the Gaussian , and robustly fills-in missing data points .", "h": ["algorithm"], "t": ["3D shape and motion"]}, {"label": "USED-FOR", "tokens": "We then extend the [[ algorithm ]] to model << temporal smoothness in object shape >> , thus allowing it to handle severe cases of missing data .", "h": ["algorithm"], "t": ["temporal smoothness in object shape"]}, {"label": "USED-FOR", "tokens": "We then extend the algorithm to model temporal smoothness in object shape , thus allowing [[ it ]] to handle severe cases of << missing data >> .", "h": ["it"], "t": ["missing data"]}, {"label": "CONJUNCTION", "tokens": "[[ Automatic summarization ]] and << information extraction >> are two important Internet services .", "h": ["Automatic summarization"], "t": ["information extraction"]}, {"label": "CONJUNCTION", "tokens": "[[ MUC ]] and << SUMMAC >> play their appropriate roles in the next generation Internet .", "h": ["MUC"], "t": ["SUMMAC"]}, {"label": "USED-FOR", "tokens": "This paper focuses on the automatic summarization and proposes two different [[ models ]] to extract sentences for << summary generation >> under two tasks initiated by SUMMAC-1 .", "h": ["models"], "t": ["summary generation"]}, {"label": "USED-FOR", "tokens": "This paper focuses on the automatic summarization and proposes two different [[ models ]] to extract sentences for summary generation under two << tasks >> initiated by SUMMAC-1 .", "h": ["models"], "t": ["tasks"]}, {"label": "PART-OF", "tokens": "This paper focuses on the automatic summarization and proposes two different models to extract sentences for summary generation under two [[ tasks ]] initiated by << SUMMAC-1 >> .", "h": ["tasks"], "t": ["SUMMAC-1"]}, {"label": "USED-FOR", "tokens": "For << categorization task >> , [[ positive feature vectors ]] and negative feature vectors are used cooperatively to construct generic , indicative summaries .", "h": ["positive feature vectors"], "t": ["categorization task"]}, {"label": "CONJUNCTION", "tokens": "For categorization task , [[ positive feature vectors ]] and << negative feature vectors >> are used cooperatively to construct generic , indicative summaries .", "h": ["positive feature vectors"], "t": ["negative feature vectors"]}, {"label": "USED-FOR", "tokens": "For categorization task , [[ positive feature vectors ]] and negative feature vectors are used cooperatively to construct << generic , indicative summaries >> .", "h": ["positive feature vectors"], "t": ["generic , indicative summaries"]}, {"label": "USED-FOR", "tokens": "For << categorization task >> , positive feature vectors and [[ negative feature vectors ]] are used cooperatively to construct generic , indicative summaries .", "h": ["negative feature vectors"], "t": ["categorization task"]}, {"label": "USED-FOR", "tokens": "For categorization task , positive feature vectors and [[ negative feature vectors ]] are used cooperatively to construct << generic , indicative summaries >> .", "h": ["negative feature vectors"], "t": ["generic , indicative summaries"]}, {"label": "USED-FOR", "tokens": "For << adhoc task >> , a [[ text model ]] based on relationship between nouns and verbs is used to filter out irrelevant discourse segment , to rank relevant sentences , and to generate the user-directed summaries .", "h": ["text model"], "t": ["adhoc task"]}, {"label": "USED-FOR", "tokens": "For adhoc task , a [[ text model ]] based on relationship between nouns and verbs is used to filter out irrelevant << discourse segment >> , to rank relevant sentences , and to generate the user-directed summaries .", "h": ["text model"], "t": ["discourse segment"]}, {"label": "USED-FOR", "tokens": "For adhoc task , a [[ text model ]] based on relationship between nouns and verbs is used to filter out irrelevant discourse segment , to rank relevant sentences , and to generate the << user-directed summaries >> .", "h": ["text model"], "t": ["user-directed summaries"]}, {"label": "EVALUATE-FOR", "tokens": "The result shows that the [[ NormF ]] of the best summary and that of the fixed summary for << adhoc tasks >> are 0.456 and 0 .", "h": ["NormF"], "t": ["adhoc tasks"]}, {"label": "EVALUATE-FOR", "tokens": "The [[ NormF ]] of the best summary and that of the fixed summary for << categorization task >> are 0.4090 and 0.4023 .", "h": ["NormF"], "t": ["categorization task"]}, {"label": "COMPARE", "tokens": "Our [[ system ]] outperforms the average << system >> in categorization task but does a common job in adhoc task .", "h": ["system"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "Our << system >> outperforms the average system in [[ categorization task ]] but does a common job in adhoc task .", "h": ["categorization task"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "Our system outperforms the average << system >> in [[ categorization task ]] but does a common job in adhoc task .", "h": ["categorization task"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "Our << system >> outperforms the average system in categorization task but does a common job in [[ adhoc task ]] .", "h": ["adhoc task"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "Our system outperforms the average system in << categorization task >> but does a common job in [[ adhoc task ]] .", "h": ["adhoc task"], "t": ["categorization task"]}, {"label": "FEATURE-OF", "tokens": "In real-world action recognition problems , low-level features can not adequately characterize the [[ rich spatial-temporal structures ]] in << action videos >> .", "h": ["rich spatial-temporal structures"], "t": ["action videos"]}, {"label": "USED-FOR", "tokens": "The second type is << data-driven attributes >> , which are learned from data using [[ dictionary learning methods ]] .", "h": ["dictionary learning methods"], "t": ["data-driven attributes"]}, {"label": "USED-FOR", "tokens": "We propose a << discriminative and compact attribute-based representation >> by selecting a subset of [[ discriminative attributes ]] from a large attribute set .", "h": ["discriminative attributes"], "t": ["discriminative and compact attribute-based representation"]}, {"label": "USED-FOR", "tokens": "Three << attribute selection criteria >> are proposed and formulated as a [[ submodular optimization problem ]] .", "h": ["submodular optimization problem"], "t": ["attribute selection criteria"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results on the [[ Olympic Sports and UCF101 datasets ]] demonstrate that the proposed << attribute-based representation >> can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches .", "h": ["Olympic Sports and UCF101 datasets"], "t": ["attribute-based representation"]}, {"label": "USED-FOR", "tokens": "Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed [[ attribute-based representation ]] can significantly boost the performance of << action recognition algorithms >> and outperform most recently proposed recognition approaches .", "h": ["attribute-based representation"], "t": ["action recognition algorithms"]}, {"label": "COMPARE", "tokens": "Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of [[ action recognition algorithms ]] and outperform most recently proposed << recognition approaches >> .", "h": ["action recognition algorithms"], "t": ["recognition approaches"]}, {"label": "USED-FOR", "tokens": "Landsbergen 's advocacy of [[ analytical inverses ]] for << compositional syntax rules >> encourages the application of Definite Clause Grammar techniques to the construction of a parser returning Montague analysis trees .", "h": ["analytical inverses"], "t": ["compositional syntax rules"]}, {"label": "USED-FOR", "tokens": "Landsbergen 's advocacy of [[ analytical inverses ]] for compositional syntax rules encourages the application of << Definite Clause Grammar techniques >> to the construction of a parser returning Montague analysis trees .", "h": ["analytical inverses"], "t": ["Definite Clause Grammar techniques"]}, {"label": "USED-FOR", "tokens": "Landsbergen 's advocacy of analytical inverses for compositional syntax rules encourages the application of [[ Definite Clause Grammar techniques ]] to the construction of a << parser returning Montague analysis trees >> .", "h": ["Definite Clause Grammar techniques"], "t": ["parser returning Montague analysis trees"]}, {"label": "USED-FOR", "tokens": "A << parser MDCC >> is presented which implements an [[ augmented Friedman - Warren algorithm ]] permitting post referencing * and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae .", "h": ["augmented Friedman - Warren algorithm"], "t": ["parser MDCC"]}, {"label": "FEATURE-OF", "tokens": "A parser MDCC is presented which implements an << augmented Friedman - Warren algorithm >> permitting [[ post referencing ]] * and interfaces with a language of intenslonal logic translator LILT so as to display the derivational history of corresponding reduced IL formulae .", "h": ["post referencing"], "t": ["augmented Friedman - Warren algorithm"]}, {"label": "USED-FOR", "tokens": "A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing * and interfaces with a language of << intenslonal logic translator LILT >> so as to display the [[ derivational history ]] of corresponding reduced IL formulae .", "h": ["derivational history"], "t": ["intenslonal logic translator LILT"]}, {"label": "FEATURE-OF", "tokens": "A parser MDCC is presented which implements an augmented Friedman - Warren algorithm permitting post referencing * and interfaces with a language of intenslonal logic translator LILT so as to display the << derivational history >> of corresponding [[ reduced IL formulae ]] .", "h": ["reduced IL formulae"], "t": ["derivational history"]}, {"label": "CONJUNCTION", "tokens": "Some familiarity with [[ Montague 's PTQ ]] and the << basic DCG mechanism >> is assumed .", "h": ["Montague 's PTQ"], "t": ["basic DCG mechanism"]}, {"label": "EVALUATE-FOR", "tokens": "<< Stochastic attention-based models >> have been shown to improve [[ computational efficiency ]] at test time , but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates .", "h": ["computational efficiency"], "t": ["Stochastic attention-based models"]}, {"label": "CONJUNCTION", "tokens": "Stochastic attention-based models have been shown to improve computational efficiency at test time , but they remain difficult to train because of [[ intractable posterior inference ]] and high variance in the << stochastic gradient estimates >> .", "h": ["intractable posterior inference"], "t": ["stochastic gradient estimates"]}, {"label": "USED-FOR", "tokens": "[[ Borrowing techniques ]] from the literature on training << deep generative models >> , we present the Wake-Sleep Recurrent Attention Model , a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients .", "h": ["Borrowing techniques"], "t": ["deep generative models"]}, {"label": "USED-FOR", "tokens": "Borrowing techniques from the literature on training deep generative models , we present the Wake-Sleep Recurrent Attention Model , a [[ method ]] for training << stochastic attention networks >> which improves posterior inference and which reduces the variability in the stochastic gradients .", "h": ["method"], "t": ["stochastic attention networks"]}, {"label": "USED-FOR", "tokens": "Borrowing techniques from the literature on training deep generative models , we present the Wake-Sleep Recurrent Attention Model , a method for training [[ stochastic attention networks ]] which improves << posterior inference >> and which reduces the variability in the stochastic gradients .", "h": ["stochastic attention networks"], "t": ["posterior inference"]}, {"label": "EVALUATE-FOR", "tokens": "We show that our << method >> can greatly speed up the [[ training time ]] for stochastic attention networks in the domains of image classification and caption generation .", "h": ["training time"], "t": ["method"]}, {"label": "FEATURE-OF", "tokens": "We show that our method can greatly speed up the [[ training time ]] for << stochastic attention networks >> in the domains of image classification and caption generation .", "h": ["training time"], "t": ["stochastic attention networks"]}, {"label": "EVALUATE-FOR", "tokens": "We show that our << method >> can greatly speed up the training time for stochastic attention networks in the domains of [[ image classification ]] and caption generation .", "h": ["image classification"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "We show that our method can greatly speed up the training time for stochastic attention networks in the domains of [[ image classification ]] and << caption generation >> .", "h": ["image classification"], "t": ["caption generation"]}, {"label": "EVALUATE-FOR", "tokens": "We show that our << method >> can greatly speed up the training time for stochastic attention networks in the domains of image classification and [[ caption generation ]] .", "h": ["caption generation"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "A new [[ exemplar-based framework ]] unifying << image completion >> , texture synthesis and image inpainting is presented in this work .", "h": ["exemplar-based framework"], "t": ["image completion"]}, {"label": "USED-FOR", "tokens": "A new [[ exemplar-based framework ]] unifying image completion , << texture synthesis >> and image inpainting is presented in this work .", "h": ["exemplar-based framework"], "t": ["texture synthesis"]}, {"label": "USED-FOR", "tokens": "A new [[ exemplar-based framework ]] unifying image completion , texture synthesis and << image inpainting >> is presented in this work .", "h": ["exemplar-based framework"], "t": ["image inpainting"]}, {"label": "CONJUNCTION", "tokens": "A new exemplar-based framework unifying [[ image completion ]] , << texture synthesis >> and image inpainting is presented in this work .", "h": ["image completion"], "t": ["texture synthesis"]}, {"label": "CONJUNCTION", "tokens": "A new exemplar-based framework unifying image completion , [[ texture synthesis ]] and << image inpainting >> is presented in this work .", "h": ["texture synthesis"], "t": ["image inpainting"]}, {"label": "COMPARE", "tokens": "Contrary to existing [[ greedy techniques ]] , these << tasks >> are posed in the form of a discrete global optimization problem with a well defined objective function .", "h": ["greedy techniques"], "t": ["tasks"]}, {"label": "FEATURE-OF", "tokens": "Contrary to existing greedy techniques , these << tasks >> are posed in the form of a [[ discrete global optimization problem ]] with a well defined objective function .", "h": ["discrete global optimization problem"], "t": ["tasks"]}, {"label": "FEATURE-OF", "tokens": "Contrary to existing greedy techniques , these tasks are posed in the form of a << discrete global optimization problem >> with a [[ well defined objective function ]] .", "h": ["well defined objective function"], "t": ["discrete global optimization problem"]}, {"label": "USED-FOR", "tokens": "For solving this << problem >> a novel [[ optimization scheme ]] , called Priority-BP , is proposed which carries two very important extensions over standard belief propagation -LRB- BP -RRB- : '' priority-based message scheduling '' and '' dynamic label pruning '' .", "h": ["optimization scheme"], "t": ["problem"]}, {"label": "HYPONYM-OF", "tokens": "For solving this problem a novel << optimization scheme >> , called [[ Priority-BP ]] , is proposed which carries two very important extensions over standard belief propagation -LRB- BP -RRB- : '' priority-based message scheduling '' and '' dynamic label pruning '' .", "h": ["Priority-BP"], "t": ["optimization scheme"]}, {"label": "PART-OF", "tokens": "For solving this problem a novel << optimization scheme >> , called Priority-BP , is proposed which carries two very important [[ extensions ]] over standard belief propagation -LRB- BP -RRB- : '' priority-based message scheduling '' and '' dynamic label pruning '' .", "h": ["extensions"], "t": ["optimization scheme"]}, {"label": "USED-FOR", "tokens": "For solving this problem a novel optimization scheme , called Priority-BP , is proposed which carries two very important << extensions >> over standard [[ belief propagation -LRB- BP -RRB- ]] : '' priority-based message scheduling '' and '' dynamic label pruning '' .", "h": ["belief propagation -LRB- BP -RRB-"], "t": ["extensions"]}, {"label": "HYPONYM-OF", "tokens": "For solving this problem a novel optimization scheme , called Priority-BP , is proposed which carries two very important << extensions >> over standard belief propagation -LRB- BP -RRB- : '' [[ priority-based message scheduling ]] '' and '' dynamic label pruning '' .", "h": ["priority-based message scheduling"], "t": ["extensions"]}, {"label": "CONJUNCTION", "tokens": "For solving this problem a novel optimization scheme , called Priority-BP , is proposed which carries two very important extensions over standard belief propagation -LRB- BP -RRB- : '' [[ priority-based message scheduling ]] '' and '' << dynamic label pruning >> '' .", "h": ["priority-based message scheduling"], "t": ["dynamic label pruning"]}, {"label": "HYPONYM-OF", "tokens": "For solving this problem a novel optimization scheme , called Priority-BP , is proposed which carries two very important << extensions >> over standard belief propagation -LRB- BP -RRB- : '' priority-based message scheduling '' and '' [[ dynamic label pruning ]] '' .", "h": ["dynamic label pruning"], "t": ["extensions"]}, {"label": "USED-FOR", "tokens": "These two [[ extensions ]] work in cooperation to deal with the << intolerable computational cost of BP >> caused by the huge number of existing labels .", "h": ["extensions"], "t": ["intolerable computational cost of BP"]}, {"label": "USED-FOR", "tokens": "Moreover , both [[ extensions ]] are generic and can therefore be applied to any << MRF energy function >> as well .", "h": ["extensions"], "t": ["MRF energy function"]}, {"label": "USED-FOR", "tokens": "The effectiveness of our << method >> is demonstrated on a wide variety of [[ image completion examples ]] .", "h": ["image completion examples"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we compare the relative effects of [[ segment order ]] , << segmentation >> and segment contiguity on the retrieval performance of a translation memory system .", "h": ["segment order"], "t": ["segmentation"]}, {"label": "USED-FOR", "tokens": "In this paper , we compare the relative effects of [[ segment order ]] , segmentation and segment contiguity on the retrieval performance of a << translation memory system >> .", "h": ["segment order"], "t": ["translation memory system"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we compare the relative effects of segment order , [[ segmentation ]] and << segment contiguity >> on the retrieval performance of a translation memory system .", "h": ["segmentation"], "t": ["segment contiguity"]}, {"label": "USED-FOR", "tokens": "In this paper , we compare the relative effects of segment order , [[ segmentation ]] and segment contiguity on the retrieval performance of a << translation memory system >> .", "h": ["segmentation"], "t": ["translation memory system"]}, {"label": "USED-FOR", "tokens": "In this paper , we compare the relative effects of segment order , segmentation and [[ segment contiguity ]] on the retrieval performance of a << translation memory system >> .", "h": ["segment contiguity"], "t": ["translation memory system"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper , we compare the relative effects of segment order , segmentation and segment contiguity on the [[ retrieval ]] performance of a << translation memory system >> .", "h": ["retrieval"], "t": ["translation memory system"]}, {"label": "USED-FOR", "tokens": "We take a selection of both << bag-of-words and segment order-sensitive string comparison methods >> , and run each over both [[ character - and word-segmented data ]] , in combination with a range of local segment contiguity models -LRB- in the form of N-grams -RRB- .", "h": ["character - and word-segmented data"], "t": ["bag-of-words and segment order-sensitive string comparison methods"]}, {"label": "CONJUNCTION", "tokens": "We take a selection of both << bag-of-words and segment order-sensitive string comparison methods >> , and run each over both character - and word-segmented data , in combination with a range of [[ local segment contiguity models ]] -LRB- in the form of N-grams -RRB- .", "h": ["local segment contiguity models"], "t": ["bag-of-words and segment order-sensitive string comparison methods"]}, {"label": "FEATURE-OF", "tokens": "We take a selection of both bag-of-words and segment order-sensitive string comparison methods , and run each over both character - and word-segmented data , in combination with a range of << local segment contiguity models >> -LRB- in the form of [[ N-grams ]] -RRB- .", "h": ["N-grams"], "t": ["local segment contiguity models"]}, {"label": "USED-FOR", "tokens": "Over two distinct datasets , we find that << indexing >> according to simple [[ character bigrams ]] produces a retrieval accuracy superior to any of the tested word N-gram models .", "h": ["character bigrams"], "t": ["indexing"]}, {"label": "COMPARE", "tokens": "Over two distinct datasets , we find that indexing according to simple [[ character bigrams ]] produces a retrieval accuracy superior to any of the tested << word N-gram models >> .", "h": ["character bigrams"], "t": ["word N-gram models"]}, {"label": "EVALUATE-FOR", "tokens": "Over two distinct datasets , we find that indexing according to simple << character bigrams >> produces a [[ retrieval accuracy ]] superior to any of the tested word N-gram models .", "h": ["retrieval accuracy"], "t": ["character bigrams"]}, {"label": "EVALUATE-FOR", "tokens": "Over two distinct datasets , we find that indexing according to simple character bigrams produces a [[ retrieval accuracy ]] superior to any of the tested << word N-gram models >> .", "h": ["retrieval accuracy"], "t": ["word N-gram models"]}, {"label": "COMPARE", "tokens": "Further , in their optimum configuration , [[ bag-of-words methods ]] are shown to be equivalent to << segment order-sensitive methods >> in terms of retrieval accuracy , but much faster .", "h": ["bag-of-words methods"], "t": ["segment order-sensitive methods"]}, {"label": "EVALUATE-FOR", "tokens": "Further , in their optimum configuration , << bag-of-words methods >> are shown to be equivalent to segment order-sensitive methods in terms of [[ retrieval accuracy ]] , but much faster .", "h": ["retrieval accuracy"], "t": ["bag-of-words methods"]}, {"label": "EVALUATE-FOR", "tokens": "Further , in their optimum configuration , bag-of-words methods are shown to be equivalent to << segment order-sensitive methods >> in terms of [[ retrieval accuracy ]] , but much faster .", "h": ["retrieval accuracy"], "t": ["segment order-sensitive methods"]}, {"label": "USED-FOR", "tokens": "In this paper we show how two standard [[ outputs ]] from information extraction -LRB- IE -RRB- systems - named entity annotations and scenario templates - can be used to enhance access to << text collections >> via a standard text browser .", "h": ["outputs"], "t": ["text collections"]}, {"label": "HYPONYM-OF", "tokens": "In this paper we show how two standard << outputs >> from information extraction -LRB- IE -RRB- systems - [[ named entity annotations ]] and scenario templates - can be used to enhance access to text collections via a standard text browser .", "h": ["named entity annotations"], "t": ["outputs"]}, {"label": "CONJUNCTION", "tokens": "In this paper we show how two standard outputs from information extraction -LRB- IE -RRB- systems - [[ named entity annotations ]] and << scenario templates >> - can be used to enhance access to text collections via a standard text browser .", "h": ["named entity annotations"], "t": ["scenario templates"]}, {"label": "HYPONYM-OF", "tokens": "In this paper we show how two standard << outputs >> from information extraction -LRB- IE -RRB- systems - named entity annotations and [[ scenario templates ]] - can be used to enhance access to text collections via a standard text browser .", "h": ["scenario templates"], "t": ["outputs"]}, {"label": "USED-FOR", "tokens": "In this paper we show how two standard outputs from information extraction -LRB- IE -RRB- systems - named entity annotations and scenario templates - can be used to enhance access to << text collections >> via a standard [[ text browser ]] .", "h": ["text browser"], "t": ["text collections"]}, {"label": "USED-FOR", "tokens": "We describe how this information is used in a [[ prototype system ]] designed to support information workers ' access to a << pharmaceutical news archive >> as part of their industry watch function .", "h": ["prototype system"], "t": ["pharmaceutical news archive"]}, {"label": "EVALUATE-FOR", "tokens": "We also report results of a preliminary , [[ qualitative user evaluation ]] of the << system >> , which while broadly positive indicates further work needs to be done on the interface to make users aware of the increased potential of IE-enhanced text browsers .", "h": ["qualitative user evaluation"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "We present a new [[ model-based bundle adjustment algorithm ]] to recover the << 3D model >> of a scene/object from a sequence of images with unknown motions .", "h": ["model-based bundle adjustment algorithm"], "t": ["3D model"]}, {"label": "USED-FOR", "tokens": "We present a new model-based bundle adjustment algorithm to recover the << 3D model >> of a scene/object from a sequence of [[ images ]] with unknown motions .", "h": ["images"], "t": ["3D model"]}, {"label": "PART-OF", "tokens": "We present a new model-based bundle adjustment algorithm to recover the 3D model of a scene/object from a sequence of << images >> with [[ unknown motions ]] .", "h": ["unknown motions"], "t": ["images"]}, {"label": "USED-FOR", "tokens": "Instead of representing scene/object by a collection of isolated 3D features -LRB- usually points -RRB- , our << algorithm >> uses a [[ surface ]] controlled by a small set of parameters .", "h": ["surface"], "t": ["algorithm"]}, {"label": "COMPARE", "tokens": "Compared with previous [[ model-based approaches ]] , our << approach >> has the following advantages .", "h": ["model-based approaches"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "First , instead of using the [[ model space ]] as a << regular-izer >> , we directly use it as our search space , thus resulting in a more elegant formulation with fewer unknowns and fewer equations .", "h": ["model space"], "t": ["regular-izer"]}, {"label": "COMPARE", "tokens": "First , instead of using the model space as a [[ regular-izer ]] , we directly use it as our << search space >> , thus resulting in a more elegant formulation with fewer unknowns and fewer equations .", "h": ["regular-izer"], "t": ["search space"]}, {"label": "USED-FOR", "tokens": "First , instead of using the model space as a regular-izer , we directly use [[ it ]] as our << search space >> , thus resulting in a more elegant formulation with fewer unknowns and fewer equations .", "h": ["it"], "t": ["search space"]}, {"label": "USED-FOR", "tokens": "Third , regarding << face modeling >> , we use a very small set of [[ face metrics ]] -LRB- meaningful deformations -RRB- to parame-terize the face geometry , resulting in a smaller search space and a better posed system .", "h": ["face metrics"], "t": ["face modeling"]}, {"label": "USED-FOR", "tokens": "Third , regarding face modeling , we use a very small set of [[ face metrics ]] -LRB- meaningful deformations -RRB- to parame-terize the << face geometry >> , resulting in a smaller search space and a better posed system .", "h": ["face metrics"], "t": ["face geometry"]}, {"label": "USED-FOR", "tokens": "Third , regarding face modeling , we use a very small set of [[ face metrics ]] -LRB- meaningful deformations -RRB- to parame-terize the face geometry , resulting in a smaller << search space >> and a better posed system .", "h": ["face metrics"], "t": ["search space"]}, {"label": "USED-FOR", "tokens": "Third , regarding face modeling , we use a very small set of [[ face metrics ]] -LRB- meaningful deformations -RRB- to parame-terize the face geometry , resulting in a smaller search space and a better << posed system >> .", "h": ["face metrics"], "t": ["posed system"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments with both [[ synthetic and real data ]] show that this new << algorithm >> is faster , more accurate and more stable than existing ones .", "h": ["synthetic and real data"], "t": ["algorithm"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments with both [[ synthetic and real data ]] show that this new algorithm is faster , more accurate and more stable than existing << ones >> .", "h": ["synthetic and real data"], "t": ["ones"]}, {"label": "COMPARE", "tokens": "Experiments with both synthetic and real data show that this new [[ algorithm ]] is faster , more accurate and more stable than existing << ones >> .", "h": ["algorithm"], "t": ["ones"]}, {"label": "USED-FOR", "tokens": "This paper presents an [[ approach ]] to the << unsupervised learning of parts of speech >> which uses both morphological and syntactic information .", "h": ["approach"], "t": ["unsupervised learning of parts of speech"]}, {"label": "USED-FOR", "tokens": "This paper presents an << approach >> to the unsupervised learning of parts of speech which uses both [[ morphological and syntactic information ]] .", "h": ["morphological and syntactic information"], "t": ["approach"]}, {"label": "COMPARE", "tokens": "While the [[ model ]] is more complex than << those >> which have been employed for unsupervised learning of POS tags in English , which use only syntactic information , the variety of languages in the world requires that we consider morphology as well .", "h": ["model"], "t": ["those"]}, {"label": "USED-FOR", "tokens": "While the model is more complex than [[ those ]] which have been employed for << unsupervised learning of POS tags in English >> , which use only syntactic information , the variety of languages in the world requires that we consider morphology as well .", "h": ["those"], "t": ["unsupervised learning of POS tags in English"]}, {"label": "USED-FOR", "tokens": "While the model is more complex than << those >> which have been employed for unsupervised learning of POS tags in English , which use only [[ syntactic information ]] , the variety of languages in the world requires that we consider morphology as well .", "h": ["syntactic information"], "t": ["those"]}, {"label": "COMPARE", "tokens": "In many languages , [[ morphology ]] provides better clues to a word 's category than << word order >> .", "h": ["morphology"], "t": ["word order"]}, {"label": "USED-FOR", "tokens": "We present the [[ computational model ]] for << POS learning >> , and present results for applying it to Bulgarian , a Slavic language with relatively free word order and rich morphology .", "h": ["computational model"], "t": ["POS learning"]}, {"label": "USED-FOR", "tokens": "We present the computational model for POS learning , and present results for applying << it >> to [[ Bulgarian ]] , a Slavic language with relatively free word order and rich morphology .", "h": ["Bulgarian"], "t": ["it"]}, {"label": "HYPONYM-OF", "tokens": "We present the computational model for POS learning , and present results for applying it to [[ Bulgarian ]] , a << Slavic language >> with relatively free word order and rich morphology .", "h": ["Bulgarian"], "t": ["Slavic language"]}, {"label": "FEATURE-OF", "tokens": "We present the computational model for POS learning , and present results for applying it to << Bulgarian >> , a Slavic language with relatively [[ free word order ]] and rich morphology .", "h": ["free word order"], "t": ["Bulgarian"]}, {"label": "CONJUNCTION", "tokens": "We present the computational model for POS learning , and present results for applying it to Bulgarian , a Slavic language with relatively [[ free word order ]] and << rich morphology >> .", "h": ["free word order"], "t": ["rich morphology"]}, {"label": "FEATURE-OF", "tokens": "We present the computational model for POS learning , and present results for applying it to << Bulgarian >> , a Slavic language with relatively free word order and [[ rich morphology ]] .", "h": ["rich morphology"], "t": ["Bulgarian"]}, {"label": "USED-FOR", "tokens": "In << MT >> , the widely used approach is to apply a [[ Chinese word segmenter ]] trained from manually annotated data , using a fixed lexicon .", "h": ["Chinese word segmenter"], "t": ["MT"]}, {"label": "USED-FOR", "tokens": "In MT , the widely used approach is to apply a << Chinese word segmenter >> trained from [[ manually annotated data ]] , using a fixed lexicon .", "h": ["manually annotated data"], "t": ["Chinese word segmenter"]}, {"label": "USED-FOR", "tokens": "Such [[ word segmentation ]] is not necessarily optimal for << translation >> .", "h": ["word segmentation"], "t": ["translation"]}, {"label": "USED-FOR", "tokens": "We propose a [[ Bayesian semi-supervised Chinese word segmentation model ]] which uses both monolingual and bilingual information to derive a << segmentation >> suitable for MT .", "h": ["Bayesian semi-supervised Chinese word segmentation model"], "t": ["segmentation"]}, {"label": "USED-FOR", "tokens": "We propose a << Bayesian semi-supervised Chinese word segmentation model >> which uses both [[ monolingual and bilingual information ]] to derive a segmentation suitable for MT .", "h": ["monolingual and bilingual information"], "t": ["Bayesian semi-supervised Chinese word segmentation model"]}, {"label": "USED-FOR", "tokens": "We propose a Bayesian semi-supervised Chinese word segmentation model which uses both monolingual and bilingual information to derive a [[ segmentation ]] suitable for << MT >> .", "h": ["segmentation"], "t": ["MT"]}, {"label": "COMPARE", "tokens": "Experiments show that our [[ method ]] improves a state-of-the-art << MT system >> in a small and a large data environment .", "h": ["method"], "t": ["MT system"]}, {"label": "USED-FOR", "tokens": "In this paper we compare two competing [[ approaches ]] to << part-of-speech tagging >> , statistical and constraint-based disambiguation , using French as our test language .", "h": ["approaches"], "t": ["part-of-speech tagging"]}, {"label": "USED-FOR", "tokens": "In this paper we compare two competing << approaches >> to part-of-speech tagging , statistical and constraint-based disambiguation , using [[ French ]] as our test language .", "h": ["French"], "t": ["approaches"]}, {"label": "COMPARE", "tokens": "We imposed a time limit on our experiment : the amount of time spent on the design of our [[ constraint system ]] was about the same as the time we used to train and test the easy-to-implement << statistical model >> .", "h": ["constraint system"], "t": ["statistical model"]}, {"label": "EVALUATE-FOR", "tokens": "The [[ accuracy ]] of the << statistical method >> is reasonably good , comparable to taggers for English .", "h": ["accuracy"], "t": ["statistical method"]}, {"label": "EVALUATE-FOR", "tokens": "The [[ accuracy ]] of the statistical method is reasonably good , comparable to << taggers >> for English .", "h": ["accuracy"], "t": ["taggers"]}, {"label": "COMPARE", "tokens": "The accuracy of the [[ statistical method ]] is reasonably good , comparable to << taggers >> for English .", "h": ["statistical method"], "t": ["taggers"]}, {"label": "USED-FOR", "tokens": "The accuracy of the statistical method is reasonably good , comparable to [[ taggers ]] for << English >> .", "h": ["taggers"], "t": ["English"]}, {"label": "USED-FOR", "tokens": "[[ Structured-light methods ]] actively generate << geometric correspondence data >> between projectors and cameras in order to facilitate robust 3D reconstruction .", "h": ["Structured-light methods"], "t": ["geometric correspondence data"]}, {"label": "USED-FOR", "tokens": "Structured-light methods actively generate [[ geometric correspondence data ]] between projectors and cameras in order to facilitate << robust 3D reconstruction >> .", "h": ["geometric correspondence data"], "t": ["robust 3D reconstruction"]}, {"label": "PART-OF", "tokens": "In this paper , we present << Photogeometric Structured Light >> whereby a standard [[ structured light method ]] is extended to include photometric methods .", "h": ["structured light method"], "t": ["Photogeometric Structured Light"]}, {"label": "PART-OF", "tokens": "In this paper , we present << Photogeometric Structured Light >> whereby a standard structured light method is extended to include [[ photometric methods ]] .", "h": ["photometric methods"], "t": ["Photogeometric Structured Light"]}, {"label": "USED-FOR", "tokens": "[[ Photometric processing ]] serves the double purpose of increasing the amount of << recovered surface detail >> and of enabling the structured-light setup to be robustly self-calibrated .", "h": ["Photometric processing"], "t": ["recovered surface detail"]}, {"label": "USED-FOR", "tokens": "[[ Photometric processing ]] serves the double purpose of increasing the amount of recovered surface detail and of enabling the << structured-light setup >> to be robustly self-calibrated .", "h": ["Photometric processing"], "t": ["structured-light setup"]}, {"label": "USED-FOR", "tokens": "Further , our << framework >> uses a [[ photogeometric optimization ]] that supports the simultaneous use of multiple cameras and projectors and yields a single and accurate multi-view 3D model which best complies with photometric and geometric data .", "h": ["photogeometric optimization"], "t": ["framework"]}, {"label": "USED-FOR", "tokens": "Further , our framework uses a photogeometric optimization that supports the simultaneous use of multiple cameras and projectors and yields a single and accurate << multi-view 3D model >> which best complies with [[ photometric and geometric data ]] .", "h": ["photometric and geometric data"], "t": ["multi-view 3D model"]}, {"label": "USED-FOR", "tokens": "In this paper , a discrimination and robustness oriented [[ adaptive learning procedure ]] is proposed to deal with the task of << syntactic ambiguity resolution >> .", "h": ["adaptive learning procedure"], "t": ["syntactic ambiguity resolution"]}, {"label": "CONJUNCTION", "tokens": "Owing to the problem of [[ insufficient training data ]] and << approximation error >> introduced by the language model , traditional statistical approaches , which resolve ambiguities by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications .", "h": ["insufficient training data"], "t": ["approximation error"]}, {"label": "USED-FOR", "tokens": "Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional [[ statistical approaches ]] , which resolve << ambiguities >> by indirectly and implicitly using maximum likelihood method , fail to achieve high performance in real applications .", "h": ["statistical approaches"], "t": ["ambiguities"]}, {"label": "USED-FOR", "tokens": "Owing to the problem of insufficient training data and approximation error introduced by the language model , traditional << statistical approaches >> , which resolve ambiguities by indirectly and implicitly using [[ maximum likelihood method ]] , fail to achieve high performance in real applications .", "h": ["maximum likelihood method"], "t": ["statistical approaches"]}, {"label": "EVALUATE-FOR", "tokens": "The [[ accuracy rate ]] of << syntactic disambiguation >> is raised from 46.0 % to 60.62 % by using this novel approach .", "h": ["accuracy rate"], "t": ["syntactic disambiguation"]}, {"label": "EVALUATE-FOR", "tokens": "The accuracy rate of [[ syntactic disambiguation ]] is raised from 46.0 % to 60.62 % by using this novel << approach >> .", "h": ["syntactic disambiguation"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "This paper presents a new [[ approach ]] to << statistical sentence generation >> in which alternative phrases are represented as packed sets of trees , or forests , and then ranked statistically to choose the best one .", "h": ["approach"], "t": ["statistical sentence generation"]}, {"label": "USED-FOR", "tokens": "[[ It ]] also facilitates more efficient << statistical ranking >> than a previous approach to statistical generation .", "h": ["It"], "t": ["statistical ranking"]}, {"label": "COMPARE", "tokens": "[[ It ]] also facilitates more efficient statistical ranking than a previous << approach >> to statistical generation .", "h": ["It"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "It also facilitates more efficient statistical ranking than a previous [[ approach ]] to << statistical generation >> .", "h": ["approach"], "t": ["statistical generation"]}, {"label": "COMPARE", "tokens": "An efficient [[ ranking algorithm ]] is described , together with experimental results showing significant improvements over simple << enumeration >> or a lattice-based approach .", "h": ["ranking algorithm"], "t": ["enumeration"]}, {"label": "COMPARE", "tokens": "An efficient [[ ranking algorithm ]] is described , together with experimental results showing significant improvements over simple enumeration or a << lattice-based approach >> .", "h": ["ranking algorithm"], "t": ["lattice-based approach"]}, {"label": "CONJUNCTION", "tokens": "An efficient ranking algorithm is described , together with experimental results showing significant improvements over simple [[ enumeration ]] or a << lattice-based approach >> .", "h": ["enumeration"], "t": ["lattice-based approach"]}, {"label": "USED-FOR", "tokens": "This article deals with the interpretation of conceptual operations underlying the communicative use of [[ natural language -LRB- NL -RRB- ]] within the << Structured Inheritance Network -LRB- SI-Nets -RRB- paradigm >> .", "h": ["natural language -LRB- NL -RRB-"], "t": ["Structured Inheritance Network -LRB- SI-Nets -RRB- paradigm"]}, {"label": "USED-FOR", "tokens": "The operations are reduced to functions of a formal language , thus changing the level of abstraction of the [[ operations ]] to be performed on << SI-Nets >> .", "h": ["operations"], "t": ["SI-Nets"]}, {"label": "USED-FOR", "tokens": "In this sense , [[ operations ]] on << SI-Nets >> are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the conceptual system of NL .", "h": ["operations"], "t": ["SI-Nets"]}, {"label": "USED-FOR", "tokens": "In this sense , operations on SI-Nets are not merely isomorphic to single epistemological objects , but can be viewed as a simulation of processes on a different level , that pertaining to the << conceptual system >> of [[ NL ]] .", "h": ["NL"], "t": ["conceptual system"]}, {"label": "COMPARE", "tokens": "For this purpose , we have designed a version of [[ KL-ONE ]] which represents the epistemological level , while the new experimental language , << KL-Conc >> , represents the conceptual level .", "h": ["KL-ONE"], "t": ["KL-Conc"]}, {"label": "FEATURE-OF", "tokens": "For this purpose , we have designed a version of << KL-ONE >> which represents the [[ epistemological level ]] , while the new experimental language , KL-Conc , represents the conceptual level .", "h": ["epistemological level"], "t": ["KL-ONE"]}, {"label": "FEATURE-OF", "tokens": "For this purpose , we have designed a version of KL-ONE which represents the epistemological level , while the new experimental language , << KL-Conc >> , represents the [[ conceptual level ]] .", "h": ["conceptual level"], "t": ["KL-Conc"]}, {"label": "USED-FOR", "tokens": "We present an [[ algorithm ]] for << calibrated camera relative pose estimation >> from lines .", "h": ["algorithm"], "t": ["calibrated camera relative pose estimation"]}, {"label": "USED-FOR", "tokens": "We evaluate the performance of the << algorithm >> using [[ synthetic and real data ]] .", "h": ["synthetic and real data"], "t": ["algorithm"]}, {"label": "CONJUNCTION", "tokens": "The intended use of the [[ algorithm ]] is with robust << hypothesize-and-test frameworks >> such as RANSAC .", "h": ["algorithm"], "t": ["hypothesize-and-test frameworks"]}, {"label": "HYPONYM-OF", "tokens": "The intended use of the algorithm is with robust << hypothesize-and-test frameworks >> such as [[ RANSAC ]] .", "h": ["RANSAC"], "t": ["hypothesize-and-test frameworks"]}, {"label": "USED-FOR", "tokens": "Our [[ approach ]] is suitable for << urban and indoor environments >> where most lines are either parallel or orthogonal to each other .", "h": ["approach"], "t": ["urban and indoor environments"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a [[ fully automated extraction system ]] , named IntEx , to identify << gene and protein interactions >> in biomedical text .", "h": ["fully automated extraction system"], "t": ["gene and protein interactions"]}, {"label": "HYPONYM-OF", "tokens": "In this paper , we present a << fully automated extraction system >> , named [[ IntEx ]] , to identify gene and protein interactions in biomedical text .", "h": ["IntEx"], "t": ["fully automated extraction system"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a fully automated extraction system , named IntEx , to identify << gene and protein interactions >> in [[ biomedical text ]] .", "h": ["biomedical text"], "t": ["gene and protein interactions"]}, {"label": "USED-FOR", "tokens": "Then , tagging << biological entities >> with the help of [[ biomedical and linguistic ontologies ]] .", "h": ["biomedical and linguistic ontologies"], "t": ["biological entities"]}, {"label": "USED-FOR", "tokens": "Our [[ extraction system ]] handles complex sentences and extracts << multiple and nested interactions >> specified in a sentence .", "h": ["extraction system"], "t": ["multiple and nested interactions"]}, {"label": "COMPARE", "tokens": "Experimental evaluations with two other state of the art << extraction systems >> indicate that the [[ IntEx system ]] achieves better performance without the labor intensive pattern engineering requirement .", "h": ["IntEx system"], "t": ["extraction systems"]}, {"label": "USED-FOR", "tokens": "This paper introduces a [[ method ]] for << computational analysis of move structures >> in abstracts of research articles .", "h": ["method"], "t": ["computational analysis of move structures"]}, {"label": "USED-FOR", "tokens": "This paper introduces a method for << computational analysis of move structures >> in [[ abstracts of research articles ]] .", "h": ["abstracts of research articles"], "t": ["computational analysis of move structures"]}, {"label": "USED-FOR", "tokens": "The method involves automatically gathering a large number of << abstracts >> from the [[ Web ]] and building a language model of abstract moves .", "h": ["Web"], "t": ["abstracts"]}, {"label": "USED-FOR", "tokens": "The method involves automatically gathering a large number of abstracts from the Web and building a << language model >> of [[ abstract moves ]] .", "h": ["abstract moves"], "t": ["language model"]}, {"label": "HYPONYM-OF", "tokens": "We also present a << prototype concordancer >> , [[ CARE ]] , which exploits the move-tagged abstracts for digital learning .", "h": ["CARE"], "t": ["prototype concordancer"]}, {"label": "USED-FOR", "tokens": "We also present a prototype concordancer , [[ CARE ]] , which exploits the << move-tagged abstracts >> for digital learning .", "h": ["CARE"], "t": ["move-tagged abstracts"]}, {"label": "USED-FOR", "tokens": "We also present a prototype concordancer , CARE , which exploits the [[ move-tagged abstracts ]] for << digital learning >> .", "h": ["move-tagged abstracts"], "t": ["digital learning"]}, {"label": "USED-FOR", "tokens": "This [[ system ]] provides a promising << approach >> to Web-based computer-assisted academic writing .", "h": ["system"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "This system provides a promising [[ approach ]] to << Web-based computer-assisted academic writing >> .", "h": ["approach"], "t": ["Web-based computer-assisted academic writing"]}, {"label": "USED-FOR", "tokens": "This work presents a [[ real-time system ]] for << multiple object tracking in dynamic scenes >> .", "h": ["real-time system"], "t": ["multiple object tracking in dynamic scenes"]}, {"label": "USED-FOR", "tokens": "A unique characteristic of the [[ system ]] is its ability to cope with << long-duration and complete occlusion >> without a prior knowledge about the shape or motion of objects .", "h": ["system"], "t": ["long-duration and complete occlusion"]}, {"label": "FEATURE-OF", "tokens": "A unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a [[ prior knowledge ]] about the << shape >> or motion of objects .", "h": ["prior knowledge"], "t": ["shape"]}, {"label": "FEATURE-OF", "tokens": "A unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a [[ prior knowledge ]] about the shape or << motion of objects >> .", "h": ["prior knowledge"], "t": ["motion of objects"]}, {"label": "CONJUNCTION", "tokens": "A unique characteristic of the system is its ability to cope with long-duration and complete occlusion without a prior knowledge about the [[ shape ]] or << motion of objects >> .", "h": ["shape"], "t": ["motion of objects"]}, {"label": "EVALUATE-FOR", "tokens": "The << system >> produces good segment and [[ tracking ]] results at a frame rate of 15-20 fps for image size of 320x240 , as demonstrated by extensive experiments performed using video sequences under different conditions indoor and outdoor with long-duration and complete occlusions in changing background .", "h": ["tracking"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "We propose a [[ method ]] of << organizing reading materials >> for vocabulary learning .", "h": ["method"], "t": ["organizing reading materials"]}, {"label": "USED-FOR", "tokens": "We propose a method of [[ organizing reading materials ]] for << vocabulary learning >> .", "h": ["organizing reading materials"], "t": ["vocabulary learning"]}, {"label": "HYPONYM-OF", "tokens": "We used a specialized vocabulary for an English certification test as the target vocabulary and used [[ English Wikipedia ]] , a << free-content encyclopedia >> , as the target corpus .", "h": ["English Wikipedia"], "t": ["free-content encyclopedia"]}, {"label": "USED-FOR", "tokens": "A novel [[ bootstrapping approach ]] to << Named Entity -LRB- NE -RRB- tagging >> using concept-based seeds and successive learners is presented .", "h": ["bootstrapping approach"], "t": ["Named Entity -LRB- NE -RRB- tagging"]}, {"label": "USED-FOR", "tokens": "A novel << bootstrapping approach >> to Named Entity -LRB- NE -RRB- tagging using [[ concept-based seeds ]] and successive learners is presented .", "h": ["concept-based seeds"], "t": ["bootstrapping approach"]}, {"label": "CONJUNCTION", "tokens": "A novel bootstrapping approach to Named Entity -LRB- NE -RRB- tagging using [[ concept-based seeds ]] and << successive learners >> is presented .", "h": ["concept-based seeds"], "t": ["successive learners"]}, {"label": "USED-FOR", "tokens": "A novel << bootstrapping approach >> to Named Entity -LRB- NE -RRB- tagging using concept-based seeds and [[ successive learners ]] is presented .", "h": ["successive learners"], "t": ["bootstrapping approach"]}, {"label": "HYPONYM-OF", "tokens": "This approach only requires a few common noun or pronoun seeds that correspond to the concept for the targeted << NE >> , e.g. he/she/man / woman for [[ PERSON NE ]] .", "h": ["PERSON NE"], "t": ["NE"]}, {"label": "USED-FOR", "tokens": "The << bootstrapping procedure >> is implemented as training two [[ successive learners ]] .", "h": ["successive learners"], "t": ["bootstrapping procedure"]}, {"label": "USED-FOR", "tokens": "First , [[ decision list ]] is used to learn the << parsing-based NE rules >> .", "h": ["decision list"], "t": ["parsing-based NE rules"]}, {"label": "USED-FOR", "tokens": "The resulting [[ NE system ]] approaches << supervised NE >> performance for some NE types .", "h": ["NE system"], "t": ["supervised NE"]}, {"label": "EVALUATE-FOR", "tokens": "We present the first known empirical test of an increasingly common speculative claim , by evaluating a representative << Chinese-to-English SMT model >> directly on [[ word sense disambiguation ]] performance , using standard WSD evaluation methodology and datasets from the Senseval-3 Chinese lexical sample task .", "h": ["word sense disambiguation"], "t": ["Chinese-to-English SMT model"]}, {"label": "EVALUATE-FOR", "tokens": "We present the first known empirical test of an increasingly common speculative claim , by evaluating a representative << Chinese-to-English SMT model >> directly on word sense disambiguation performance , using standard [[ WSD evaluation methodology ]] and datasets from the Senseval-3 Chinese lexical sample task .", "h": ["WSD evaluation methodology"], "t": ["Chinese-to-English SMT model"]}, {"label": "EVALUATE-FOR", "tokens": "We present the first known empirical test of an increasingly common speculative claim , by evaluating a representative << Chinese-to-English SMT model >> directly on word sense disambiguation performance , using standard WSD evaluation methodology and datasets from the [[ Senseval-3 Chinese lexical sample task ]] .", "h": ["Senseval-3 Chinese lexical sample task"], "t": ["Chinese-to-English SMT model"]}, {"label": "EVALUATE-FOR", "tokens": "Much effort has been put in designing and evaluating << dedicated word sense disambiguation -LRB- WSD -RRB- models >> , in particular with the [[ Senseval series of workshops ]] .", "h": ["Senseval series of workshops"], "t": ["dedicated word sense disambiguation -LRB- WSD -RRB- models"]}, {"label": "EVALUATE-FOR", "tokens": "At the same time , the recent improvements in the [[ BLEU scores ]] of << statistical machine translation -LRB- SMT -RRB- >> suggests that SMT models are good at predicting the right translation of the words in source language sentences .", "h": ["BLEU scores"], "t": ["statistical machine translation -LRB- SMT -RRB-"]}, {"label": "USED-FOR", "tokens": "At the same time , the recent improvements in the BLEU scores of statistical machine translation -LRB- SMT -RRB- suggests that [[ SMT models ]] are good at predicting the right << translation >> of the words in source language sentences .", "h": ["SMT models"], "t": ["translation"]}, {"label": "EVALUATE-FOR", "tokens": "Surprisingly however , the [[ WSD accuracy ]] of << SMT models >> has never been evaluated and compared with that of the dedicated WSD models .", "h": ["WSD accuracy"], "t": ["SMT models"]}, {"label": "COMPARE", "tokens": "Surprisingly however , the << WSD accuracy >> of SMT models has never been evaluated and compared with [[ that ]] of the dedicated WSD models .", "h": ["that"], "t": ["WSD accuracy"]}, {"label": "EVALUATE-FOR", "tokens": "We present controlled experiments showing the [[ WSD accuracy ]] of current typical << SMT models >> to be significantly lower than that of all the dedicated WSD models considered .", "h": ["WSD accuracy"], "t": ["SMT models"]}, {"label": "COMPARE", "tokens": "We present controlled experiments showing the << WSD accuracy >> of current typical SMT models to be significantly lower than [[ that ]] of all the dedicated WSD models considered .", "h": ["that"], "t": ["WSD accuracy"]}, {"label": "COMPARE", "tokens": "This tends to support the view that despite recent speculative claims to the contrary , current [[ SMT models ]] do have limitations in comparison with << dedicated WSD models >> , and that SMT should benefit from the better predictions made by the WSD models .", "h": ["SMT models"], "t": ["dedicated WSD models"]}, {"label": "USED-FOR", "tokens": "This tends to support the view that despite recent speculative claims to the contrary , current SMT models do have limitations in comparison with dedicated WSD models , and that << SMT >> should benefit from the better predictions made by the [[ WSD models ]] .", "h": ["WSD models"], "t": ["SMT"]}, {"label": "USED-FOR", "tokens": "In this paper we present a novel , customizable : << IE paradigm >> that takes advantage of [[ predicate-argument structures ]] .", "h": ["predicate-argument structures"], "t": ["IE paradigm"]}, {"label": "USED-FOR", "tokens": "<< It >> is based on : -LRB- 1 -RRB- an extended set of [[ features ]] ; and -LRB- 2 -RRB- inductive decision tree learning .", "h": ["features"], "t": ["It"]}, {"label": "CONJUNCTION", "tokens": "It is based on : -LRB- 1 -RRB- an extended set of [[ features ]] ; and -LRB- 2 -RRB- << inductive decision tree learning >> .", "h": ["features"], "t": ["inductive decision tree learning"]}, {"label": "USED-FOR", "tokens": "<< It >> is based on : -LRB- 1 -RRB- an extended set of features ; and -LRB- 2 -RRB- [[ inductive decision tree learning ]] .", "h": ["inductive decision tree learning"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "The experimental results prove our claim that accurate [[ predicate-argument structures ]] enable high quality << IE >> results .", "h": ["predicate-argument structures"], "t": ["IE"]}, {"label": "USED-FOR", "tokens": "In this paper we present a [[ statistical profile ]] of the << Named Entity task >> , a specific information extraction task for which corpora in several languages are available .", "h": ["statistical profile"], "t": ["Named Entity task"]}, {"label": "HYPONYM-OF", "tokens": "In this paper we present a statistical profile of the [[ Named Entity task ]] , a specific << information extraction task >> for which corpora in several languages are available .", "h": ["Named Entity task"], "t": ["information extraction task"]}, {"label": "USED-FOR", "tokens": "Using the results of the [[ statistical analysis ]] , we propose an << algorithm >> for lower bound estimation for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis .", "h": ["statistical analysis"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "Using the results of the statistical analysis , we propose an [[ algorithm ]] for << lower bound estimation >> for Named Entity corpora and discuss the significance of the cross-lingual comparisons provided by the analysis .", "h": ["algorithm"], "t": ["lower bound estimation"]}, {"label": "USED-FOR", "tokens": "Using the results of the statistical analysis , we propose an algorithm for [[ lower bound estimation ]] for << Named Entity corpora >> and discuss the significance of the cross-lingual comparisons provided by the analysis .", "h": ["lower bound estimation"], "t": ["Named Entity corpora"]}, {"label": "HYPONYM-OF", "tokens": "We attack an inexplicably << under-explored language genre of spoken language >> -- [[ lyrics in music ]] -- via completely unsuper-vised induction of an SMT-style stochastic transduction grammar for hip hop lyrics , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .", "h": ["lyrics in music"], "t": ["under-explored language genre of spoken language"]}, {"label": "USED-FOR", "tokens": "We attack an inexplicably << under-explored language genre of spoken language >> -- lyrics in music -- via completely [[ unsuper-vised induction ]] of an SMT-style stochastic transduction grammar for hip hop lyrics , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .", "h": ["unsuper-vised induction"], "t": ["under-explored language genre of spoken language"]}, {"label": "USED-FOR", "tokens": "We attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely [[ unsuper-vised induction ]] of an << SMT-style stochastic transduction grammar >> for hip hop lyrics , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .", "h": ["unsuper-vised induction"], "t": ["SMT-style stochastic transduction grammar"]}, {"label": "USED-FOR", "tokens": "We attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely [[ unsuper-vised induction ]] of an SMT-style stochastic transduction grammar for hip hop lyrics , yielding a << fully-automatically learned challenge-response system >> that produces rhyming lyrics given an input .", "h": ["unsuper-vised induction"], "t": ["fully-automatically learned challenge-response system"]}, {"label": "FEATURE-OF", "tokens": "We attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely unsuper-vised induction of an << SMT-style stochastic transduction grammar >> for [[ hip hop lyrics ]] , yielding a fully-automatically learned challenge-response system that produces rhyming lyrics given an input .", "h": ["hip hop lyrics"], "t": ["SMT-style stochastic transduction grammar"]}, {"label": "USED-FOR", "tokens": "We attack an inexplicably under-explored language genre of spoken language -- lyrics in music -- via completely unsuper-vised induction of an SMT-style stochastic transduction grammar for hip hop lyrics , yielding a [[ fully-automatically learned challenge-response system ]] that produces << rhyming lyrics >> given an input .", "h": ["fully-automatically learned challenge-response system"], "t": ["rhyming lyrics"]}, {"label": "COMPARE", "tokens": "In spite of the level of difficulty of the challenge , the [[ model ]] nevertheless produces fluent output as judged by human evaluators , and performs significantly better than widely used << phrase-based SMT models >> upon the same task .", "h": ["model"], "t": ["phrase-based SMT models"]}, {"label": "EVALUATE-FOR", "tokens": "In spite of the level of difficulty of the challenge , the << model >> nevertheless produces fluent output as judged by human evaluators , and performs significantly better than widely used phrase-based SMT models upon the same [[ task ]] .", "h": ["task"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "In spite of the level of difficulty of the challenge , the model nevertheless produces fluent output as judged by human evaluators , and performs significantly better than widely used << phrase-based SMT models >> upon the same [[ task ]] .", "h": ["task"], "t": ["phrase-based SMT models"]}, {"label": "USED-FOR", "tokens": "In this paper , we investigate the problem of automatically << predicting segment boundaries >> in [[ spoken multiparty dialogue ]] .", "h": ["spoken multiparty dialogue"], "t": ["predicting segment boundaries"]}, {"label": "USED-FOR", "tokens": "We first apply [[ approaches ]] that have been proposed for << predicting top-level topic shifts >> to the problem of identifying subtopic boundaries .", "h": ["approaches"], "t": ["predicting top-level topic shifts"]}, {"label": "USED-FOR", "tokens": "We first apply [[ approaches ]] that have been proposed for predicting top-level topic shifts to the problem of << identifying subtopic boundaries >> .", "h": ["approaches"], "t": ["identifying subtopic boundaries"]}, {"label": "FEATURE-OF", "tokens": "We first apply approaches that have been proposed for [[ predicting top-level topic shifts ]] to the problem of << identifying subtopic boundaries >> .", "h": ["predicting top-level topic shifts"], "t": ["identifying subtopic boundaries"]}, {"label": "COMPARE", "tokens": "We then explore the impact on performance of using [[ ASR output ]] as opposed to << human transcription >> .", "h": ["ASR output"], "t": ["human transcription"]}, {"label": "PART-OF", "tokens": "Examination of the effect of features shows that << predicting top-level and predicting subtopic boundaries >> are two distinct tasks : -LRB- 1 -RRB- for [[ predicting subtopic boundaries ]] , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .", "h": ["predicting subtopic boundaries"], "t": ["predicting top-level and predicting subtopic boundaries"]}, {"label": "USED-FOR", "tokens": "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -LRB- 1 -RRB- for << predicting subtopic boundaries >> , the [[ lexical cohesion-based approach ]] alone can achieve competitive results , -LRB- 2 -RRB- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .", "h": ["lexical cohesion-based approach"], "t": ["predicting subtopic boundaries"]}, {"label": "PART-OF", "tokens": "Examination of the effect of features shows that << predicting top-level and predicting subtopic boundaries >> are two distinct tasks : -LRB- 1 -RRB- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for [[ predicting top-level boundaries ]] , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .", "h": ["predicting top-level boundaries"], "t": ["predicting top-level and predicting subtopic boundaries"]}, {"label": "USED-FOR", "tokens": "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -LRB- 1 -RRB- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for << predicting top-level boundaries >> , the [[ machine learning approach ]] that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .", "h": ["machine learning approach"], "t": ["predicting top-level boundaries"]}, {"label": "CONJUNCTION", "tokens": "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -LRB- 1 -RRB- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for predicting top-level boundaries , the << machine learning approach >> that combines [[ lexical-cohesion and conversational features ]] performs best , and -LRB- 3 -RRB- conversational cues , such as cue phrases and overlapping speech , are better indicators for the top-level prediction task .", "h": ["lexical-cohesion and conversational features"], "t": ["machine learning approach"]}, {"label": "HYPONYM-OF", "tokens": "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -LRB- 1 -RRB- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- << conversational cues >> , such as [[ cue phrases ]] and overlapping speech , are better indicators for the top-level prediction task .", "h": ["cue phrases"], "t": ["conversational cues"]}, {"label": "HYPONYM-OF", "tokens": "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -LRB- 1 -RRB- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- << conversational cues >> , such as cue phrases and [[ overlapping speech ]] , are better indicators for the top-level prediction task .", "h": ["overlapping speech"], "t": ["conversational cues"]}, {"label": "CONJUNCTION", "tokens": "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -LRB- 1 -RRB- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- conversational cues , such as << cue phrases >> and [[ overlapping speech ]] , are better indicators for the top-level prediction task .", "h": ["overlapping speech"], "t": ["cue phrases"]}, {"label": "USED-FOR", "tokens": "Examination of the effect of features shows that predicting top-level and predicting subtopic boundaries are two distinct tasks : -LRB- 1 -RRB- for predicting subtopic boundaries , the lexical cohesion-based approach alone can achieve competitive results , -LRB- 2 -RRB- for predicting top-level boundaries , the machine learning approach that combines lexical-cohesion and conversational features performs best , and -LRB- 3 -RRB- conversational cues , such as cue phrases and overlapping speech , are better [[ indicators ]] for the << top-level prediction task >> .", "h": ["indicators"], "t": ["top-level prediction task"]}, {"label": "FEATURE-OF", "tokens": "We also find that the [[ transcription errors ]] inevitable in << ASR output >> have a negative impact on models that combine lexical-cohesion and conversational features , but do not change the general preference of approach for the two tasks .", "h": ["transcription errors"], "t": ["ASR output"]}, {"label": "CONJUNCTION", "tokens": "We also find that the transcription errors inevitable in ASR output have a negative impact on [[ models ]] that combine << lexical-cohesion and conversational features >> , but do not change the general preference of approach for the two tasks .", "h": ["models"], "t": ["lexical-cohesion and conversational features"]}, {"label": "USED-FOR", "tokens": "We describe a simple [[ unsupervised technique ]] for learning << morphology >> by identifying hubs in an automaton .", "h": ["unsupervised technique"], "t": ["morphology"]}, {"label": "USED-FOR", "tokens": "We describe a simple << unsupervised technique >> for learning morphology by identifying [[ hubs ]] in an automaton .", "h": ["hubs"], "t": ["unsupervised technique"]}, {"label": "PART-OF", "tokens": "We describe a simple unsupervised technique for learning morphology by identifying [[ hubs ]] in an << automaton >> .", "h": ["hubs"], "t": ["automaton"]}, {"label": "HYPONYM-OF", "tokens": "For our purposes , a [[ hub ]] is a << node >> in a graph with in-degree greater than one and out-degree greater than one .", "h": ["hub"], "t": ["node"]}, {"label": "PART-OF", "tokens": "For our purposes , a hub is a [[ node ]] in a << graph >> with in-degree greater than one and out-degree greater than one .", "h": ["node"], "t": ["graph"]}, {"label": "USED-FOR", "tokens": "We create a [[ word-trie ]] , transform it into a minimal DFA , then identify << hubs >> .", "h": ["word-trie"], "t": ["hubs"]}, {"label": "USED-FOR", "tokens": "We create a word-trie , transform it into a [[ minimal DFA ]] , then identify << hubs >> .", "h": ["minimal DFA"], "t": ["hubs"]}, {"label": "PART-OF", "tokens": "In << Bayesian machine learning >> , [[ conjugate priors ]] are popular , mostly due to mathematical convenience .", "h": ["conjugate priors"], "t": ["Bayesian machine learning"]}, {"label": "FEATURE-OF", "tokens": "Specifically , we formulate the << conjugate prior >> in the form of [[ Bregman divergence ]] and show that it is the inherent geometry of conjugate priors that makes them appropriate and intuitive .", "h": ["Bregman divergence"], "t": ["conjugate prior"]}, {"label": "USED-FOR", "tokens": "We use this [[ geometric understanding of conjugate priors ]] to derive the << hyperparameters >> and expression of the prior used to couple the generative and discriminative components of a hybrid model for semi-supervised learning .", "h": ["geometric understanding of conjugate priors"], "t": ["hyperparameters"]}, {"label": "USED-FOR", "tokens": "We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the [[ prior ]] used to couple the << generative and discriminative components >> of a hybrid model for semi-supervised learning .", "h": ["prior"], "t": ["generative and discriminative components"]}, {"label": "PART-OF", "tokens": "We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the [[ generative and discriminative components ]] of a << hybrid model >> for semi-supervised learning .", "h": ["generative and discriminative components"], "t": ["hybrid model"]}, {"label": "USED-FOR", "tokens": "We use this geometric understanding of conjugate priors to derive the hyperparameters and expression of the prior used to couple the generative and discriminative components of a [[ hybrid model ]] for << semi-supervised learning >> .", "h": ["hybrid model"], "t": ["semi-supervised learning"]}, {"label": "HYPONYM-OF", "tokens": "This paper defines a << generative probabilistic model of parse trees >> , which we call [[ PCFG-LA ]] .", "h": ["PCFG-LA"], "t": ["generative probabilistic model of parse trees"]}, {"label": "USED-FOR", "tokens": "This << model >> is an extension of [[ PCFG ]] in which non-terminal symbols are augmented with latent variables .", "h": ["PCFG"], "t": ["model"]}, {"label": "PART-OF", "tokens": "This model is an extension of << PCFG >> in which [[ non-terminal symbols ]] are augmented with latent variables .", "h": ["non-terminal symbols"], "t": ["PCFG"]}, {"label": "USED-FOR", "tokens": "This model is an extension of PCFG in which << non-terminal symbols >> are augmented with [[ latent variables ]] .", "h": ["latent variables"], "t": ["non-terminal symbols"]}, {"label": "USED-FOR", "tokens": "<< Finegrained CFG rules >> are automatically induced from a [[ parsed corpus ]] by training a PCFG-LA model using an EM-algorithm .", "h": ["parsed corpus"], "t": ["Finegrained CFG rules"]}, {"label": "USED-FOR", "tokens": "Finegrained CFG rules are automatically induced from a parsed corpus by training a << PCFG-LA model >> using an [[ EM-algorithm ]] .", "h": ["EM-algorithm"], "t": ["PCFG-LA model"]}, {"label": "USED-FOR", "tokens": "Because << exact parsing >> with a [[ PCFG-LA ]] is NP-hard , several approximations are described and empirically compared .", "h": ["PCFG-LA"], "t": ["exact parsing"]}, {"label": "EVALUATE-FOR", "tokens": "In experiments using the [[ Penn WSJ corpus ]] , our automatically trained << model >> gave a performance of 86.6 % -LRB- F1 , sentences < 40 words -RRB- , which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection .", "h": ["Penn WSJ corpus"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "In experiments using the [[ Penn WSJ corpus ]] , our automatically trained model gave a performance of 86.6 % -LRB- F1 , sentences < 40 words -RRB- , which is comparable to that of an << unlexicalized PCFG parser >> created using extensive manual feature selection .", "h": ["Penn WSJ corpus"], "t": ["unlexicalized PCFG parser"]}, {"label": "COMPARE", "tokens": "In experiments using the Penn WSJ corpus , our automatically trained [[ model ]] gave a performance of 86.6 % -LRB- F1 , sentences < 40 words -RRB- , which is comparable to that of an << unlexicalized PCFG parser >> created using extensive manual feature selection .", "h": ["model"], "t": ["unlexicalized PCFG parser"]}, {"label": "EVALUATE-FOR", "tokens": "In experiments using the Penn WSJ corpus , our automatically trained << model >> gave a performance of 86.6 % -LRB- [[ F1 ]] , sentences < 40 words -RRB- , which is comparable to that of an unlexicalized PCFG parser created using extensive manual feature selection .", "h": ["F1"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "In experiments using the Penn WSJ corpus , our automatically trained model gave a performance of 86.6 % -LRB- [[ F1 ]] , sentences < 40 words -RRB- , which is comparable to that of an << unlexicalized PCFG parser >> created using extensive manual feature selection .", "h": ["F1"], "t": ["unlexicalized PCFG parser"]}, {"label": "USED-FOR", "tokens": "In experiments using the Penn WSJ corpus , our automatically trained model gave a performance of 86.6 % -LRB- F1 , sentences < 40 words -RRB- , which is comparable to that of an << unlexicalized PCFG parser >> created using extensive [[ manual feature selection ]] .", "h": ["manual feature selection"], "t": ["unlexicalized PCFG parser"]}, {"label": "USED-FOR", "tokens": "First , we present a new paradigm for << speaker-independent -LRB- SI -RRB- training of hidden Markov models -LRB- HMM -RRB- >> , which uses a large amount of [[ speech ]] from a few speakers instead of the traditional practice of using a little speech from many speakers .", "h": ["speech"], "t": ["speaker-independent -LRB- SI -RRB- training of hidden Markov models -LRB- HMM -RRB-"]}, {"label": "COMPARE", "tokens": "In addition , combination of the training speakers is done by [[ averaging the statistics of independently trained models ]] rather than the usual << pooling of all the speech data >> from many speakers prior to training .", "h": ["averaging the statistics of independently trained models"], "t": ["pooling of all the speech data"]}, {"label": "EVALUATE-FOR", "tokens": "With only 12 training speakers for << SI recognition >> , we achieved a 7.5 % [[ word error rate ]] on a standard grammar and test set from the DARPA Resource Management corpus .", "h": ["word error rate"], "t": ["SI recognition"]}, {"label": "EVALUATE-FOR", "tokens": "With only 12 training speakers for << SI recognition >> , we achieved a 7.5 % word error rate on a standard grammar and test set from the [[ DARPA Resource Management corpus ]] .", "h": ["DARPA Resource Management corpus"], "t": ["SI recognition"]}, {"label": "EVALUATE-FOR", "tokens": "Second , we show a significant improvement for << speaker adaptation -LRB- SA -RRB- >> using the new [[ SI corpus ]] and a small amount of speech from the new -LRB- target -RRB- speaker .", "h": ["SI corpus"], "t": ["speaker adaptation -LRB- SA -RRB-"]}, {"label": "EVALUATE-FOR", "tokens": "Using only 40 utterances from the target speaker for << adaptation >> , the [[ error rate ]] dropped to 4.1 % -- a 45 % reduction in error compared to the SI result .", "h": ["error rate"], "t": ["adaptation"]}, {"label": "PART-OF", "tokens": "[[ Dictionary construction ]] , one of the most difficult tasks in developing a << machine translation system >> , is expensive .", "h": ["Dictionary construction"], "t": ["machine translation system"]}, {"label": "USED-FOR", "tokens": "To avoid this problem , we investigate how we build a << dictionary >> using existing [[ linguistic resources ]] .", "h": ["linguistic resources"], "t": ["dictionary"]}, {"label": "USED-FOR", "tokens": "Our algorithm can be applied to any language pairs , but for the present we focus on building a << Korean-to-Japanese dictionary >> using [[ English ]] as a pivot .", "h": ["English"], "t": ["Korean-to-Japanese dictionary"]}, {"label": "EVALUATE-FOR", "tokens": "We attempt three ways of [[ automatic construction ]] to corroborate the effect of the << directionality of dictionaries >> .", "h": ["automatic construction"], "t": ["directionality of dictionaries"]}, {"label": "USED-FOR", "tokens": "First , we introduce << `` one-time look up '' method >> using a [[ Korean-to-English and a Japanese-to-English dictionary ]] .", "h": ["Korean-to-English and a Japanese-to-English dictionary"], "t": ["`` one-time look up '' method"]}, {"label": "USED-FOR", "tokens": "Second , we show a << method >> using [[ `` overlapping constraint '' ]] with a Korean-to-English dictionary and an English-to-Japanese dictionary .", "h": ["`` overlapping constraint ''"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "Second , we show a << method >> using `` overlapping constraint '' with a [[ Korean-to-English dictionary ]] and an English-to-Japanese dictionary .", "h": ["Korean-to-English dictionary"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "Second , we show a method using `` overlapping constraint '' with a [[ Korean-to-English dictionary ]] and an << English-to-Japanese dictionary >> .", "h": ["Korean-to-English dictionary"], "t": ["English-to-Japanese dictionary"]}, {"label": "USED-FOR", "tokens": "Second , we show a << method >> using `` overlapping constraint '' with a Korean-to-English dictionary and an [[ English-to-Japanese dictionary ]] .", "h": ["English-to-Japanese dictionary"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "Third , we consider another alternative [[ method ]] rarely used for building a << dictionary >> : an English-to-Korean dictionary and English-to-Japanese dictionary .", "h": ["method"], "t": ["dictionary"]}, {"label": "HYPONYM-OF", "tokens": "Third , we consider another alternative method rarely used for building a << dictionary >> : an [[ English-to-Korean dictionary ]] and English-to-Japanese dictionary .", "h": ["English-to-Korean dictionary"], "t": ["dictionary"]}, {"label": "CONJUNCTION", "tokens": "Third , we consider another alternative method rarely used for building a dictionary : an [[ English-to-Korean dictionary ]] and << English-to-Japanese dictionary >> .", "h": ["English-to-Korean dictionary"], "t": ["English-to-Japanese dictionary"]}, {"label": "HYPONYM-OF", "tokens": "Third , we consider another alternative method rarely used for building a << dictionary >> : an English-to-Korean dictionary and [[ English-to-Japanese dictionary ]] .", "h": ["English-to-Japanese dictionary"], "t": ["dictionary"]}, {"label": "USED-FOR", "tokens": "An empirical comparison of [[ CFG filtering techniques ]] for << LTAG >> and HPSG is presented .", "h": ["CFG filtering techniques"], "t": ["LTAG"]}, {"label": "USED-FOR", "tokens": "An empirical comparison of [[ CFG filtering techniques ]] for LTAG and << HPSG >> is presented .", "h": ["CFG filtering techniques"], "t": ["HPSG"]}, {"label": "COMPARE", "tokens": "An empirical comparison of CFG filtering techniques for [[ LTAG ]] and << HPSG >> is presented .", "h": ["LTAG"], "t": ["HPSG"]}, {"label": "USED-FOR", "tokens": "We demonstrate that an [[ approximation of HPSG ]] produces a more effective << CFG filter >> than that of LTAG .", "h": ["approximation of HPSG"], "t": ["CFG filter"]}, {"label": "COMPARE", "tokens": "We demonstrate that an approximation of HPSG produces a more effective [[ CFG filter ]] than << that >> of LTAG .", "h": ["CFG filter"], "t": ["that"]}, {"label": "USED-FOR", "tokens": "We demonstrate that an approximation of HPSG produces a more effective CFG filter than [[ that ]] of << LTAG >> .", "h": ["that"], "t": ["LTAG"]}, {"label": "USED-FOR", "tokens": "<< Syntax-based statistical machine translation -LRB- MT -RRB- >> aims at applying [[ statistical models ]] to structured data .", "h": ["statistical models"], "t": ["Syntax-based statistical machine translation -LRB- MT -RRB-"]}, {"label": "USED-FOR", "tokens": "Syntax-based statistical machine translation -LRB- MT -RRB- aims at applying << statistical models >> to [[ structured data ]] .", "h": ["structured data"], "t": ["statistical models"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a << syntax-based statistical machine translation system >> based on a [[ probabilistic synchronous dependency insertion grammar ]] .", "h": ["probabilistic synchronous dependency insertion grammar"], "t": ["syntax-based statistical machine translation system"]}, {"label": "HYPONYM-OF", "tokens": "[[ Synchronous dependency insertion grammars ]] are a version of << synchronous grammars >> defined on dependency trees .", "h": ["Synchronous dependency insertion grammars"], "t": ["synchronous grammars"]}, {"label": "FEATURE-OF", "tokens": "<< Synchronous dependency insertion grammars >> are a version of synchronous grammars defined on [[ dependency trees ]] .", "h": ["dependency trees"], "t": ["Synchronous dependency insertion grammars"]}, {"label": "USED-FOR", "tokens": "We first introduce our [[ approach ]] to inducing such a << grammar >> from parallel corpora .", "h": ["approach"], "t": ["grammar"]}, {"label": "USED-FOR", "tokens": "We first introduce our approach to inducing such a << grammar >> from [[ parallel corpora ]] .", "h": ["parallel corpora"], "t": ["grammar"]}, {"label": "USED-FOR", "tokens": "Second , we describe the [[ graphical model ]] for the << machine translation task >> , which can also be viewed as a stochastic tree-to-tree transducer .", "h": ["graphical model"], "t": ["machine translation task"]}, {"label": "USED-FOR", "tokens": "Second , we describe the << graphical model >> for the machine translation task , which can also be viewed as a [[ stochastic tree-to-tree transducer ]] .", "h": ["stochastic tree-to-tree transducer"], "t": ["graphical model"]}, {"label": "USED-FOR", "tokens": "We introduce a [[ polynomial time decoding algorithm ]] for the << model >> .", "h": ["polynomial time decoding algorithm"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "We evaluate the outputs of our << MT system >> using the [[ NIST and Bleu automatic MT evaluation software ]] .", "h": ["NIST and Bleu automatic MT evaluation software"], "t": ["MT system"]}, {"label": "COMPARE", "tokens": "The result shows that our [[ system ]] outperforms the << baseline system >> based on the IBM models in both translation speed and quality .", "h": ["system"], "t": ["baseline system"]}, {"label": "USED-FOR", "tokens": "The result shows that our system outperforms the << baseline system >> based on the [[ IBM models ]] in both translation speed and quality .", "h": ["IBM models"], "t": ["baseline system"]}, {"label": "EVALUATE-FOR", "tokens": "The result shows that our << system >> outperforms the baseline system based on the IBM models in both [[ translation speed and quality ]] .", "h": ["translation speed and quality"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "The result shows that our system outperforms the << baseline system >> based on the IBM models in both [[ translation speed and quality ]] .", "h": ["translation speed and quality"], "t": ["baseline system"]}, {"label": "USED-FOR", "tokens": "We propose a << framework >> to derive the distance between concepts from [[ distributional measures of word co-occurrences ]] .", "h": ["distributional measures of word co-occurrences"], "t": ["framework"]}, {"label": "USED-FOR", "tokens": "We show that the newly proposed [[ concept-distance measures ]] outperform traditional distributional word-distance measures in the << tasks >> of -LRB- 1 -RRB- ranking word pairs in order of semantic distance , and -LRB- 2 -RRB- correcting real-word spelling errors .", "h": ["concept-distance measures"], "t": ["tasks"]}, {"label": "COMPARE", "tokens": "We show that the newly proposed << concept-distance measures >> outperform traditional [[ distributional word-distance measures ]] in the tasks of -LRB- 1 -RRB- ranking word pairs in order of semantic distance , and -LRB- 2 -RRB- correcting real-word spelling errors .", "h": ["distributional word-distance measures"], "t": ["concept-distance measures"]}, {"label": "USED-FOR", "tokens": "We show that the newly proposed concept-distance measures outperform traditional [[ distributional word-distance measures ]] in the << tasks >> of -LRB- 1 -RRB- ranking word pairs in order of semantic distance , and -LRB- 2 -RRB- correcting real-word spelling errors .", "h": ["distributional word-distance measures"], "t": ["tasks"]}, {"label": "HYPONYM-OF", "tokens": "We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the << tasks >> of -LRB- 1 -RRB- [[ ranking word pairs in order of semantic distance ]] , and -LRB- 2 -RRB- correcting real-word spelling errors .", "h": ["ranking word pairs in order of semantic distance"], "t": ["tasks"]}, {"label": "HYPONYM-OF", "tokens": "We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the << tasks >> of -LRB- 1 -RRB- ranking word pairs in order of semantic distance , and -LRB- 2 -RRB- [[ correcting real-word spelling errors ]] .", "h": ["correcting real-word spelling errors"], "t": ["tasks"]}, {"label": "CONJUNCTION", "tokens": "We show that the newly proposed concept-distance measures outperform traditional distributional word-distance measures in the tasks of -LRB- 1 -RRB- << ranking word pairs in order of semantic distance >> , and -LRB- 2 -RRB- [[ correcting real-word spelling errors ]] .", "h": ["correcting real-word spelling errors"], "t": ["ranking word pairs in order of semantic distance"]}, {"label": "EVALUATE-FOR", "tokens": "In the latter [[ task ]] , of all the << WordNet-based measures >> , only that proposed by Jiang and Conrath outperforms the best distributional concept-distance measures .", "h": ["task"], "t": ["WordNet-based measures"]}, {"label": "EVALUATE-FOR", "tokens": "In the latter [[ task ]] , of all the WordNet-based measures , only that proposed by Jiang and Conrath outperforms the best << distributional concept-distance measures >> .", "h": ["task"], "t": ["distributional concept-distance measures"]}, {"label": "COMPARE", "tokens": "In the latter task , of all the << WordNet-based measures >> , only that proposed by Jiang and Conrath outperforms the best [[ distributional concept-distance measures ]] .", "h": ["distributional concept-distance measures"], "t": ["WordNet-based measures"]}, {"label": "CONJUNCTION", "tokens": "One of the main results of this work is the definition of a relation between [[ broad semantic classes ]] and << LCS meaning components >> .", "h": ["broad semantic classes"], "t": ["LCS meaning components"]}, {"label": "USED-FOR", "tokens": "Our [[ acquisition program - LEXICALL - ]] takes , as input , the result of previous work on verb classification and thematic grid tagging , and outputs << LCS representations >> for different languages .", "h": ["acquisition program - LEXICALL -"], "t": ["LCS representations"]}, {"label": "USED-FOR", "tokens": "Our << acquisition program - LEXICALL - >> takes , as input , the result of previous work on [[ verb classification ]] and thematic grid tagging , and outputs LCS representations for different languages .", "h": ["verb classification"], "t": ["acquisition program - LEXICALL -"]}, {"label": "CONJUNCTION", "tokens": "Our acquisition program - LEXICALL - takes , as input , the result of previous work on [[ verb classification ]] and << thematic grid tagging >> , and outputs LCS representations for different languages .", "h": ["verb classification"], "t": ["thematic grid tagging"]}, {"label": "USED-FOR", "tokens": "Our << acquisition program - LEXICALL - >> takes , as input , the result of previous work on verb classification and [[ thematic grid tagging ]] , and outputs LCS representations for different languages .", "h": ["thematic grid tagging"], "t": ["acquisition program - LEXICALL -"]}, {"label": "USED-FOR", "tokens": "These [[ representations ]] have been ported into << English , Arabic and Spanish lexicons >> , each containing approximately 9000 verbs .", "h": ["representations"], "t": ["English , Arabic and Spanish lexicons"]}, {"label": "USED-FOR", "tokens": "We are currently using these [[ lexicons ]] in an << operational foreign language tutoring >> and machine translation .", "h": ["lexicons"], "t": ["operational foreign language tutoring"]}, {"label": "USED-FOR", "tokens": "We are currently using these [[ lexicons ]] in an operational foreign language tutoring and << machine translation >> .", "h": ["lexicons"], "t": ["machine translation"]}, {"label": "CONJUNCTION", "tokens": "We are currently using these lexicons in an [[ operational foreign language tutoring ]] and << machine translation >> .", "h": ["operational foreign language tutoring"], "t": ["machine translation"]}, {"label": "USED-FOR", "tokens": "The theoretical study of the [[ range concatenation grammar -LSB- RCG -RSB- formalism ]] has revealed many attractive properties which may be used in << NLP >> .", "h": ["range concatenation grammar -LSB- RCG -RSB- formalism"], "t": ["NLP"]}, {"label": "FEATURE-OF", "tokens": "In particular , << range concatenation languages -LSB- RCL -RSB- >> can be parsed in [[ polynomial time ]] and many classical grammatical formalisms can be translated into equivalent RCGs without increasing their worst-case parsing time complexity .", "h": ["polynomial time"], "t": ["range concatenation languages -LSB- RCL -RSB-"]}, {"label": "EVALUATE-FOR", "tokens": "In particular , range concatenation languages -LSB- RCL -RSB- can be parsed in polynomial time and many classical << grammatical formalisms >> can be translated into equivalent RCGs without increasing their [[ worst-case parsing time complexity ]] .", "h": ["worst-case parsing time complexity"], "t": ["grammatical formalisms"]}, {"label": "FEATURE-OF", "tokens": "For example , after translation into an equivalent RCG , any << tree adjoining grammar >> can be parsed in [[ O -LRB- n6 -RRB- time ]] .", "h": ["O -LRB- n6 -RRB- time"], "t": ["tree adjoining grammar"]}, {"label": "USED-FOR", "tokens": "In this paper , we study a [[ parsing technique ]] whose purpose is to improve the practical efficiency of << RCL parsers >> .", "h": ["parsing technique"], "t": ["RCL parsers"]}, {"label": "USED-FOR", "tokens": "The non-deterministic parsing choices of the [[ main parser ]] for a << language L >> are directed by a guide which uses the shared derivation forest output by a prior RCL parser for a suitable superset of L .", "h": ["main parser"], "t": ["language L"]}, {"label": "USED-FOR", "tokens": "The non-deterministic parsing choices of the main parser for a language L are directed by a guide which uses the << shared derivation forest >> output by a prior [[ RCL parser ]] for a suitable superset of L .", "h": ["RCL parser"], "t": ["shared derivation forest"]}, {"label": "EVALUATE-FOR", "tokens": "The results of a practical evaluation of this << method >> on a [[ wide coverage English grammar ]] are given .", "h": ["wide coverage English grammar"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "In this paper we introduce [[ Ant-Q ]] , a family of algorithms which present many similarities with Q-learning -LRB- Watkins , 1989 -RRB- , and which we apply to the solution of << symmetric and asym-metric instances of the traveling salesman problem -LRB- TSP -RRB- >> .", "h": ["Ant-Q"], "t": ["symmetric and asym-metric instances of the traveling salesman problem -LRB- TSP -RRB-"]}, {"label": "USED-FOR", "tokens": "<< Ant-Q algorithms >> were inspired by work on the [[ ant system -LRB- AS -RRB- ]] , a distributed algorithm for combinatorial optimization based on the metaphor of ant colonies which was recently proposed in -LRB- Dorigo , 1992 ; Dorigo , Maniezzo and Colorni , 1996 -RRB- .", "h": ["ant system -LRB- AS -RRB-"], "t": ["Ant-Q algorithms"]}, {"label": "HYPONYM-OF", "tokens": "Ant-Q algorithms were inspired by work on the [[ ant system -LRB- AS -RRB- ]] , a << distributed algorithm >> for combinatorial optimization based on the metaphor of ant colonies which was recently proposed in -LRB- Dorigo , 1992 ; Dorigo , Maniezzo and Colorni , 1996 -RRB- .", "h": ["ant system -LRB- AS -RRB-"], "t": ["distributed algorithm"]}, {"label": "USED-FOR", "tokens": "Ant-Q algorithms were inspired by work on the ant system -LRB- AS -RRB- , a [[ distributed algorithm ]] for << combinatorial optimization >> based on the metaphor of ant colonies which was recently proposed in -LRB- Dorigo , 1992 ; Dorigo , Maniezzo and Colorni , 1996 -RRB- .", "h": ["distributed algorithm"], "t": ["combinatorial optimization"]}, {"label": "HYPONYM-OF", "tokens": "We show that [[ AS ]] is a particular instance of the << Ant-Q family >> , and that there are instances of this family which perform better than AS .", "h": ["AS"], "t": ["Ant-Q family"]}, {"label": "PART-OF", "tokens": "We show that AS is a particular instance of the Ant-Q family , and that there are [[ instances ]] of this << family >> which perform better than AS .", "h": ["instances"], "t": ["family"]}, {"label": "COMPARE", "tokens": "We show that AS is a particular instance of the Ant-Q family , and that there are [[ instances ]] of this family which perform better than << AS >> .", "h": ["instances"], "t": ["AS"]}, {"label": "USED-FOR", "tokens": "We experimentally investigate the functioning of Ant-Q and we show that the results obtained by [[ Ant-Q ]] on << symmetric TSP >> 's are competitive with those obtained by other heuristic approaches based on neural networks or local search .", "h": ["Ant-Q"], "t": ["symmetric TSP"]}, {"label": "COMPARE", "tokens": "We experimentally investigate the functioning of Ant-Q and we show that the results obtained by [[ Ant-Q ]] on symmetric TSP 's are competitive with those obtained by other << heuristic approaches >> based on neural networks or local search .", "h": ["Ant-Q"], "t": ["heuristic approaches"]}, {"label": "USED-FOR", "tokens": "We experimentally investigate the functioning of Ant-Q and we show that the results obtained by Ant-Q on symmetric TSP 's are competitive with those obtained by other << heuristic approaches >> based on [[ neural networks ]] or local search .", "h": ["neural networks"], "t": ["heuristic approaches"]}, {"label": "CONJUNCTION", "tokens": "We experimentally investigate the functioning of Ant-Q and we show that the results obtained by Ant-Q on symmetric TSP 's are competitive with those obtained by other heuristic approaches based on [[ neural networks ]] or << local search >> .", "h": ["neural networks"], "t": ["local search"]}, {"label": "USED-FOR", "tokens": "We experimentally investigate the functioning of Ant-Q and we show that the results obtained by Ant-Q on symmetric TSP 's are competitive with those obtained by other << heuristic approaches >> based on neural networks or [[ local search ]] .", "h": ["local search"], "t": ["heuristic approaches"]}, {"label": "USED-FOR", "tokens": "Finally , we apply [[ Ant-Q ]] to some difficult << asymmetric TSP >> 's obtaining very good results : Ant-Q was able to find solutions of a quality which usually can be found only by very specialized algorithms .", "h": ["Ant-Q"], "t": ["asymmetric TSP"]}, {"label": "USED-FOR", "tokens": "In this paper , we develop a [[ geometric framework ]] for << linear or nonlinear discriminant subspace learning and classification >> .", "h": ["geometric framework"], "t": ["linear or nonlinear discriminant subspace learning and classification"]}, {"label": "USED-FOR", "tokens": "In our framework , the << structures of classes >> are conceptualized as a [[ semi-Riemannian manifold ]] which is considered as a submanifold embedded in an ambient semi-Riemannian space .", "h": ["semi-Riemannian manifold"], "t": ["structures of classes"]}, {"label": "PART-OF", "tokens": "In our framework , the structures of classes are conceptualized as a semi-Riemannian manifold which is considered as a [[ submanifold ]] embedded in an << ambient semi-Riemannian space >> .", "h": ["submanifold"], "t": ["ambient semi-Riemannian space"]}, {"label": "USED-FOR", "tokens": "The << class structures >> of original samples can be characterized and deformed by [[ local metrics of the semi-Riemannian space ]] .", "h": ["local metrics of the semi-Riemannian space"], "t": ["class structures"]}, {"label": "USED-FOR", "tokens": "<< Semi-Riemannian metrics >> are uniquely determined by the [[ smoothing of discrete functions ]] and the nullity of the semi-Riemannian space .", "h": ["smoothing of discrete functions"], "t": ["Semi-Riemannian metrics"]}, {"label": "CONJUNCTION", "tokens": "Semi-Riemannian metrics are uniquely determined by the [[ smoothing of discrete functions ]] and the << nullity of the semi-Riemannian space >> .", "h": ["smoothing of discrete functions"], "t": ["nullity of the semi-Riemannian space"]}, {"label": "USED-FOR", "tokens": "<< Semi-Riemannian metrics >> are uniquely determined by the smoothing of discrete functions and the [[ nullity of the semi-Riemannian space ]] .", "h": ["nullity of the semi-Riemannian space"], "t": ["Semi-Riemannian metrics"]}, {"label": "FEATURE-OF", "tokens": "Based on the geometrization of class structures , optimizing << class structures >> in the [[ feature space ]] is equivalent to maximizing the quadratic quantities of metric tensors in the semi-Riemannian space .", "h": ["feature space"], "t": ["class structures"]}, {"label": "FEATURE-OF", "tokens": "Based on the geometrization of class structures , optimizing class structures in the feature space is equivalent to maximizing the << quadratic quantities of metric tensors >> in the [[ semi-Riemannian space ]] .", "h": ["semi-Riemannian space"], "t": ["quadratic quantities of metric tensors"]}, {"label": "USED-FOR", "tokens": "Based on the proposed [[ framework ]] , a novel << algorithm >> , dubbed as Semi-Riemannian Discriminant Analysis -LRB- SRDA -RRB- , is presented for subspace-based classification .", "h": ["framework"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "Based on the proposed framework , a novel [[ algorithm ]] , dubbed as Semi-Riemannian Discriminant Analysis -LRB- SRDA -RRB- , is presented for << subspace-based classification >> .", "h": ["algorithm"], "t": ["subspace-based classification"]}, {"label": "COMPARE", "tokens": "The performance of [[ SRDA ]] is tested on face recognition -LRB- singular case -RRB- and handwritten capital letter classification -LRB- nonsingular case -RRB- against existing << algorithms >> .", "h": ["SRDA"], "t": ["algorithms"]}, {"label": "EVALUATE-FOR", "tokens": "The performance of << SRDA >> is tested on [[ face recognition -LRB- singular case ]] -RRB- and handwritten capital letter classification -LRB- nonsingular case -RRB- against existing algorithms .", "h": ["face recognition -LRB- singular case"], "t": ["SRDA"]}, {"label": "CONJUNCTION", "tokens": "The performance of SRDA is tested on [[ face recognition -LRB- singular case ]] -RRB- and << handwritten capital letter classification -LRB- nonsingular case -RRB- >> against existing algorithms .", "h": ["face recognition -LRB- singular case"], "t": ["handwritten capital letter classification -LRB- nonsingular case -RRB-"]}, {"label": "EVALUATE-FOR", "tokens": "The performance of SRDA is tested on [[ face recognition -LRB- singular case ]] -RRB- and handwritten capital letter classification -LRB- nonsingular case -RRB- against existing << algorithms >> .", "h": ["face recognition -LRB- singular case"], "t": ["algorithms"]}, {"label": "EVALUATE-FOR", "tokens": "The performance of << SRDA >> is tested on face recognition -LRB- singular case -RRB- and [[ handwritten capital letter classification -LRB- nonsingular case -RRB- ]] against existing algorithms .", "h": ["handwritten capital letter classification -LRB- nonsingular case -RRB-"], "t": ["SRDA"]}, {"label": "EVALUATE-FOR", "tokens": "The performance of SRDA is tested on face recognition -LRB- singular case -RRB- and [[ handwritten capital letter classification -LRB- nonsingular case -RRB- ]] against existing << algorithms >> .", "h": ["handwritten capital letter classification -LRB- nonsingular case -RRB-"], "t": ["algorithms"]}, {"label": "USED-FOR", "tokens": "The experimental results show that [[ SRDA ]] works well on << recognition >> and classification , implying that semi-Riemannian geometry is a promising new tool for pattern recognition and machine learning .", "h": ["SRDA"], "t": ["recognition"]}, {"label": "USED-FOR", "tokens": "The experimental results show that [[ SRDA ]] works well on recognition and << classification >> , implying that semi-Riemannian geometry is a promising new tool for pattern recognition and machine learning .", "h": ["SRDA"], "t": ["classification"]}, {"label": "CONJUNCTION", "tokens": "The experimental results show that SRDA works well on [[ recognition ]] and << classification >> , implying that semi-Riemannian geometry is a promising new tool for pattern recognition and machine learning .", "h": ["recognition"], "t": ["classification"]}, {"label": "USED-FOR", "tokens": "The experimental results show that SRDA works well on recognition and classification , implying that [[ semi-Riemannian geometry ]] is a promising new tool for << pattern recognition >> and machine learning .", "h": ["semi-Riemannian geometry"], "t": ["pattern recognition"]}, {"label": "USED-FOR", "tokens": "The experimental results show that SRDA works well on recognition and classification , implying that [[ semi-Riemannian geometry ]] is a promising new tool for pattern recognition and << machine learning >> .", "h": ["semi-Riemannian geometry"], "t": ["machine learning"]}, {"label": "CONJUNCTION", "tokens": "The experimental results show that SRDA works well on recognition and classification , implying that semi-Riemannian geometry is a promising new tool for [[ pattern recognition ]] and << machine learning >> .", "h": ["pattern recognition"], "t": ["machine learning"]}, {"label": "COMPARE", "tokens": "A [[ deterministic parser ]] is under development which represents a departure from traditional << deterministic parsers >> in that it combines both symbolic and connectionist components .", "h": ["deterministic parser"], "t": ["deterministic parsers"]}, {"label": "PART-OF", "tokens": "A deterministic parser is under development which represents a departure from traditional deterministic parsers in that << it >> combines both [[ symbolic and connectionist components ]] .", "h": ["symbolic and connectionist components"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "The << connectionist component >> is trained either from [[ patterns ]] derived from the rules of a deterministic grammar .", "h": ["patterns"], "t": ["connectionist component"]}, {"label": "USED-FOR", "tokens": "The connectionist component is trained either from << patterns >> derived from the [[ rules of a deterministic grammar ]] .", "h": ["rules of a deterministic grammar"], "t": ["patterns"]}, {"label": "USED-FOR", "tokens": "The development and evolution of such a [[ hybrid architecture ]] has lead to a << parser >> which is superior to any known deterministic parser .", "h": ["hybrid architecture"], "t": ["parser"]}, {"label": "COMPARE", "tokens": "The development and evolution of such a hybrid architecture has lead to a [[ parser ]] which is superior to any known << deterministic parser >> .", "h": ["parser"], "t": ["deterministic parser"]}, {"label": "USED-FOR", "tokens": "Experiments are described and powerful [[ training techniques ]] are demonstrated that permit << decision-making >> by the connectionist component in the parsing process .", "h": ["training techniques"], "t": ["decision-making"]}, {"label": "USED-FOR", "tokens": "Experiments are described and powerful training techniques are demonstrated that permit << decision-making >> by the [[ connectionist component ]] in the parsing process .", "h": ["connectionist component"], "t": ["decision-making"]}, {"label": "PART-OF", "tokens": "Experiments are described and powerful training techniques are demonstrated that permit decision-making by the [[ connectionist component ]] in the << parsing process >> .", "h": ["connectionist component"], "t": ["parsing process"]}, {"label": "USED-FOR", "tokens": "Data are presented which show how a [[ connectionist -LRB- neural -RRB- network ]] trained with linguistic rules can parse both << expected -LRB- grammatical -RRB- sentences >> as well as some novel -LRB- ungrammatical or lexically ambiguous -RRB- sentences .", "h": ["connectionist -LRB- neural -RRB- network"], "t": ["expected -LRB- grammatical -RRB- sentences"]}, {"label": "USED-FOR", "tokens": "Data are presented which show how a [[ connectionist -LRB- neural -RRB- network ]] trained with linguistic rules can parse both expected -LRB- grammatical -RRB- sentences as well as some novel << -LRB- ungrammatical or lexically ambiguous -RRB- sentences >> .", "h": ["connectionist -LRB- neural -RRB- network"], "t": ["-LRB- ungrammatical or lexically ambiguous -RRB- sentences"]}, {"label": "USED-FOR", "tokens": "Data are presented which show how a << connectionist -LRB- neural -RRB- network >> trained with [[ linguistic rules ]] can parse both expected -LRB- grammatical -RRB- sentences as well as some novel -LRB- ungrammatical or lexically ambiguous -RRB- sentences .", "h": ["linguistic rules"], "t": ["connectionist -LRB- neural -RRB- network"]}, {"label": "CONJUNCTION", "tokens": "Data are presented which show how a connectionist -LRB- neural -RRB- network trained with linguistic rules can parse both [[ expected -LRB- grammatical -RRB- sentences ]] as well as some novel << -LRB- ungrammatical or lexically ambiguous -RRB- sentences >> .", "h": ["expected -LRB- grammatical -RRB- sentences"], "t": ["-LRB- ungrammatical or lexically ambiguous -RRB- sentences"]}, {"label": "USED-FOR", "tokens": "Robust << natural language interpretation >> requires strong [[ semantic domain models ]] , fail-soft recovery heuristics , and very flexible control structures .", "h": ["semantic domain models"], "t": ["natural language interpretation"]}, {"label": "CONJUNCTION", "tokens": "Robust natural language interpretation requires strong [[ semantic domain models ]] , << fail-soft recovery heuristics >> , and very flexible control structures .", "h": ["semantic domain models"], "t": ["fail-soft recovery heuristics"]}, {"label": "USED-FOR", "tokens": "Robust << natural language interpretation >> requires strong semantic domain models , [[ fail-soft recovery heuristics ]] , and very flexible control structures .", "h": ["fail-soft recovery heuristics"], "t": ["natural language interpretation"]}, {"label": "CONJUNCTION", "tokens": "Robust natural language interpretation requires strong semantic domain models , [[ fail-soft recovery heuristics ]] , and very flexible << control structures >> .", "h": ["fail-soft recovery heuristics"], "t": ["control structures"]}, {"label": "USED-FOR", "tokens": "Robust << natural language interpretation >> requires strong semantic domain models , fail-soft recovery heuristics , and very flexible [[ control structures ]] .", "h": ["control structures"], "t": ["natural language interpretation"]}, {"label": "COMPARE", "tokens": "Although [[ single-strategy parsers ]] have met with a measure of success , a << multi-strategy approach >> is shown to provide a much higher degree of flexibility , redundancy , and ability to bring task-specific domain knowledge -LRB- in addition to general linguistic knowledge -RRB- to bear on both grammatical and ungrammatical input .", "h": ["single-strategy parsers"], "t": ["multi-strategy approach"]}, {"label": "CONJUNCTION", "tokens": "Although single-strategy parsers have met with a measure of success , a multi-strategy approach is shown to provide a much higher degree of flexibility , redundancy , and ability to bring [[ task-specific domain knowledge ]] -LRB- in addition to << general linguistic knowledge >> -RRB- to bear on both grammatical and ungrammatical input .", "h": ["task-specific domain knowledge"], "t": ["general linguistic knowledge"]}, {"label": "PART-OF", "tokens": "A << parsing algorithm >> is presented that integrates several different [[ parsing strategies ]] , with case-frame instantiation dominating .", "h": ["parsing strategies"], "t": ["parsing algorithm"]}, {"label": "HYPONYM-OF", "tokens": "A parsing algorithm is presented that integrates several different << parsing strategies >> , with [[ case-frame instantiation ]] dominating .", "h": ["case-frame instantiation"], "t": ["parsing strategies"]}, {"label": "USED-FOR", "tokens": "Each of these [[ parsing strategies ]] exploits different types of knowledge ; and their combination provides a strong framework in which to process << conjunctions >> , fragmentary input , and ungrammatical structures , as well as less exotic , grammatically correct input .", "h": ["parsing strategies"], "t": ["conjunctions"]}, {"label": "USED-FOR", "tokens": "Each of these [[ parsing strategies ]] exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , << fragmentary input >> , and ungrammatical structures , as well as less exotic , grammatically correct input .", "h": ["parsing strategies"], "t": ["fragmentary input"]}, {"label": "USED-FOR", "tokens": "Each of these [[ parsing strategies ]] exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , fragmentary input , and << ungrammatical structures >> , as well as less exotic , grammatically correct input .", "h": ["parsing strategies"], "t": ["ungrammatical structures"]}, {"label": "USED-FOR", "tokens": "Each of these [[ parsing strategies ]] exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , fragmentary input , and ungrammatical structures , as well as less << exotic , grammatically correct input >> .", "h": ["parsing strategies"], "t": ["exotic , grammatically correct input"]}, {"label": "CONJUNCTION", "tokens": "Each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process [[ conjunctions ]] , << fragmentary input >> , and ungrammatical structures , as well as less exotic , grammatically correct input .", "h": ["conjunctions"], "t": ["fragmentary input"]}, {"label": "CONJUNCTION", "tokens": "Each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , [[ fragmentary input ]] , and << ungrammatical structures >> , as well as less exotic , grammatically correct input .", "h": ["fragmentary input"], "t": ["ungrammatical structures"]}, {"label": "CONJUNCTION", "tokens": "Each of these parsing strategies exploits different types of knowledge ; and their combination provides a strong framework in which to process conjunctions , fragmentary input , and [[ ungrammatical structures ]] , as well as less << exotic , grammatically correct input >> .", "h": ["ungrammatical structures"], "t": ["exotic , grammatically correct input"]}, {"label": "USED-FOR", "tokens": "Several [[ specific heuristics ]] for handling << ungrammatical input >> are presented within this multi-strategy framework .", "h": ["specific heuristics"], "t": ["ungrammatical input"]}, {"label": "PART-OF", "tokens": "Several [[ specific heuristics ]] for handling ungrammatical input are presented within this << multi-strategy framework >> .", "h": ["specific heuristics"], "t": ["multi-strategy framework"]}, {"label": "USED-FOR", "tokens": "Recently , [[ Stacked Auto-Encoders -LRB- SAE -RRB- ]] have been successfully used for << learning imbalanced datasets >> .", "h": ["Stacked Auto-Encoders -LRB- SAE -RRB-"], "t": ["learning imbalanced datasets"]}, {"label": "USED-FOR", "tokens": "In this paper , for the first time , we propose to use a [[ Neural Network classifier ]] furnished by an SAE structure for detecting the errors made by a strong << Automatic Speech Recognition -LRB- ASR -RRB- system >> .", "h": ["Neural Network classifier"], "t": ["Automatic Speech Recognition -LRB- ASR -RRB- system"]}, {"label": "USED-FOR", "tokens": "In this paper , for the first time , we propose to use a << Neural Network classifier >> furnished by an [[ SAE structure ]] for detecting the errors made by a strong Automatic Speech Recognition -LRB- ASR -RRB- system .", "h": ["SAE structure"], "t": ["Neural Network classifier"]}, {"label": "USED-FOR", "tokens": "[[ Error detection ]] on an << automatic transcription >> provided by a '' strong '' ASR system , i.e. exhibiting a small word error rate , is difficult due to the limited number of '' positive '' examples -LRB- i.e. words erroneously recognized -RRB- available for training a binary classi-fier .", "h": ["Error detection"], "t": ["automatic transcription"]}, {"label": "USED-FOR", "tokens": "In this paper we investigate and compare different types of [[ classifiers ]] for << automatically detecting ASR errors >> , including the one based on a stacked auto-encoder architecture .", "h": ["classifiers"], "t": ["automatically detecting ASR errors"]}, {"label": "HYPONYM-OF", "tokens": "In this paper we investigate and compare different types of << classifiers >> for automatically detecting ASR errors , including the [[ one ]] based on a stacked auto-encoder architecture .", "h": ["one"], "t": ["classifiers"]}, {"label": "USED-FOR", "tokens": "In this paper we investigate and compare different types of classifiers for automatically detecting ASR errors , including the << one >> based on a [[ stacked auto-encoder architecture ]] .", "h": ["stacked auto-encoder architecture"], "t": ["one"]}, {"label": "FEATURE-OF", "tokens": "We show the effectiveness of the latter by measuring and comparing performance on the << automatic transcriptions >> of an [[ English corpus ]] collected from TED talks .", "h": ["English corpus"], "t": ["automatic transcriptions"]}, {"label": "USED-FOR", "tokens": "We show the effectiveness of the latter by measuring and comparing performance on the automatic transcriptions of an << English corpus >> collected from [[ TED talks ]] .", "h": ["TED talks"], "t": ["English corpus"]}, {"label": "EVALUATE-FOR", "tokens": "Performance of each investigated << classifier >> is evaluated both via [[ receiving operating curve ]] and via a measure , called mean absolute error , related to the quality in predicting the corresponding word error rate .", "h": ["receiving operating curve"], "t": ["classifier"]}, {"label": "CONJUNCTION", "tokens": "Performance of each investigated classifier is evaluated both via [[ receiving operating curve ]] and via a << measure >> , called mean absolute error , related to the quality in predicting the corresponding word error rate .", "h": ["receiving operating curve"], "t": ["measure"]}, {"label": "EVALUATE-FOR", "tokens": "Performance of each investigated << classifier >> is evaluated both via receiving operating curve and via a [[ measure ]] , called mean absolute error , related to the quality in predicting the corresponding word error rate .", "h": ["measure"], "t": ["classifier"]}, {"label": "USED-FOR", "tokens": "The results demonstrates that the [[ classifier ]] based on SAE detects the << ASR errors >> better than the other classification methods .", "h": ["classifier"], "t": ["ASR errors"]}, {"label": "COMPARE", "tokens": "The results demonstrates that the [[ classifier ]] based on SAE detects the ASR errors better than the other << classification methods >> .", "h": ["classifier"], "t": ["classification methods"]}, {"label": "USED-FOR", "tokens": "The results demonstrates that the << classifier >> based on [[ SAE ]] detects the ASR errors better than the other classification methods .", "h": ["SAE"], "t": ["classifier"]}, {"label": "USED-FOR", "tokens": "The results demonstrates that the classifier based on SAE detects the << ASR errors >> better than the other [[ classification methods ]] .", "h": ["classification methods"], "t": ["ASR errors"]}, {"label": "USED-FOR", "tokens": "Within the EU Network of Excellence PASCAL , a challenge was organized to design a [[ statistical machine learning algorithm ]] that segments words into the << smallest meaning-bearing units of language >> , morphemes .", "h": ["statistical machine learning algorithm"], "t": ["smallest meaning-bearing units of language"]}, {"label": "HYPONYM-OF", "tokens": "Within the EU Network of Excellence PASCAL , a challenge was organized to design a statistical machine learning algorithm that segments words into the << smallest meaning-bearing units of language >> , [[ morphemes ]] .", "h": ["morphemes"], "t": ["smallest meaning-bearing units of language"]}, {"label": "USED-FOR", "tokens": "Ideally , [[ these ]] are basic vocabulary units suitable for different << tasks >> , such as speech and text understanding , machine translation , information retrieval , and statistical language modeling .", "h": ["these"], "t": ["tasks"]}, {"label": "HYPONYM-OF", "tokens": "Ideally , these are basic vocabulary units suitable for different << tasks >> , such as [[ speech and text understanding ]] , machine translation , information retrieval , and statistical language modeling .", "h": ["speech and text understanding"], "t": ["tasks"]}, {"label": "CONJUNCTION", "tokens": "Ideally , these are basic vocabulary units suitable for different tasks , such as [[ speech and text understanding ]] , << machine translation >> , information retrieval , and statistical language modeling .", "h": ["speech and text understanding"], "t": ["machine translation"]}, {"label": "HYPONYM-OF", "tokens": "Ideally , these are basic vocabulary units suitable for different << tasks >> , such as speech and text understanding , [[ machine translation ]] , information retrieval , and statistical language modeling .", "h": ["machine translation"], "t": ["tasks"]}, {"label": "CONJUNCTION", "tokens": "Ideally , these are basic vocabulary units suitable for different tasks , such as speech and text understanding , [[ machine translation ]] , << information retrieval >> , and statistical language modeling .", "h": ["machine translation"], "t": ["information retrieval"]}, {"label": "HYPONYM-OF", "tokens": "Ideally , these are basic vocabulary units suitable for different << tasks >> , such as speech and text understanding , machine translation , [[ information retrieval ]] , and statistical language modeling .", "h": ["information retrieval"], "t": ["tasks"]}, {"label": "CONJUNCTION", "tokens": "Ideally , these are basic vocabulary units suitable for different tasks , such as speech and text understanding , machine translation , [[ information retrieval ]] , and << statistical language modeling >> .", "h": ["information retrieval"], "t": ["statistical language modeling"]}, {"label": "HYPONYM-OF", "tokens": "Ideally , these are basic vocabulary units suitable for different << tasks >> , such as speech and text understanding , machine translation , information retrieval , and [[ statistical language modeling ]] .", "h": ["statistical language modeling"], "t": ["tasks"]}, {"label": "USED-FOR", "tokens": "In this paper , we evaluate the application of these [[ segmen-tation algorithms ]] to << large vocabulary speech recognition >> using statistical n-gram language models based on the proposed word segments instead of entire words .", "h": ["segmen-tation algorithms"], "t": ["large vocabulary speech recognition"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper , we evaluate the application of these << segmen-tation algorithms >> to large vocabulary speech recognition using [[ statistical n-gram language models ]] based on the proposed word segments instead of entire words .", "h": ["statistical n-gram language models"], "t": ["segmen-tation algorithms"]}, {"label": "HYPONYM-OF", "tokens": "Experiments were done for two << ag-glutinative and morphologically rich languages >> : [[ Finnish ]] and Turk-ish .", "h": ["Finnish"], "t": ["ag-glutinative and morphologically rich languages"]}, {"label": "CONJUNCTION", "tokens": "Experiments were done for two ag-glutinative and morphologically rich languages : [[ Finnish ]] and << Turk-ish >> .", "h": ["Finnish"], "t": ["Turk-ish"]}, {"label": "HYPONYM-OF", "tokens": "Experiments were done for two << ag-glutinative and morphologically rich languages >> : Finnish and [[ Turk-ish ]] .", "h": ["Turk-ish"], "t": ["ag-glutinative and morphologically rich languages"]}, {"label": "USED-FOR", "tokens": "This paper describes a recently collected [[ spoken language corpus ]] for the << ATIS -LRB- Air Travel Information System -RRB- domain >> .", "h": ["spoken language corpus"], "t": ["ATIS -LRB- Air Travel Information System -RRB- domain"]}, {"label": "EVALUATE-FOR", "tokens": "We summarize the motivation for this effort , the goals , the implementation of a multi-site data collection paradigm , and the accomplishments of MADCOW in monitoring the collection and distribution of 12,000 utterances of [[ spontaneous speech ]] from five sites for use in a << multi-site common evaluation of speech , natural language and spoken language >> .", "h": ["spontaneous speech"], "t": ["multi-site common evaluation of speech , natural language and spoken language"]}, {"label": "USED-FOR", "tokens": "This paper proposes the [[ Hierarchical Directed Acyclic Graph -LRB- HDAG -RRB- Kernel ]] for << structured natural language data >> .", "h": ["Hierarchical Directed Acyclic Graph -LRB- HDAG -RRB- Kernel"], "t": ["structured natural language data"]}, {"label": "USED-FOR", "tokens": "We applied the proposed [[ method ]] to << question classification and sentence alignment tasks >> to evaluate its performance as a similarity measure and a kernel function .", "h": ["method"], "t": ["question classification and sentence alignment tasks"]}, {"label": "EVALUATE-FOR", "tokens": "We applied the proposed << method >> to question classification and sentence alignment tasks to evaluate its performance as a [[ similarity measure ]] and a kernel function .", "h": ["similarity measure"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "We applied the proposed method to question classification and sentence alignment tasks to evaluate its performance as a [[ similarity measure ]] and a << kernel function >> .", "h": ["similarity measure"], "t": ["kernel function"]}, {"label": "EVALUATE-FOR", "tokens": "We applied the proposed << method >> to question classification and sentence alignment tasks to evaluate its performance as a similarity measure and a [[ kernel function ]] .", "h": ["kernel function"], "t": ["method"]}, {"label": "COMPARE", "tokens": "The results of the experiments demonstrate that the [[ HDAG Kernel ]] is superior to other << kernel functions >> and baseline methods .", "h": ["HDAG Kernel"], "t": ["kernel functions"]}, {"label": "COMPARE", "tokens": "The results of the experiments demonstrate that the [[ HDAG Kernel ]] is superior to other kernel functions and << baseline methods >> .", "h": ["HDAG Kernel"], "t": ["baseline methods"]}, {"label": "CONJUNCTION", "tokens": "The results of the experiments demonstrate that the HDAG Kernel is superior to other [[ kernel functions ]] and << baseline methods >> .", "h": ["kernel functions"], "t": ["baseline methods"]}, {"label": "USED-FOR", "tokens": "We propose a solution to the challenge of the << CoNLL 2008 shared task >> that uses a [[ generative history-based latent variable model ]] to predict the most likely derivation of a synchronous dependency parser for both syntactic and semantic dependencies .", "h": ["generative history-based latent variable model"], "t": ["CoNLL 2008 shared task"]}, {"label": "USED-FOR", "tokens": "We propose a solution to the challenge of the CoNLL 2008 shared task that uses a [[ generative history-based latent variable model ]] to predict the most likely derivation of a << synchronous dependency parser >> for both syntactic and semantic dependencies .", "h": ["generative history-based latent variable model"], "t": ["synchronous dependency parser"]}, {"label": "USED-FOR", "tokens": "We propose a solution to the challenge of the CoNLL 2008 shared task that uses a generative history-based latent variable model to predict the most likely derivation of a [[ synchronous dependency parser ]] for both << syntactic and semantic dependencies >> .", "h": ["synchronous dependency parser"], "t": ["syntactic and semantic dependencies"]}, {"label": "EVALUATE-FOR", "tokens": "The submitted << model >> yields 79.1 % [[ macro-average F1 performance ]] , for the joint task , 86.9 % syntactic dependencies LAS and 71.0 % semantic dependencies F1 .", "h": ["macro-average F1 performance"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "The submitted model yields 79.1 % [[ macro-average F1 performance ]] , for the joint << task >> , 86.9 % syntactic dependencies LAS and 71.0 % semantic dependencies F1 .", "h": ["macro-average F1 performance"], "t": ["task"]}, {"label": "EVALUATE-FOR", "tokens": "The submitted model yields 79.1 % macro-average F1 performance , for the joint << task >> , 86.9 % [[ syntactic dependencies LAS ]] and 71.0 % semantic dependencies F1 .", "h": ["syntactic dependencies LAS"], "t": ["task"]}, {"label": "CONJUNCTION", "tokens": "The submitted model yields 79.1 % macro-average F1 performance , for the joint task , 86.9 % [[ syntactic dependencies LAS ]] and 71.0 % << semantic dependencies F1 >> .", "h": ["syntactic dependencies LAS"], "t": ["semantic dependencies F1"]}, {"label": "EVALUATE-FOR", "tokens": "The submitted model yields 79.1 % macro-average F1 performance , for the joint << task >> , 86.9 % syntactic dependencies LAS and 71.0 % [[ semantic dependencies F1 ]] .", "h": ["semantic dependencies F1"], "t": ["task"]}, {"label": "EVALUATE-FOR", "tokens": "A larger << model >> trained after the deadline achieves 80.5 % [[ macro-average F1 ]] , 87.6 % syntactic dependencies LAS , and 73.1 % semantic dependencies F1 .", "h": ["macro-average F1"], "t": ["model"]}, {"label": "CONJUNCTION", "tokens": "A larger model trained after the deadline achieves 80.5 % [[ macro-average F1 ]] , 87.6 % << syntactic dependencies LAS >> , and 73.1 % semantic dependencies F1 .", "h": ["macro-average F1"], "t": ["syntactic dependencies LAS"]}, {"label": "EVALUATE-FOR", "tokens": "A larger << model >> trained after the deadline achieves 80.5 % macro-average F1 , 87.6 % [[ syntactic dependencies LAS ]] , and 73.1 % semantic dependencies F1 .", "h": ["syntactic dependencies LAS"], "t": ["model"]}, {"label": "CONJUNCTION", "tokens": "A larger model trained after the deadline achieves 80.5 % macro-average F1 , 87.6 % [[ syntactic dependencies LAS ]] , and 73.1 % << semantic dependencies F1 >> .", "h": ["syntactic dependencies LAS"], "t": ["semantic dependencies F1"]}, {"label": "EVALUATE-FOR", "tokens": "A larger << model >> trained after the deadline achieves 80.5 % macro-average F1 , 87.6 % syntactic dependencies LAS , and 73.1 % [[ semantic dependencies F1 ]] .", "h": ["semantic dependencies F1"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "We present an [[ approach ]] to annotating a level of << discourse structure >> that is based on identifying discourse connectives and their arguments .", "h": ["approach"], "t": ["discourse structure"]}, {"label": "USED-FOR", "tokens": "We present an << approach >> to annotating a level of discourse structure that is based on identifying [[ discourse connectives ]] and their arguments .", "h": ["discourse connectives"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "The [[ PDTB ]] is being built directly on top of the Penn TreeBank and Propbank , thus supporting the << extraction of useful syntactic and semantic features >> and providing a richer substrate for the development and evaluation of practical algorithms .", "h": ["PDTB"], "t": ["extraction of useful syntactic and semantic features"]}, {"label": "EVALUATE-FOR", "tokens": "The [[ PDTB ]] is being built directly on top of the Penn TreeBank and Propbank , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of << practical algorithms >> .", "h": ["PDTB"], "t": ["practical algorithms"]}, {"label": "USED-FOR", "tokens": "The << PDTB >> is being built directly on top of the [[ Penn TreeBank ]] and Propbank , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms .", "h": ["Penn TreeBank"], "t": ["PDTB"]}, {"label": "CONJUNCTION", "tokens": "The PDTB is being built directly on top of the [[ Penn TreeBank ]] and << Propbank >> , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms .", "h": ["Penn TreeBank"], "t": ["Propbank"]}, {"label": "USED-FOR", "tokens": "The << PDTB >> is being built directly on top of the Penn TreeBank and [[ Propbank ]] , thus supporting the extraction of useful syntactic and semantic features and providing a richer substrate for the development and evaluation of practical algorithms .", "h": ["Propbank"], "t": ["PDTB"]}, {"label": "FEATURE-OF", "tokens": "We provide a detailed preliminary analysis of << inter-annotator agreement >> - both the [[ level of agreement ]] and the types of inter-annotator variation .", "h": ["level of agreement"], "t": ["inter-annotator agreement"]}, {"label": "CONJUNCTION", "tokens": "We provide a detailed preliminary analysis of inter-annotator agreement - both the [[ level of agreement ]] and the types of << inter-annotator variation >> .", "h": ["level of agreement"], "t": ["inter-annotator variation"]}, {"label": "FEATURE-OF", "tokens": "We provide a detailed preliminary analysis of << inter-annotator agreement >> - both the level of agreement and the types of [[ inter-annotator variation ]] .", "h": ["inter-annotator variation"], "t": ["inter-annotator agreement"]}, {"label": "USED-FOR", "tokens": "Currently , [[ N-gram models ]] are the most common and widely used models for << statistical language modeling >> .", "h": ["N-gram models"], "t": ["statistical language modeling"]}, {"label": "USED-FOR", "tokens": "In this paper , we investigated an alternative way to build language models , i.e. , using [[ artificial neural networks ]] to learn the << language model >> .", "h": ["artificial neural networks"], "t": ["language model"]}, {"label": "USED-FOR", "tokens": "Our experiment result shows that the [[ neural network ]] can learn a << language model >> that has performance even better than standard statistical methods .", "h": ["neural network"], "t": ["language model"]}, {"label": "COMPARE", "tokens": "Our experiment result shows that the [[ neural network ]] can learn a language model that has performance even better than standard << statistical methods >> .", "h": ["neural network"], "t": ["statistical methods"]}, {"label": "CONJUNCTION", "tokens": "Existing works in the field usually do not encode either the << temporal evolution >> or the [[ intensity of the observed facial displays ]] .", "h": ["intensity of the observed facial displays"], "t": ["temporal evolution"]}, {"label": "USED-FOR", "tokens": "In this paper , << intrinsic topology of multidimensional continuous facial >> affect data is first modeled by an [[ ordinal man-ifold ]] .", "h": ["ordinal man-ifold"], "t": ["intrinsic topology of multidimensional continuous facial"]}, {"label": "PART-OF", "tokens": "This [[ topology ]] is then incorporated into the << Hidden Conditional Ordinal Random Field -LRB- H-CORF -RRB- framework >> for dynamic ordinal regression by constraining H-CORF parameters to lie on the ordinal manifold .", "h": ["topology"], "t": ["Hidden Conditional Ordinal Random Field -LRB- H-CORF -RRB- framework"]}, {"label": "USED-FOR", "tokens": "This topology is then incorporated into the [[ Hidden Conditional Ordinal Random Field -LRB- H-CORF -RRB- framework ]] for << dynamic ordinal regression >> by constraining H-CORF parameters to lie on the ordinal manifold .", "h": ["Hidden Conditional Ordinal Random Field -LRB- H-CORF -RRB- framework"], "t": ["dynamic ordinal regression"]}, {"label": "USED-FOR", "tokens": "The resulting [[ model ]] attains << simultaneous dynamic recognition >> and intensity estimation of facial expressions of multiple emotions .", "h": ["model"], "t": ["simultaneous dynamic recognition"]}, {"label": "USED-FOR", "tokens": "The resulting [[ model ]] attains simultaneous dynamic recognition and << intensity estimation of facial expressions >> of multiple emotions .", "h": ["model"], "t": ["intensity estimation of facial expressions"]}, {"label": "CONJUNCTION", "tokens": "The resulting model attains [[ simultaneous dynamic recognition ]] and << intensity estimation of facial expressions >> of multiple emotions .", "h": ["simultaneous dynamic recognition"], "t": ["intensity estimation of facial expressions"]}, {"label": "EVALUATE-FOR", "tokens": "To the best of our knowledge , << the proposed method >> is the first one to achieve this on both deliberate as well as [[ spontaneous facial affect data ]] .", "h": ["spontaneous facial affect data"], "t": ["the proposed method"]}, {"label": "HYPONYM-OF", "tokens": "Recent advances in linear classification have shown that for << applications >> such as [[ document classification ]] , the training can be extremely efficient .", "h": ["document classification"], "t": ["applications"]}, {"label": "COMPARE", "tokens": "These methods can not be easily applied to [[ data ]] larger than the << memory capacity >> due to the random access to the disk .", "h": ["data"], "t": ["memory capacity"]}, {"label": "USED-FOR", "tokens": "We propose and analyze a [[ block minimization framework ]] for << data >> larger than the memory size .", "h": ["block minimization framework"], "t": ["data"]}, {"label": "COMPARE", "tokens": "We propose and analyze a block minimization framework for [[ data ]] larger than the << memory size >> .", "h": ["data"], "t": ["memory size"]}, {"label": "USED-FOR", "tokens": "We investigate two implementations of the proposed [[ framework ]] for << primal and dual SVMs >> , respectively .", "h": ["framework"], "t": ["primal and dual SVMs"]}, {"label": "EVALUATE-FOR", "tokens": "This in turn affects the [[ accuracy ]] of << word sense disambiguation -LRB- WSD -RRB- systems >> trained and applied on different domains .", "h": ["accuracy"], "t": ["word sense disambiguation -LRB- WSD -RRB- systems"]}, {"label": "USED-FOR", "tokens": "This paper presents a [[ method ]] to estimate the << sense priors of words >> drawn from a new domain , and highlights the importance of using well calibrated probabilities when performing these estimations .", "h": ["method"], "t": ["sense priors of words"]}, {"label": "FEATURE-OF", "tokens": "This paper presents a method to estimate the << sense priors of words >> drawn from a [[ new domain ]] , and highlights the importance of using well calibrated probabilities when performing these estimations .", "h": ["new domain"], "t": ["sense priors of words"]}, {"label": "USED-FOR", "tokens": "This paper presents a method to estimate the sense priors of words drawn from a new domain , and highlights the importance of using [[ well calibrated probabilities ]] when performing these << estimations >> .", "h": ["well calibrated probabilities"], "t": ["estimations"]}, {"label": "USED-FOR", "tokens": "By using [[ well calibrated probabilities ]] , we are able to estimate the << sense priors >> effectively to achieve significant improvements in WSD accuracy .", "h": ["well calibrated probabilities"], "t": ["sense priors"]}, {"label": "USED-FOR", "tokens": "<< It >> was compiled from various resources such as [[ encyclopedias ]] and dictionaries , public databases of proper names and toponyms , collocations obtained from Czech WordNet , lists of botanical and zoological terms and others .", "h": ["encyclopedias"], "t": ["It"]}, {"label": "CONJUNCTION", "tokens": "It was compiled from various resources such as [[ encyclopedias ]] and << dictionaries >> , public databases of proper names and toponyms , collocations obtained from Czech WordNet , lists of botanical and zoological terms and others .", "h": ["encyclopedias"], "t": ["dictionaries"]}, {"label": "USED-FOR", "tokens": "<< It >> was compiled from various resources such as encyclopedias and [[ dictionaries ]] , public databases of proper names and toponyms , collocations obtained from Czech WordNet , lists of botanical and zoological terms and others .", "h": ["dictionaries"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "<< It >> was compiled from various resources such as encyclopedias and dictionaries , [[ public databases of proper names and toponyms ]] , collocations obtained from Czech WordNet , lists of botanical and zoological terms and others .", "h": ["public databases of proper names and toponyms"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "<< It >> was compiled from various resources such as encyclopedias and dictionaries , public databases of proper names and toponyms , [[ collocations ]] obtained from Czech WordNet , lists of botanical and zoological terms and others .", "h": ["collocations"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "<< It >> was compiled from various resources such as encyclopedias and dictionaries , public databases of proper names and toponyms , collocations obtained from Czech WordNet , [[ lists of botanical and zoological terms ]] and others .", "h": ["lists of botanical and zoological terms"], "t": ["It"]}, {"label": "CONJUNCTION", "tokens": "It was compiled from various resources such as encyclopedias and dictionaries , public databases of proper names and toponyms , << collocations >> obtained from Czech WordNet , [[ lists of botanical and zoological terms ]] and others .", "h": ["lists of botanical and zoological terms"], "t": ["collocations"]}, {"label": "USED-FOR", "tokens": "We compare the built << MWEs database >> with the corpus data from [[ Czech National Corpus ]] -LRB- approx .", "h": ["Czech National Corpus"], "t": ["MWEs database"]}, {"label": "USED-FOR", "tokens": "To obtain a more complete list of MWEs we propose and use a [[ technique ]] exploiting the << Word Sketch Engine >> , which allows us to work with statistical parameters such as frequency of MWEs and their components as well as with the salience for the whole MWEs .", "h": ["technique"], "t": ["Word Sketch Engine"]}, {"label": "FEATURE-OF", "tokens": "To obtain a more complete list of MWEs we propose and use a technique exploiting the << Word Sketch Engine >> , which allows us to work with [[ statistical parameters ]] such as frequency of MWEs and their components as well as with the salience for the whole MWEs .", "h": ["statistical parameters"], "t": ["Word Sketch Engine"]}, {"label": "USED-FOR", "tokens": "We also discuss exploitation of the [[ database ]] for working out a more adequate << tagging >> and lemmatization .", "h": ["database"], "t": ["tagging"]}, {"label": "USED-FOR", "tokens": "We also discuss exploitation of the [[ database ]] for working out a more adequate tagging and << lemmatization >> .", "h": ["database"], "t": ["lemmatization"]}, {"label": "CONJUNCTION", "tokens": "We also discuss exploitation of the database for working out a more adequate [[ tagging ]] and << lemmatization >> .", "h": ["tagging"], "t": ["lemmatization"]}, {"label": "USED-FOR", "tokens": "The final goal is to be able to recognize [[ MWEs ]] in corpus text and lemmatize them as complete lexical units , i. e. to make << tagging >> and lemmatization more adequate .", "h": ["MWEs"], "t": ["tagging"]}, {"label": "USED-FOR", "tokens": "The final goal is to be able to recognize [[ MWEs ]] in corpus text and lemmatize them as complete lexical units , i. e. to make tagging and << lemmatization >> more adequate .", "h": ["MWEs"], "t": ["lemmatization"]}, {"label": "CONJUNCTION", "tokens": "The final goal is to be able to recognize MWEs in corpus text and lemmatize them as complete lexical units , i. e. to make [[ tagging ]] and << lemmatization >> more adequate .", "h": ["tagging"], "t": ["lemmatization"]}, {"label": "USED-FOR", "tokens": "We describe the ongoing construction of a large , [[ semantically annotated corpus ]] resource as reliable basis for the << large-scale acquisition of word-semantic information >> , e.g. the construction of domain-independent lexica .", "h": ["semantically annotated corpus"], "t": ["large-scale acquisition of word-semantic information"]}, {"label": "HYPONYM-OF", "tokens": "We describe the ongoing construction of a large , semantically annotated corpus resource as reliable basis for the << large-scale acquisition of word-semantic information >> , e.g. the [[ construction of domain-independent lexica ]] .", "h": ["construction of domain-independent lexica"], "t": ["large-scale acquisition of word-semantic information"]}, {"label": "PART-OF", "tokens": "The backbone of the annotation are [[ semantic roles ]] in the << frame semantics paradigm >> .", "h": ["semantic roles"], "t": ["frame semantics paradigm"]}, {"label": "CONJUNCTION", "tokens": "On this basis , we discuss the problems of [[ vagueness ]] and << ambiguity >> in semantic annotation .", "h": ["vagueness"], "t": ["ambiguity"]}, {"label": "FEATURE-OF", "tokens": "On this basis , we discuss the problems of [[ vagueness ]] and ambiguity in << semantic annotation >> .", "h": ["vagueness"], "t": ["semantic annotation"]}, {"label": "FEATURE-OF", "tokens": "On this basis , we discuss the problems of vagueness and [[ ambiguity ]] in << semantic annotation >> .", "h": ["ambiguity"], "t": ["semantic annotation"]}, {"label": "HYPONYM-OF", "tokens": "[[ Statistical machine translation -LRB- SMT -RRB- ]] is currently one of the hot spots in << natural language processing >> .", "h": ["Statistical machine translation -LRB- SMT -RRB-"], "t": ["natural language processing"]}, {"label": "COMPARE", "tokens": "Over the last few years dramatic improvements have been made , and a number of comparative evaluations have shown , that [[ SMT ]] gives competitive results to << rule-based translation systems >> , requiring significantly less development time .", "h": ["SMT"], "t": ["rule-based translation systems"]}, {"label": "USED-FOR", "tokens": "This is particularly important when building [[ translation systems ]] for << new language pairs >> or new domains .", "h": ["translation systems"], "t": ["new language pairs"]}, {"label": "USED-FOR", "tokens": "This is particularly important when building [[ translation systems ]] for new language pairs or << new domains >> .", "h": ["translation systems"], "t": ["new domains"]}, {"label": "CONJUNCTION", "tokens": "This is particularly important when building translation systems for [[ new language pairs ]] or << new domains >> .", "h": ["new language pairs"], "t": ["new domains"]}, {"label": "HYPONYM-OF", "tokens": "[[ STTK ]] , a << statistical machine translation tool kit >> , will be introduced and used to build a working translation system .", "h": ["STTK"], "t": ["statistical machine translation tool kit"]}, {"label": "USED-FOR", "tokens": "[[ STTK ]] , a statistical machine translation tool kit , will be introduced and used to build a working << translation system >> .", "h": ["STTK"], "t": ["translation system"]}, {"label": "USED-FOR", "tokens": "[[ STTK ]] has been developed by the presenter and co-workers over a number of years and is currently used as the basis of CMU 's << SMT system >> .", "h": ["STTK"], "t": ["SMT system"]}, {"label": "CONJUNCTION", "tokens": "[[ It ]] has also successfully been coupled with << rule-based and example based machine translation modules >> to build a multi engine machine translation system .", "h": ["It"], "t": ["rule-based and example based machine translation modules"]}, {"label": "USED-FOR", "tokens": "[[ It ]] has also successfully been coupled with rule-based and example based machine translation modules to build a << multi engine machine translation system >> .", "h": ["It"], "t": ["multi engine machine translation system"]}, {"label": "USED-FOR", "tokens": "It has also successfully been coupled with [[ rule-based and example based machine translation modules ]] to build a << multi engine machine translation system >> .", "h": ["rule-based and example based machine translation modules"], "t": ["multi engine machine translation system"]}, {"label": "USED-FOR", "tokens": "This paper presents an [[ unsupervised learning approach ]] to building a << non-English -LRB- Arabic -RRB- stemmer >> .", "h": ["unsupervised learning approach"], "t": ["non-English -LRB- Arabic -RRB- stemmer"]}, {"label": "USED-FOR", "tokens": "The << stemming model >> is based on [[ statistical machine translation ]] and it uses an English stemmer and a small -LRB- 10K sentences -RRB- parallel corpus as its sole training resources .", "h": ["statistical machine translation"], "t": ["stemming model"]}, {"label": "USED-FOR", "tokens": "The stemming model is based on statistical machine translation and << it >> uses an [[ English stemmer ]] and a small -LRB- 10K sentences -RRB- parallel corpus as its sole training resources .", "h": ["English stemmer"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "The stemming model is based on statistical machine translation and << it >> uses an English stemmer and a small -LRB- 10K sentences -RRB- [[ parallel corpus ]] as its sole training resources .", "h": ["parallel corpus"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "[[ Monolingual , unannotated text ]] can be used to further improve the << stemmer >> by allowing it to adapt to a desired domain or genre .", "h": ["Monolingual , unannotated text"], "t": ["stemmer"]}, {"label": "COMPARE", "tokens": "Our [[ resource-frugal approach ]] results in 87.5 % agreement with a state of the art , proprietary << Arabic stemmer >> built using rules , affix lists , and human annotated text , in addition to an unsupervised component .", "h": ["resource-frugal approach"], "t": ["Arabic stemmer"]}, {"label": "EVALUATE-FOR", "tokens": "Our << resource-frugal approach >> results in 87.5 % [[ agreement ]] with a state of the art , proprietary Arabic stemmer built using rules , affix lists , and human annotated text , in addition to an unsupervised component .", "h": ["agreement"], "t": ["resource-frugal approach"]}, {"label": "EVALUATE-FOR", "tokens": "Our resource-frugal approach results in 87.5 % [[ agreement ]] with a state of the art , proprietary << Arabic stemmer >> built using rules , affix lists , and human annotated text , in addition to an unsupervised component .", "h": ["agreement"], "t": ["Arabic stemmer"]}, {"label": "USED-FOR", "tokens": "Our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary << Arabic stemmer >> built using [[ rules ]] , affix lists , and human annotated text , in addition to an unsupervised component .", "h": ["rules"], "t": ["Arabic stemmer"]}, {"label": "CONJUNCTION", "tokens": "Our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary Arabic stemmer built using [[ rules ]] , << affix lists >> , and human annotated text , in addition to an unsupervised component .", "h": ["rules"], "t": ["affix lists"]}, {"label": "USED-FOR", "tokens": "Our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary << Arabic stemmer >> built using rules , [[ affix lists ]] , and human annotated text , in addition to an unsupervised component .", "h": ["affix lists"], "t": ["Arabic stemmer"]}, {"label": "CONJUNCTION", "tokens": "Our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary Arabic stemmer built using rules , [[ affix lists ]] , and << human annotated text >> , in addition to an unsupervised component .", "h": ["affix lists"], "t": ["human annotated text"]}, {"label": "USED-FOR", "tokens": "Our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary << Arabic stemmer >> built using rules , affix lists , and [[ human annotated text ]] , in addition to an unsupervised component .", "h": ["human annotated text"], "t": ["Arabic stemmer"]}, {"label": "CONJUNCTION", "tokens": "Our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary Arabic stemmer built using rules , affix lists , and [[ human annotated text ]] , in addition to an << unsupervised component >> .", "h": ["human annotated text"], "t": ["unsupervised component"]}, {"label": "USED-FOR", "tokens": "Our resource-frugal approach results in 87.5 % agreement with a state of the art , proprietary << Arabic stemmer >> built using rules , affix lists , and human annotated text , in addition to an [[ unsupervised component ]] .", "h": ["unsupervised component"], "t": ["Arabic stemmer"]}, {"label": "USED-FOR", "tokens": "<< Task-based evaluation >> using [[ Arabic information retrieval ]] indicates an improvement of 22-38 % in average precision over unstemmed text , and 96 % of the performance of the proprietary stemmer above .", "h": ["Arabic information retrieval"], "t": ["Task-based evaluation"]}, {"label": "EVALUATE-FOR", "tokens": "<< Task-based evaluation >> using Arabic information retrieval indicates an improvement of 22-38 % in [[ average precision ]] over unstemmed text , and 96 % of the performance of the proprietary stemmer above .", "h": ["average precision"], "t": ["Task-based evaluation"]}, {"label": "EVALUATE-FOR", "tokens": "Task-based evaluation using Arabic information retrieval indicates an improvement of 22-38 % in [[ average precision ]] over << unstemmed text >> , and 96 % of the performance of the proprietary stemmer above .", "h": ["average precision"], "t": ["unstemmed text"]}, {"label": "USED-FOR", "tokens": "The paper assesses the capability of an [[ HMM-based TTS system ]] to produce << German speech >> .", "h": ["HMM-based TTS system"], "t": ["German speech"]}, {"label": "USED-FOR", "tokens": "In addition , the [[ system ]] is adapted to a small set of << football announcements >> , in an exploratory attempt to synthe-sise expressive speech .", "h": ["system"], "t": ["football announcements"]}, {"label": "USED-FOR", "tokens": "In addition , the [[ system ]] is adapted to a small set of football announcements , in an exploratory attempt to synthe-sise << expressive speech >> .", "h": ["system"], "t": ["expressive speech"]}, {"label": "USED-FOR", "tokens": "We conclude that the [[ HMMs ]] are able to produce highly << intelligible neutral German speech >> , with a stable quality , and that the expressivity is partially captured in spite of the small size of the football dataset .", "h": ["HMMs"], "t": ["intelligible neutral German speech"]}, {"label": "CONJUNCTION", "tokens": "Furthermore , in contrast to the approach of Dalrymple et al. -LSB- 1991 -RSB- , the treatment directly encodes the intuitive distinction between [[ full NPs ]] and the << referential elements >> that corefer with them through what we term role linking .", "h": ["full NPs"], "t": ["referential elements"]}, {"label": "USED-FOR", "tokens": "Finally , the [[ analysis ]] extends directly to other << discourse copying phenomena >> .", "h": ["analysis"], "t": ["discourse copying phenomena"]}, {"label": "PART-OF", "tokens": "How to obtain [[ hierarchical relations ]] -LRB- e.g. superordinate - hyponym relation , synonym relation -RRB- is one of the most important problems for << thesaurus construction >> .", "h": ["hierarchical relations"], "t": ["thesaurus construction"]}, {"label": "HYPONYM-OF", "tokens": "How to obtain << hierarchical relations >> -LRB- e.g. [[ superordinate - hyponym relation ]] , synonym relation -RRB- is one of the most important problems for thesaurus construction .", "h": ["superordinate - hyponym relation"], "t": ["hierarchical relations"]}, {"label": "CONJUNCTION", "tokens": "How to obtain hierarchical relations -LRB- e.g. [[ superordinate - hyponym relation ]] , << synonym relation >> -RRB- is one of the most important problems for thesaurus construction .", "h": ["superordinate - hyponym relation"], "t": ["synonym relation"]}, {"label": "HYPONYM-OF", "tokens": "How to obtain << hierarchical relations >> -LRB- e.g. superordinate - hyponym relation , [[ synonym relation ]] -RRB- is one of the most important problems for thesaurus construction .", "h": ["synonym relation"], "t": ["hierarchical relations"]}, {"label": "USED-FOR", "tokens": "A pilot system for extracting these << relations >> automatically from an ordinary [[ Japanese language dictionary ]] -LRB- Shinmeikai Kokugojiten , published by Sansei-do , in machine readable form -RRB- is given .", "h": ["Japanese language dictionary"], "t": ["relations"]}, {"label": "USED-FOR", "tokens": "The << features >> of the [[ definition sentences ]] in the dictionary , the mechanical extraction of the hierarchical relations and the estimation of the results are discussed .", "h": ["definition sentences"], "t": ["features"]}, {"label": "PART-OF", "tokens": "The features of the [[ definition sentences ]] in the << dictionary >> , the mechanical extraction of the hierarchical relations and the estimation of the results are discussed .", "h": ["definition sentences"], "t": ["dictionary"]}, {"label": "EVALUATE-FOR", "tokens": "This is evident most compellingly by the very low [[ recognition rate ]] of all existing << face recognition systems >> when applied to live CCTV camera input .", "h": ["recognition rate"], "t": ["face recognition systems"]}, {"label": "USED-FOR", "tokens": "This is evident most compellingly by the very low recognition rate of all existing << face recognition systems >> when applied to [[ live CCTV camera input ]] .", "h": ["live CCTV camera input"], "t": ["face recognition systems"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a [[ Bayesian framework ]] to perform multi-modal -LRB- such as variations in viewpoint and illumination -RRB- << face image super-resolution >> for recognition in tensor space .", "h": ["Bayesian framework"], "t": ["face image super-resolution"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we present a Bayesian framework to perform multi-modal -LRB- such as variations in [[ viewpoint ]] and << illumination >> -RRB- face image super-resolution for recognition in tensor space .", "h": ["viewpoint"], "t": ["illumination"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a Bayesian framework to perform multi-modal -LRB- such as variations in viewpoint and illumination -RRB- [[ face image super-resolution ]] for << recognition >> in tensor space .", "h": ["face image super-resolution"], "t": ["recognition"]}, {"label": "FEATURE-OF", "tokens": "In this paper , we present a Bayesian framework to perform multi-modal -LRB- such as variations in viewpoint and illumination -RRB- face image super-resolution for << recognition >> in [[ tensor space ]] .", "h": ["tensor space"], "t": ["recognition"]}, {"label": "USED-FOR", "tokens": "Given a [[ single modal low-resolution face image ]] , we benefit from the multiple factor interactions of training tensor , and super-resolve its << high-resolution reconstructions >> across different modalities for face recognition .", "h": ["single modal low-resolution face image"], "t": ["high-resolution reconstructions"]}, {"label": "USED-FOR", "tokens": "Given a single modal low-resolution face image , we benefit from the [[ multiple factor interactions of training tensor ]] , and super-resolve its << high-resolution reconstructions >> across different modalities for face recognition .", "h": ["multiple factor interactions of training tensor"], "t": ["high-resolution reconstructions"]}, {"label": "USED-FOR", "tokens": "Given a single modal low-resolution face image , we benefit from the multiple factor interactions of training tensor , and super-resolve its [[ high-resolution reconstructions ]] across different modalities for << face recognition >> .", "h": ["high-resolution reconstructions"], "t": ["face recognition"]}, {"label": "FEATURE-OF", "tokens": "Given a single modal low-resolution face image , we benefit from the multiple factor interactions of training tensor , and super-resolve its << high-resolution reconstructions >> across different [[ modalities ]] for face recognition .", "h": ["modalities"], "t": ["high-resolution reconstructions"]}, {"label": "HYPONYM-OF", "tokens": "Instead of performing << pixel-domain super-resolution and recognition >> independently as two separate sequential processes , we integrate the tasks of [[ super-resolution ]] and recognition by directly computing a maximum likelihood identity parameter vector in high-resolution tensor space for recognition .", "h": ["super-resolution"], "t": ["pixel-domain super-resolution and recognition"]}, {"label": "CONJUNCTION", "tokens": "Instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of [[ super-resolution ]] and << recognition >> by directly computing a maximum likelihood identity parameter vector in high-resolution tensor space for recognition .", "h": ["super-resolution"], "t": ["recognition"]}, {"label": "HYPONYM-OF", "tokens": "Instead of performing << pixel-domain super-resolution and recognition >> independently as two separate sequential processes , we integrate the tasks of super-resolution and [[ recognition ]] by directly computing a maximum likelihood identity parameter vector in high-resolution tensor space for recognition .", "h": ["recognition"], "t": ["pixel-domain super-resolution and recognition"]}, {"label": "USED-FOR", "tokens": "Instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of << super-resolution >> and recognition by directly computing a [[ maximum likelihood identity parameter vector ]] in high-resolution tensor space for recognition .", "h": ["maximum likelihood identity parameter vector"], "t": ["super-resolution"]}, {"label": "USED-FOR", "tokens": "Instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of super-resolution and << recognition >> by directly computing a [[ maximum likelihood identity parameter vector ]] in high-resolution tensor space for recognition .", "h": ["maximum likelihood identity parameter vector"], "t": ["recognition"]}, {"label": "USED-FOR", "tokens": "Instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of super-resolution and recognition by directly computing a [[ maximum likelihood identity parameter vector ]] in high-resolution tensor space for << recognition >> .", "h": ["maximum likelihood identity parameter vector"], "t": ["recognition"]}, {"label": "FEATURE-OF", "tokens": "Instead of performing pixel-domain super-resolution and recognition independently as two separate sequential processes , we integrate the tasks of super-resolution and recognition by directly computing a << maximum likelihood identity parameter vector >> in [[ high-resolution tensor space ]] for recognition .", "h": ["high-resolution tensor space"], "t": ["maximum likelihood identity parameter vector"]}, {"label": "USED-FOR", "tokens": "We show results from << multi-modal super-resolution and face recognition >> experiments across different imaging modalities , using [[ low-resolution images ]] as testing inputs and demonstrate improved recognition rates over standard tensorface and eigenface representations .", "h": ["low-resolution images"], "t": ["multi-modal super-resolution and face recognition"]}, {"label": "EVALUATE-FOR", "tokens": "We show results from << multi-modal super-resolution and face recognition >> experiments across different imaging modalities , using low-resolution images as testing inputs and demonstrate improved [[ recognition rates ]] over standard tensorface and eigenface representations .", "h": ["recognition rates"], "t": ["multi-modal super-resolution and face recognition"]}, {"label": "EVALUATE-FOR", "tokens": "We show results from multi-modal super-resolution and face recognition experiments across different imaging modalities , using low-resolution images as testing inputs and demonstrate improved [[ recognition rates ]] over standard << tensorface and eigenface representations >> .", "h": ["recognition rates"], "t": ["tensorface and eigenface representations"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe a [[ phrase-based unigram model ]] for << statistical machine translation >> that uses a much simpler set of model parameters than similar phrase-based models .", "h": ["phrase-based unigram model"], "t": ["statistical machine translation"]}, {"label": "COMPARE", "tokens": "In this paper , we describe a [[ phrase-based unigram model ]] for statistical machine translation that uses a much simpler set of model parameters than similar << phrase-based models >> .", "h": ["phrase-based unigram model"], "t": ["phrase-based models"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe a << phrase-based unigram model >> for statistical machine translation that uses a much simpler set of [[ model parameters ]] than similar phrase-based models .", "h": ["model parameters"], "t": ["phrase-based unigram model"]}, {"label": "USED-FOR", "tokens": "During << decoding >> , we use a [[ block unigram model ]] and a word-based trigram language model .", "h": ["block unigram model"], "t": ["decoding"]}, {"label": "USED-FOR", "tokens": "During << decoding >> , we use a block unigram model and a [[ word-based trigram language model ]] .", "h": ["word-based trigram language model"], "t": ["decoding"]}, {"label": "CONJUNCTION", "tokens": "During decoding , we use a << block unigram model >> and a [[ word-based trigram language model ]] .", "h": ["word-based trigram language model"], "t": ["block unigram model"]}, {"label": "USED-FOR", "tokens": "During training , the << blocks >> are learned from [[ source interval projections ]] using an underlying word alignment .", "h": ["source interval projections"], "t": ["blocks"]}, {"label": "USED-FOR", "tokens": "During training , the blocks are learned from << source interval projections >> using an underlying [[ word alignment ]] .", "h": ["word alignment"], "t": ["source interval projections"]}, {"label": "USED-FOR", "tokens": "We show experimental results on << block selection criteria >> based on [[ unigram counts ]] and phrase length .", "h": ["unigram counts"], "t": ["block selection criteria"]}, {"label": "CONJUNCTION", "tokens": "We show experimental results on block selection criteria based on [[ unigram counts ]] and << phrase length >> .", "h": ["unigram counts"], "t": ["phrase length"]}, {"label": "USED-FOR", "tokens": "We show experimental results on << block selection criteria >> based on unigram counts and [[ phrase length ]] .", "h": ["phrase length"], "t": ["block selection criteria"]}, {"label": "USED-FOR", "tokens": "This paper develops a new [[ approach ]] for extremely << fast detection >> in domains where the distribution of positive and negative examples is highly skewed -LRB- e.g. face detection or database retrieval -RRB- .", "h": ["approach"], "t": ["fast detection"]}, {"label": "CONJUNCTION", "tokens": "This paper develops a new approach for extremely fast detection in domains where the distribution of positive and negative examples is highly skewed -LRB- e.g. [[ face detection ]] or << database retrieval >> -RRB- .", "h": ["face detection"], "t": ["database retrieval"]}, {"label": "USED-FOR", "tokens": "In such domains a [[ cascade of simple classifiers ]] each trained to achieve high detection rates and modest false positive rates can yield a final << detector >> with many desirable features : including high detection rates , very low false positive rates , and fast performance .", "h": ["cascade of simple classifiers"], "t": ["detector"]}, {"label": "EVALUATE-FOR", "tokens": "In such domains a cascade of simple << classifiers >> each trained to achieve high [[ detection rates ]] and modest false positive rates can yield a final detector with many desirable features : including high detection rates , very low false positive rates , and fast performance .", "h": ["detection rates"], "t": ["classifiers"]}, {"label": "CONJUNCTION", "tokens": "In such domains a cascade of simple classifiers each trained to achieve high [[ detection rates ]] and << modest false positive rates >> can yield a final detector with many desirable features : including high detection rates , very low false positive rates , and fast performance .", "h": ["detection rates"], "t": ["modest false positive rates"]}, {"label": "EVALUATE-FOR", "tokens": "In such domains a cascade of simple << classifiers >> each trained to achieve high detection rates and [[ modest false positive rates ]] can yield a final detector with many desirable features : including high detection rates , very low false positive rates , and fast performance .", "h": ["modest false positive rates"], "t": ["classifiers"]}, {"label": "FEATURE-OF", "tokens": "In such domains a cascade of simple classifiers each trained to achieve high detection rates and modest false positive rates can yield a final << detector >> with many desirable [[ features ]] : including high detection rates , very low false positive rates , and fast performance .", "h": ["features"], "t": ["detector"]}, {"label": "COMPARE", "tokens": "Achieving extremely high [[ detection rates ]] , rather than << low error >> , is not a task typically addressed by machine learning algorithms .", "h": ["detection rates"], "t": ["low error"]}, {"label": "USED-FOR", "tokens": "We propose a new variant of [[ AdaBoost ]] as a mechanism for training the simple << classifiers >> used in the cascade .", "h": ["AdaBoost"], "t": ["classifiers"]}, {"label": "USED-FOR", "tokens": "We propose a new variant of AdaBoost as a mechanism for training the simple [[ classifiers ]] used in the << cascade >> .", "h": ["classifiers"], "t": ["cascade"]}, {"label": "USED-FOR", "tokens": "Experimental results in the domain of << face detection >> show the [[ training algorithm ]] yields significant improvements in performance over conventional AdaBoost .", "h": ["training algorithm"], "t": ["face detection"]}, {"label": "USED-FOR", "tokens": "Experimental results in the domain of << face detection >> show the training algorithm yields significant improvements in performance over conventional [[ AdaBoost ]] .", "h": ["AdaBoost"], "t": ["face detection"]}, {"label": "COMPARE", "tokens": "Experimental results in the domain of face detection show the << training algorithm >> yields significant improvements in performance over conventional [[ AdaBoost ]] .", "h": ["AdaBoost"], "t": ["training algorithm"]}, {"label": "CONJUNCTION", "tokens": "The final face detection system can process 15 frames per second , achieves over 90 % [[ detection ]] , and a << false positive rate >> of 1 in a 1,000,000 .", "h": ["detection"], "t": ["false positive rate"]}, {"label": "USED-FOR", "tokens": "This paper proposes a [[ method ]] for learning << joint embed-dings of images and text >> using a two-branch neural network with multiple layers of linear projections followed by nonlinearities .", "h": ["method"], "t": ["joint embed-dings of images and text"]}, {"label": "USED-FOR", "tokens": "This paper proposes a << method >> for learning joint embed-dings of images and text using a [[ two-branch neural network ]] with multiple layers of linear projections followed by nonlinearities .", "h": ["two-branch neural network"], "t": ["method"]}, {"label": "PART-OF", "tokens": "This paper proposes a method for learning joint embed-dings of images and text using a << two-branch neural network >> with [[ multiple layers of linear projections ]] followed by nonlinearities .", "h": ["multiple layers of linear projections"], "t": ["two-branch neural network"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes a method for learning joint embed-dings of images and text using a two-branch neural network with [[ multiple layers of linear projections ]] followed by << nonlinearities >> .", "h": ["multiple layers of linear projections"], "t": ["nonlinearities"]}, {"label": "PART-OF", "tokens": "This paper proposes a method for learning joint embed-dings of images and text using a << two-branch neural network >> with multiple layers of linear projections followed by [[ nonlinearities ]] .", "h": ["nonlinearities"], "t": ["two-branch neural network"]}, {"label": "USED-FOR", "tokens": "The << network >> is trained using a [[ large-margin objective ]] that combines cross-view ranking constraints with within-view neighborhood structure preservation constraints inspired by metric learning literature .", "h": ["large-margin objective"], "t": ["network"]}, {"label": "FEATURE-OF", "tokens": "The network is trained using a << large-margin objective >> that combines [[ cross-view ranking constraints ]] with within-view neighborhood structure preservation constraints inspired by metric learning literature .", "h": ["cross-view ranking constraints"], "t": ["large-margin objective"]}, {"label": "CONJUNCTION", "tokens": "The network is trained using a large-margin objective that combines [[ cross-view ranking constraints ]] with << within-view neighborhood structure preservation constraints >> inspired by metric learning literature .", "h": ["cross-view ranking constraints"], "t": ["within-view neighborhood structure preservation constraints"]}, {"label": "FEATURE-OF", "tokens": "The network is trained using a << large-margin objective >> that combines cross-view ranking constraints with [[ within-view neighborhood structure preservation constraints ]] inspired by metric learning literature .", "h": ["within-view neighborhood structure preservation constraints"], "t": ["large-margin objective"]}, {"label": "EVALUATE-FOR", "tokens": "Extensive experiments show that our << approach >> gains significant improvements in [[ accuracy ]] for image-to-text and text-to-image retrieval .", "h": ["accuracy"], "t": ["approach"]}, {"label": "EVALUATE-FOR", "tokens": "Extensive experiments show that our << approach >> gains significant improvements in accuracy for [[ image-to-text and text-to-image retrieval ]] .", "h": ["image-to-text and text-to-image retrieval"], "t": ["approach"]}, {"label": "EVALUATE-FOR", "tokens": "Our << method >> achieves new state-of-the-art results on the [[ Flickr30K and MSCOCO image-sentence datasets ]] and shows promise on the new task of phrase lo-calization on the Flickr30K Entities dataset .", "h": ["Flickr30K and MSCOCO image-sentence datasets"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "Our << method >> achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of [[ phrase lo-calization ]] on the Flickr30K Entities dataset .", "h": ["phrase lo-calization"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "Our method achieves new state-of-the-art results on the Flickr30K and MSCOCO image-sentence datasets and shows promise on the new task of << phrase lo-calization >> on the [[ Flickr30K Entities dataset ]] .", "h": ["Flickr30K Entities dataset"], "t": ["phrase lo-calization"]}, {"label": "USED-FOR", "tokens": "We investigate that claim by adopting a simple [[ MT-based paraphrasing technique ]] and evaluating << QA system >> performance on paraphrased questions .", "h": ["MT-based paraphrasing technique"], "t": ["QA system"]}, {"label": "EVALUATE-FOR", "tokens": "We investigate that claim by adopting a simple MT-based paraphrasing technique and evaluating << QA system >> performance on [[ paraphrased questions ]] .", "h": ["paraphrased questions"], "t": ["QA system"]}, {"label": "USED-FOR", "tokens": "The << TAP-XL Automated Analyst 's Assistant >> is an application designed to help an English - speaking analyst write a topical report , culling information from a large inflow of [[ multilingual , multimedia data ]] .", "h": ["multilingual , multimedia data"], "t": ["TAP-XL Automated Analyst 's Assistant"]}, {"label": "USED-FOR", "tokens": "<< It >> gives users the ability to spend their time finding more data relevant to their task , and gives them translingual reach into other languages by leveraging [[ human language technology ]] .", "h": ["human language technology"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "This paper discusses the application of [[ Unification Categorial Grammar -LRB- UCG -RRB- ]] to the framework of << Isomorphic Grammars >> for Machine Translation pioneered by Landsbergen .", "h": ["Unification Categorial Grammar -LRB- UCG -RRB-"], "t": ["Isomorphic Grammars"]}, {"label": "USED-FOR", "tokens": "This paper discusses the application of Unification Categorial Grammar -LRB- UCG -RRB- to the framework of [[ Isomorphic Grammars ]] for << Machine Translation >> pioneered by Landsbergen .", "h": ["Isomorphic Grammars"], "t": ["Machine Translation"]}, {"label": "USED-FOR", "tokens": "The [[ Isomorphic Grammars approach ]] to << MT >> involves developing the grammars of the Source and Target languages in parallel , in order to ensure that SL and TL expressions which stand in the translation relation have isomorphic derivations .", "h": ["Isomorphic Grammars approach"], "t": ["MT"]}, {"label": "USED-FOR", "tokens": "After introducing this [[ approach ]] to << MT system design >> , and the basics of monolingual UCG , we will show how the two can be integrated , and present an example from an implemented bi-directional English-Spanish fragment .", "h": ["approach"], "t": ["MT system design"]}, {"label": "USED-FOR", "tokens": "After introducing this [[ approach ]] to MT system design , and the basics of << monolingual UCG >> , we will show how the two can be integrated , and present an example from an implemented bi-directional English-Spanish fragment .", "h": ["approach"], "t": ["monolingual UCG"]}, {"label": "CONJUNCTION", "tokens": "After introducing this approach to [[ MT system design ]] , and the basics of << monolingual UCG >> , we will show how the two can be integrated , and present an example from an implemented bi-directional English-Spanish fragment .", "h": ["MT system design"], "t": ["monolingual UCG"]}, {"label": "HYPONYM-OF", "tokens": "After introducing this approach to [[ MT system design ]] , and the basics of monolingual UCG , we will show how the << two >> can be integrated , and present an example from an implemented bi-directional English-Spanish fragment .", "h": ["MT system design"], "t": ["two"]}, {"label": "HYPONYM-OF", "tokens": "After introducing this approach to MT system design , and the basics of [[ monolingual UCG ]] , we will show how the << two >> can be integrated , and present an example from an implemented bi-directional English-Spanish fragment .", "h": ["monolingual UCG"], "t": ["two"]}, {"label": "PART-OF", "tokens": "In the << security domain >> a key problem is [[ identifying rare behaviours of interest ]] .", "h": ["identifying rare behaviours of interest"], "t": ["security domain"]}, {"label": "USED-FOR", "tokens": "[[ Training examples ]] for these << behaviours >> may or may not exist , and if they do exist there will be few examples , quite probably one .", "h": ["Training examples"], "t": ["behaviours"]}, {"label": "USED-FOR", "tokens": "We present a novel [[ weakly supervised algorithm ]] that can detect << behaviours >> that either have never before been seen or for which there are few examples .", "h": ["weakly supervised algorithm"], "t": ["behaviours"]}, {"label": "USED-FOR", "tokens": "[[ Global context ]] is modelled , allowing the << detection of abnormal behaviours >> that in isolation appear normal .", "h": ["Global context"], "t": ["detection of abnormal behaviours"]}, {"label": "USED-FOR", "tokens": "We have developed a [[ computational model ]] of the process of describing the layout of an apartment or house , a much-studied << discourse task >> first characterized linguistically by Linde -LRB- 1974 -RRB- .", "h": ["computational model"], "t": ["discourse task"]}, {"label": "PART-OF", "tokens": "The [[ model ]] is embodied in a << program >> , APT , that can reproduce segments of actual tape-recorded descriptions , using organizational and discourse strategies derived through analysis of our corpus .", "h": ["model"], "t": ["program"]}, {"label": "USED-FOR", "tokens": "The model is embodied in a program , << APT >> , that can reproduce segments of actual tape-recorded descriptions , using [[ organizational and discourse strategies ]] derived through analysis of our corpus .", "h": ["organizational and discourse strategies"], "t": ["APT"]}, {"label": "USED-FOR", "tokens": "This paper proposes a practical [[ approach ]] employing n-gram models and error-correction rules for << Thai key prediction >> and Thai-English language identification .", "h": ["approach"], "t": ["Thai key prediction"]}, {"label": "USED-FOR", "tokens": "This paper proposes a practical [[ approach ]] employing n-gram models and error-correction rules for Thai key prediction and << Thai-English language identification >> .", "h": ["approach"], "t": ["Thai-English language identification"]}, {"label": "USED-FOR", "tokens": "This paper proposes a practical << approach >> employing [[ n-gram models ]] and error-correction rules for Thai key prediction and Thai-English language identification .", "h": ["n-gram models"], "t": ["approach"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes a practical approach employing [[ n-gram models ]] and << error-correction rules >> for Thai key prediction and Thai-English language identification .", "h": ["n-gram models"], "t": ["error-correction rules"]}, {"label": "USED-FOR", "tokens": "This paper proposes a practical << approach >> employing n-gram models and [[ error-correction rules ]] for Thai key prediction and Thai-English language identification .", "h": ["error-correction rules"], "t": ["approach"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes a practical approach employing n-gram models and error-correction rules for [[ Thai key prediction ]] and << Thai-English language identification >> .", "h": ["Thai key prediction"], "t": ["Thai-English language identification"]}, {"label": "USED-FOR", "tokens": "The paper also proposes << rule-reduction algorithm >> applying [[ mutual information ]] to reduce the error-correction rules .", "h": ["mutual information"], "t": ["rule-reduction algorithm"]}, {"label": "USED-FOR", "tokens": "The paper also proposes rule-reduction algorithm applying [[ mutual information ]] to reduce the << error-correction rules >> .", "h": ["mutual information"], "t": ["error-correction rules"]}, {"label": "USED-FOR", "tokens": "Our [[ algorithm ]] reported more than 99 % accuracy in both << language identification >> and key prediction .", "h": ["algorithm"], "t": ["language identification"]}, {"label": "USED-FOR", "tokens": "Our [[ algorithm ]] reported more than 99 % accuracy in both language identification and << key prediction >> .", "h": ["algorithm"], "t": ["key prediction"]}, {"label": "EVALUATE-FOR", "tokens": "Our << algorithm >> reported more than 99 % [[ accuracy ]] in both language identification and key prediction .", "h": ["accuracy"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "This paper concerns the [[ discourse understanding process ]] in << spoken dialogue systems >> .", "h": ["discourse understanding process"], "t": ["spoken dialogue systems"]}, {"label": "USED-FOR", "tokens": "This process enables the [[ system ]] to understand << user utterances >> based on the context of a dialogue .", "h": ["system"], "t": ["user utterances"]}, {"label": "USED-FOR", "tokens": "This paper proposes a [[ method ]] for resolving this << ambiguity >> based on statistical information obtained from dialogue corpora .", "h": ["method"], "t": ["ambiguity"]}, {"label": "USED-FOR", "tokens": "This paper proposes a << method >> for resolving this ambiguity based on [[ statistical information ]] obtained from dialogue corpora .", "h": ["statistical information"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "This paper proposes a method for resolving this ambiguity based on << statistical information >> obtained from [[ dialogue corpora ]] .", "h": ["dialogue corpora"], "t": ["statistical information"]}, {"label": "USED-FOR", "tokens": "Unlike conventional << methods >> that use [[ hand-crafted rules ]] , the proposed method enables easy design of the discourse understanding process .", "h": ["hand-crafted rules"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "Experiment results have shown that a [[ system ]] that exploits the proposed << method >> performs sufficiently and that holding multiple candidates for understanding results is effective .", "h": ["system"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "We consider the problem of << question-focused sentence retrieval >> from complex [[ news articles ]] describing multi-event stories published over time .", "h": ["news articles"], "t": ["question-focused sentence retrieval"]}, {"label": "FEATURE-OF", "tokens": "We consider the problem of question-focused sentence retrieval from complex << news articles >> describing [[ multi-event stories ]] published over time .", "h": ["multi-event stories"], "t": ["news articles"]}, {"label": "USED-FOR", "tokens": "To address the << sentence retrieval problem >> , we apply a [[ stochastic , graph-based method ]] for comparing the relative importance of the textual units , which was previously used successfully for generic summarization .", "h": ["stochastic , graph-based method"], "t": ["sentence retrieval problem"]}, {"label": "USED-FOR", "tokens": "To address the sentence retrieval problem , we apply a [[ stochastic , graph-based method ]] for comparing the relative importance of the textual units , which was previously used successfully for << generic summarization >> .", "h": ["stochastic , graph-based method"], "t": ["generic summarization"]}, {"label": "COMPARE", "tokens": "Currently , we present a topic-sensitive version of our method and hypothesize that << it >> can outperform a competitive [[ baseline ]] , which compares the similarity of each sentence to the input question via IDF-weighted word overlap .", "h": ["baseline"], "t": ["it"]}, {"label": "COMPARE", "tokens": "In our experiments , the [[ method ]] achieves a TRDR score that is significantly higher than that of the << baseline >> .", "h": ["method"], "t": ["baseline"]}, {"label": "EVALUATE-FOR", "tokens": "In our experiments , the << method >> achieves a [[ TRDR score ]] that is significantly higher than that of the baseline .", "h": ["TRDR score"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "In our experiments , the method achieves a [[ TRDR score ]] that is significantly higher than that of the << baseline >> .", "h": ["TRDR score"], "t": ["baseline"]}, {"label": "USED-FOR", "tokens": "This paper proposes that << sentence analysis >> should be treated as [[ defeasible reasoning ]] , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .", "h": ["defeasible reasoning"], "t": ["sentence analysis"]}, {"label": "USED-FOR", "tokens": "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a [[ treatment ]] for << Japanese sentence analyses >> using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .", "h": ["treatment"], "t": ["Japanese sentence analyses"]}, {"label": "USED-FOR", "tokens": "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for << Japanese sentence analyses >> using an [[ argumentation system ]] by Konolige , which is a formalization of defeasible reasoning , that includes arguments and defeat rules that capture defeasibility .", "h": ["argumentation system"], "t": ["Japanese sentence analyses"]}, {"label": "PART-OF", "tokens": "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a << formalization of defeasible reasoning >> , that includes [[ arguments ]] and defeat rules that capture defeasibility .", "h": ["arguments"], "t": ["formalization of defeasible reasoning"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes [[ arguments ]] and << defeat rules >> that capture defeasibility .", "h": ["arguments"], "t": ["defeat rules"]}, {"label": "PART-OF", "tokens": "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a << formalization of defeasible reasoning >> , that includes arguments and [[ defeat rules ]] that capture defeasibility .", "h": ["defeat rules"], "t": ["formalization of defeasible reasoning"]}, {"label": "FEATURE-OF", "tokens": "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes << arguments >> and defeat rules that capture [[ defeasibility ]] .", "h": ["defeasibility"], "t": ["arguments"]}, {"label": "FEATURE-OF", "tokens": "This paper proposes that sentence analysis should be treated as defeasible reasoning , and presents such a treatment for Japanese sentence analyses using an argumentation system by Konolige , which is a formalization of defeasible reasoning , that includes arguments and << defeat rules >> that capture [[ defeasibility ]] .", "h": ["defeasibility"], "t": ["defeat rules"]}, {"label": "USED-FOR", "tokens": "It gives an overview of [[ methods ]] used for << visual speech animation >> , parameterization of a human face and a tongue , necessary data sources and a synthesis method .", "h": ["methods"], "t": ["visual speech animation"]}, {"label": "USED-FOR", "tokens": "A [[ 3D animation model ]] is used for a << pseudo-muscular animation schema >> to create such animation of visual speech which is usable for a lipreading .", "h": ["3D animation model"], "t": ["pseudo-muscular animation schema"]}, {"label": "USED-FOR", "tokens": "A 3D animation model is used for a [[ pseudo-muscular animation schema ]] to create such << animation of visual speech >> which is usable for a lipreading .", "h": ["pseudo-muscular animation schema"], "t": ["animation of visual speech"]}, {"label": "USED-FOR", "tokens": "A 3D animation model is used for a pseudo-muscular animation schema to create such [[ animation of visual speech ]] which is usable for a << lipreading >> .", "h": ["animation of visual speech"], "t": ["lipreading"]}, {"label": "USED-FOR", "tokens": "Furthermore , a problem of [[ forming articulatory trajectories ]] is formulated to solve << labial coarticulation effects >> .", "h": ["forming articulatory trajectories"], "t": ["labial coarticulation effects"]}, {"label": "USED-FOR", "tokens": "[[ It ]] is used for the << synthesis method >> based on a selection of articulatory targets and interpolation technique .", "h": ["It"], "t": ["synthesis method"]}, {"label": "USED-FOR", "tokens": "<< It >> is used for the synthesis method based on a [[ selection of articulatory targets ]] and interpolation technique .", "h": ["selection of articulatory targets"], "t": ["It"]}, {"label": "CONJUNCTION", "tokens": "It is used for the synthesis method based on a [[ selection of articulatory targets ]] and << interpolation technique >> .", "h": ["selection of articulatory targets"], "t": ["interpolation technique"]}, {"label": "USED-FOR", "tokens": "<< It >> is used for the synthesis method based on a selection of articulatory targets and [[ interpolation technique ]] .", "h": ["interpolation technique"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "However , our experience with TACITUS ; especially in the MUC-3 evaluation , has shown that principled [[ techniques ]] for << syntactic and pragmatic analysis >> can be bolstered with methods for achieving robustness .", "h": ["techniques"], "t": ["syntactic and pragmatic analysis"]}, {"label": "EVALUATE-FOR", "tokens": "However , our experience with TACITUS ; especially in the MUC-3 evaluation , has shown that principled techniques for syntactic and pragmatic analysis can be bolstered with << methods >> for achieving [[ robustness ]] .", "h": ["robustness"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "We describe [[ three techniques ]] for making << syntactic analysis >> more robust -- an agenda-based scheduling parser , a recovery technique for failed parses , and a new technique called terminal substring parsing .", "h": ["three techniques"], "t": ["syntactic analysis"]}, {"label": "HYPONYM-OF", "tokens": "We describe << three techniques >> for making syntactic analysis more robust -- an [[ agenda-based scheduling parser ]] , a recovery technique for failed parses , and a new technique called terminal substring parsing .", "h": ["agenda-based scheduling parser"], "t": ["three techniques"]}, {"label": "CONJUNCTION", "tokens": "We describe three techniques for making syntactic analysis more robust -- an [[ agenda-based scheduling parser ]] , a << recovery technique >> for failed parses , and a new technique called terminal substring parsing .", "h": ["agenda-based scheduling parser"], "t": ["recovery technique"]}, {"label": "HYPONYM-OF", "tokens": "We describe << three techniques >> for making syntactic analysis more robust -- an agenda-based scheduling parser , a [[ recovery technique ]] for failed parses , and a new technique called terminal substring parsing .", "h": ["recovery technique"], "t": ["three techniques"]}, {"label": "USED-FOR", "tokens": "We describe three techniques for making << syntactic analysis >> more robust -- an agenda-based scheduling parser , a [[ recovery technique ]] for failed parses , and a new technique called terminal substring parsing .", "h": ["recovery technique"], "t": ["syntactic analysis"]}, {"label": "USED-FOR", "tokens": "We describe three techniques for making syntactic analysis more robust -- an agenda-based scheduling parser , a [[ recovery technique ]] for << failed parses >> , and a new technique called terminal substring parsing .", "h": ["recovery technique"], "t": ["failed parses"]}, {"label": "CONJUNCTION", "tokens": "We describe three techniques for making syntactic analysis more robust -- an agenda-based scheduling parser , a [[ recovery technique ]] for failed parses , and a new << technique >> called terminal substring parsing .", "h": ["recovery technique"], "t": ["technique"]}, {"label": "HYPONYM-OF", "tokens": "We describe << three techniques >> for making syntactic analysis more robust -- an agenda-based scheduling parser , a recovery technique for failed parses , and a new [[ technique ]] called terminal substring parsing .", "h": ["technique"], "t": ["three techniques"]}, {"label": "USED-FOR", "tokens": "For << pragmatics processing >> , we describe how the method of [[ abductive inference ]] is inherently robust , in that an interpretation is always possible , so that in the absence of the required world knowledge , performance degrades gracefully .", "h": ["abductive inference"], "t": ["pragmatics processing"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes a [[ Hidden Markov Model -LRB- HMM -RRB- ]] and an << HMM-based chunk tagger >> , from which a named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system is built to recognize and classify names , times and numerical quantities .", "h": ["Hidden Markov Model -LRB- HMM -RRB-"], "t": ["HMM-based chunk tagger"]}, {"label": "USED-FOR", "tokens": "This paper proposes a [[ Hidden Markov Model -LRB- HMM -RRB- ]] and an HMM-based chunk tagger , from which a << named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system >> is built to recognize and classify names , times and numerical quantities .", "h": ["Hidden Markov Model -LRB- HMM -RRB-"], "t": ["named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system"]}, {"label": "USED-FOR", "tokens": "This paper proposes a Hidden Markov Model -LRB- HMM -RRB- and an [[ HMM-based chunk tagger ]] , from which a << named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system >> is built to recognize and classify names , times and numerical quantities .", "h": ["HMM-based chunk tagger"], "t": ["named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system"]}, {"label": "USED-FOR", "tokens": "This paper proposes a Hidden Markov Model -LRB- HMM -RRB- and an HMM-based chunk tagger , from which a [[ named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system ]] is built to recognize and classify << names >> , times and numerical quantities .", "h": ["named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system"], "t": ["names"]}, {"label": "USED-FOR", "tokens": "This paper proposes a Hidden Markov Model -LRB- HMM -RRB- and an HMM-based chunk tagger , from which a [[ named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system ]] is built to recognize and classify names , << times and numerical quantities >> .", "h": ["named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system"], "t": ["times and numerical quantities"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes a Hidden Markov Model -LRB- HMM -RRB- and an HMM-based chunk tagger , from which a named entity -LRB- NE -RRB- recognition -LRB- NER -RRB- system is built to recognize and classify [[ names ]] , << times and numerical quantities >> .", "h": ["names"], "t": ["times and numerical quantities"]}, {"label": "HYPONYM-OF", "tokens": "Through the HMM , our system is able to apply and integrate four types of internal and external evidences : 1 -RRB- simple << deterministic internal feature of the words >> , such as [[ capitalization ]] and digitalization ; 2 -RRB- internal semantic feature of important triggers ; 3 -RRB- internal gazetteer feature ; 4 -RRB- external macro context feature .", "h": ["capitalization"], "t": ["deterministic internal feature of the words"]}, {"label": "CONJUNCTION", "tokens": "Through the HMM , our system is able to apply and integrate four types of internal and external evidences : 1 -RRB- simple deterministic internal feature of the words , such as [[ capitalization ]] and << digitalization >> ; 2 -RRB- internal semantic feature of important triggers ; 3 -RRB- internal gazetteer feature ; 4 -RRB- external macro context feature .", "h": ["capitalization"], "t": ["digitalization"]}, {"label": "HYPONYM-OF", "tokens": "Through the HMM , our system is able to apply and integrate four types of internal and external evidences : 1 -RRB- simple << deterministic internal feature of the words >> , such as capitalization and [[ digitalization ]] ; 2 -RRB- internal semantic feature of important triggers ; 3 -RRB- internal gazetteer feature ; 4 -RRB- external macro context feature .", "h": ["digitalization"], "t": ["deterministic internal feature of the words"]}, {"label": "EVALUATE-FOR", "tokens": "Evaluation of our << system >> on [[ MUC-6 and MUC-7 English NE tasks ]] achieves F-measures of 96.6 % and 94.1 % respectively .", "h": ["MUC-6 and MUC-7 English NE tasks"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "Evaluation of our << system >> on MUC-6 and MUC-7 English NE tasks achieves [[ F-measures ]] of 96.6 % and 94.1 % respectively .", "h": ["F-measures"], "t": ["system"]}, {"label": "PART-OF", "tokens": "Two [[ themes ]] have evolved in << speech and text image processing >> work at Xerox PARC that expand and redefine the role of recognition technology in document-oriented applications .", "h": ["themes"], "t": ["speech and text image processing"]}, {"label": "USED-FOR", "tokens": "Two themes have evolved in speech and text image processing work at Xerox PARC that expand and redefine the role of [[ recognition technology ]] in << document-oriented applications >> .", "h": ["recognition technology"], "t": ["document-oriented applications"]}, {"label": "CONJUNCTION", "tokens": "One is the development of [[ systems ]] that provide functionality similar to that of << text processors >> but operate directly on audio and scanned image data .", "h": ["systems"], "t": ["text processors"]}, {"label": "USED-FOR", "tokens": "One is the development of << systems >> that provide functionality similar to that of text processors but operate directly on [[ audio and scanned image data ]] .", "h": ["audio and scanned image data"], "t": ["systems"]}, {"label": "USED-FOR", "tokens": "A second , related << theme >> is the use of [[ speech and text-image recognition ]] to retrieve arbitrary , user-specified information from documents with signal content .", "h": ["speech and text-image recognition"], "t": ["theme"]}, {"label": "USED-FOR", "tokens": "A second , related theme is the use of << speech and text-image recognition >> to retrieve arbitrary , user-specified information from [[ documents with signal content ]] .", "h": ["documents with signal content"], "t": ["speech and text-image recognition"]}, {"label": "HYPONYM-OF", "tokens": "This paper discusses three << research >> initiatives at PARC that exemplify these themes : a [[ text-image editor ]] -LSB- 1 -RSB- , a wordspotter for voice editing and indexing -LSB- 12 -RSB- , and a decoding framework for scanned-document content retrieval -LSB- 4 -RSB- .", "h": ["text-image editor"], "t": ["research"]}, {"label": "CONJUNCTION", "tokens": "This paper discusses three research initiatives at PARC that exemplify these themes : a [[ text-image editor ]] -LSB- 1 -RSB- , a << wordspotter >> for voice editing and indexing -LSB- 12 -RSB- , and a decoding framework for scanned-document content retrieval -LSB- 4 -RSB- .", "h": ["text-image editor"], "t": ["wordspotter"]}, {"label": "HYPONYM-OF", "tokens": "This paper discusses three << research >> initiatives at PARC that exemplify these themes : a text-image editor -LSB- 1 -RSB- , a [[ wordspotter ]] for voice editing and indexing -LSB- 12 -RSB- , and a decoding framework for scanned-document content retrieval -LSB- 4 -RSB- .", "h": ["wordspotter"], "t": ["research"]}, {"label": "CONJUNCTION", "tokens": "This paper discusses three research initiatives at PARC that exemplify these themes : a text-image editor -LSB- 1 -RSB- , a [[ wordspotter ]] for << voice editing and indexing >> -LSB- 12 -RSB- , and a decoding framework for scanned-document content retrieval -LSB- 4 -RSB- .", "h": ["wordspotter"], "t": ["voice editing and indexing"]}, {"label": "CONJUNCTION", "tokens": "This paper discusses three research initiatives at PARC that exemplify these themes : a text-image editor -LSB- 1 -RSB- , a wordspotter for [[ voice editing and indexing ]] -LSB- 12 -RSB- , and a << decoding framework >> for scanned-document content retrieval -LSB- 4 -RSB- .", "h": ["voice editing and indexing"], "t": ["decoding framework"]}, {"label": "HYPONYM-OF", "tokens": "This paper discusses three << research >> initiatives at PARC that exemplify these themes : a text-image editor -LSB- 1 -RSB- , a wordspotter for voice editing and indexing -LSB- 12 -RSB- , and a [[ decoding framework ]] for scanned-document content retrieval -LSB- 4 -RSB- .", "h": ["decoding framework"], "t": ["research"]}, {"label": "USED-FOR", "tokens": "This paper discusses three research initiatives at PARC that exemplify these themes : a text-image editor -LSB- 1 -RSB- , a wordspotter for voice editing and indexing -LSB- 12 -RSB- , and a [[ decoding framework ]] for << scanned-document content retrieval >> -LSB- 4 -RSB- .", "h": ["decoding framework"], "t": ["scanned-document content retrieval"]}, {"label": "USED-FOR", "tokens": "The problem of << predicting image or video interestingness >> from their [[ low-level feature representations ]] has received increasing interest .", "h": ["low-level feature representations"], "t": ["predicting image or video interestingness"]}, {"label": "USED-FOR", "tokens": "To make the annotation less subjective and more reliable , recent studies employ [[ crowdsourcing tools ]] to collect << pairwise comparisons >> -- relying on majority voting to prune the annotation outliers/errors .", "h": ["crowdsourcing tools"], "t": ["pairwise comparisons"]}, {"label": "USED-FOR", "tokens": "To make the annotation less subjective and more reliable , recent studies employ << crowdsourcing tools >> to collect pairwise comparisons -- relying on [[ majority voting ]] to prune the annotation outliers/errors .", "h": ["majority voting"], "t": ["crowdsourcing tools"]}, {"label": "USED-FOR", "tokens": "To make the annotation less subjective and more reliable , recent studies employ crowdsourcing tools to collect pairwise comparisons -- relying on [[ majority voting ]] to prune the << annotation outliers/errors >> .", "h": ["majority voting"], "t": ["annotation outliers/errors"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a more principled [[ way ]] to identify << annotation outliers >> by formulating the interestingness prediction task as a unified robust learning to rank problem , tackling both the outlier detection and interestingness prediction tasks jointly .", "h": ["way"], "t": ["annotation outliers"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a more principled [[ way ]] to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem , tackling both the << outlier detection >> and interestingness prediction tasks jointly .", "h": ["way"], "t": ["outlier detection"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a more principled [[ way ]] to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem , tackling both the outlier detection and << interestingness prediction tasks >> jointly .", "h": ["way"], "t": ["interestingness prediction tasks"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a more principled way to identify << annotation outliers >> by formulating the [[ interestingness prediction task ]] as a unified robust learning to rank problem , tackling both the outlier detection and interestingness prediction tasks jointly .", "h": ["interestingness prediction task"], "t": ["annotation outliers"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a more principled way to identify annotation outliers by formulating the << interestingness prediction task >> as a [[ unified robust learning ]] to rank problem , tackling both the outlier detection and interestingness prediction tasks jointly .", "h": ["unified robust learning"], "t": ["interestingness prediction task"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a more principled way to identify annotation outliers by formulating the interestingness prediction task as a [[ unified robust learning ]] to << rank problem >> , tackling both the outlier detection and interestingness prediction tasks jointly .", "h": ["unified robust learning"], "t": ["rank problem"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we propose a more principled way to identify annotation outliers by formulating the interestingness prediction task as a unified robust learning to rank problem , tackling both the [[ outlier detection ]] and << interestingness prediction tasks >> jointly .", "h": ["outlier detection"], "t": ["interestingness prediction tasks"]}, {"label": "EVALUATE-FOR", "tokens": "Extensive experiments on both [[ image and video interestingness benchmark datasets ]] demonstrate that our new << approach >> significantly outperforms state-of-the-art alternatives .", "h": ["image and video interestingness benchmark datasets"], "t": ["approach"]}, {"label": "COMPARE", "tokens": "Extensive experiments on both image and video interestingness benchmark datasets demonstrate that our new [[ approach ]] significantly outperforms << state-of-the-art alternatives >> .", "h": ["approach"], "t": ["state-of-the-art alternatives"]}, {"label": "CONJUNCTION", "tokens": "Many << description logics -LRB- DLs -RRB- >> combine [[ knowledge representation ]] on an abstract , logical level with an interface to `` concrete '' domains such as numbers and strings .", "h": ["knowledge representation"], "t": ["description logics -LRB- DLs -RRB-"]}, {"label": "USED-FOR", "tokens": "We describe an implementation of [[ data-driven selection ]] of emphatic facial displays for an << embodied conversational agent >> in a dialogue system .", "h": ["data-driven selection"], "t": ["embodied conversational agent"]}, {"label": "USED-FOR", "tokens": "We describe an implementation of << data-driven selection >> of [[ emphatic facial displays ]] for an embodied conversational agent in a dialogue system .", "h": ["emphatic facial displays"], "t": ["data-driven selection"]}, {"label": "PART-OF", "tokens": "We describe an implementation of data-driven selection of emphatic facial displays for an [[ embodied conversational agent ]] in a << dialogue system >> .", "h": ["embodied conversational agent"], "t": ["dialogue system"]}, {"label": "USED-FOR", "tokens": "The [[ data ]] from those recordings was used in a range of << models >> for generating facial displays , each model making use of a different amount of context or choosing displays differently within a context .", "h": ["data"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "The data from those recordings was used in a range of [[ models ]] for generating << facial displays >> , each model making use of a different amount of context or choosing displays differently within a context .", "h": ["models"], "t": ["facial displays"]}, {"label": "EVALUATE-FOR", "tokens": "The << models >> were evaluated in two ways : by [[ cross-validation ]] against the corpus , and by asking users to rate the output .", "h": ["cross-validation"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "When << classifying high-dimensional sequence data >> , traditional methods -LRB- e.g. , [[ HMMs ]] , CRFs -RRB- may require large amounts of training data to avoid overfitting .", "h": ["HMMs"], "t": ["classifying high-dimensional sequence data"]}, {"label": "CONJUNCTION", "tokens": "When classifying high-dimensional sequence data , traditional methods -LRB- e.g. , [[ HMMs ]] , << CRFs >> -RRB- may require large amounts of training data to avoid overfitting .", "h": ["HMMs"], "t": ["CRFs"]}, {"label": "USED-FOR", "tokens": "When << classifying high-dimensional sequence data >> , traditional methods -LRB- e.g. , HMMs , [[ CRFs ]] -RRB- may require large amounts of training data to avoid overfitting .", "h": ["CRFs"], "t": ["classifying high-dimensional sequence data"]}, {"label": "USED-FOR", "tokens": "In such cases [[ dimensionality reduction ]] can be employed to find a << low-dimensional representation >> on which classification can be done more efficiently .", "h": ["dimensionality reduction"], "t": ["low-dimensional representation"]}, {"label": "USED-FOR", "tokens": "In such cases dimensionality reduction can be employed to find a [[ low-dimensional representation ]] on which << classification >> can be done more efficiently .", "h": ["low-dimensional representation"], "t": ["classification"]}, {"label": "USED-FOR", "tokens": "[[ Existing methods ]] for << supervised dimensionality reduction >> often presume that the data is densely sampled so that a neighborhood graph structure can be formed , or that the data arises from a known distribution .", "h": ["Existing methods"], "t": ["supervised dimensionality reduction"]}, {"label": "USED-FOR", "tokens": "[[ Sufficient dimension reduction techniques ]] aim to find a << low dimensional representation >> such that the remaining degrees of freedom become conditionally independent of the output values .", "h": ["Sufficient dimension reduction techniques"], "t": ["low dimensional representation"]}, {"label": "USED-FOR", "tokens": "Spatial , temporal and periodic information is combined in a principled manner , and an optimal [[ manifold ]] is learned for the << end-task >> .", "h": ["manifold"], "t": ["end-task"]}, {"label": "EVALUATE-FOR", "tokens": "We demonstrate the effectiveness of our << approach >> on several tasks involving the [[ discrimination of human gesture and motion categories ]] , as well as on a database of dynamic textures .", "h": ["discrimination of human gesture and motion categories"], "t": ["approach"]}, {"label": "EVALUATE-FOR", "tokens": "We demonstrate the effectiveness of our << approach >> on several tasks involving the discrimination of human gesture and motion categories , as well as on a [[ database of dynamic textures ]] .", "h": ["database of dynamic textures"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "We present an efficient [[ algorithm ]] for << chart-based phrase structure parsing >> of natural language that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task .", "h": ["algorithm"], "t": ["chart-based phrase structure parsing"]}, {"label": "USED-FOR", "tokens": "We present an efficient algorithm for << chart-based phrase structure parsing >> of [[ natural language ]] that is tailored to the problem of extracting specific information from unrestricted texts where many of the words are unknown and much of the text is irrelevant to the task .", "h": ["natural language"], "t": ["chart-based phrase structure parsing"]}, {"label": "USED-FOR", "tokens": "This is facilitated through the use of << phrase boundary heuristics >> based on the placement of [[ function words ]] , and by heuristic rules that permit certain kinds of phrases to be deduced despite the presence of unknown words .", "h": ["function words"], "t": ["phrase boundary heuristics"]}, {"label": "USED-FOR", "tokens": "A further << reduction in the search space >> is achieved by using [[ semantic ]] rather than syntactic categories on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .", "h": ["semantic"], "t": ["reduction in the search space"]}, {"label": "COMPARE", "tokens": "A further reduction in the search space is achieved by using [[ semantic ]] rather than << syntactic categories >> on the terminal and non-terminal edges , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .", "h": ["semantic"], "t": ["syntactic categories"]}, {"label": "FEATURE-OF", "tokens": "A further reduction in the search space is achieved by using [[ semantic ]] rather than syntactic categories on the << terminal and non-terminal edges >> , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .", "h": ["semantic"], "t": ["terminal and non-terminal edges"]}, {"label": "FEATURE-OF", "tokens": "A further reduction in the search space is achieved by using semantic rather than [[ syntactic categories ]] on the << terminal and non-terminal edges >> , thereby reducing the amount of ambiguity and thus the number of edges , since only edges with a valid semantic interpretation are ever introduced .", "h": ["syntactic categories"], "t": ["terminal and non-terminal edges"]}, {"label": "USED-FOR", "tokens": "[[ Automatic estimation of word significance ]] oriented for << speech-based Information Retrieval -LRB- IR -RRB- >> is addressed .", "h": ["Automatic estimation of word significance"], "t": ["speech-based Information Retrieval -LRB- IR -RRB-"]}, {"label": "EVALUATE-FOR", "tokens": "Since the significance of words differs in IR , << automatic speech recognition -LRB- ASR -RRB- >> performance has been evaluated based on [[ weighted word error rate -LRB- WWER -RRB- ]] , which gives a weight on errors from the viewpoint of IR , instead of word error rate -LRB- WER -RRB- , which treats all words uniformly .", "h": ["weighted word error rate -LRB- WWER -RRB-"], "t": ["automatic speech recognition -LRB- ASR -RRB-"]}, {"label": "COMPARE", "tokens": "Since the significance of words differs in IR , automatic speech recognition -LRB- ASR -RRB- performance has been evaluated based on << weighted word error rate -LRB- WWER -RRB- >> , which gives a weight on errors from the viewpoint of IR , instead of [[ word error rate -LRB- WER -RRB- ]] , which treats all words uniformly .", "h": ["word error rate -LRB- WER -RRB-"], "t": ["weighted word error rate -LRB- WWER -RRB-"]}, {"label": "USED-FOR", "tokens": "A [[ decoding strategy ]] that minimizes << WWER >> based on a Minimum Bayes-Risk framework has been shown , and the reduction of errors on both ASR and IR has been reported .", "h": ["decoding strategy"], "t": ["WWER"]}, {"label": "USED-FOR", "tokens": "A << decoding strategy >> that minimizes WWER based on a [[ Minimum Bayes-Risk framework ]] has been shown , and the reduction of errors on both ASR and IR has been reported .", "h": ["Minimum Bayes-Risk framework"], "t": ["decoding strategy"]}, {"label": "CONJUNCTION", "tokens": "A decoding strategy that minimizes WWER based on a Minimum Bayes-Risk framework has been shown , and the reduction of errors on both [[ ASR ]] and << IR >> has been reported .", "h": ["ASR"], "t": ["IR"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper , we propose an [[ automatic estimation method ]] for << word significance -LRB- weights -RRB- >> based on its influence on IR .", "h": ["automatic estimation method"], "t": ["word significance -LRB- weights -RRB-"]}, {"label": "EVALUATE-FOR", "tokens": "Specifically , weights are estimated so that [[ evaluation measures ]] of << ASR >> and IR are equivalent .", "h": ["evaluation measures"], "t": ["ASR"]}, {"label": "EVALUATE-FOR", "tokens": "Specifically , weights are estimated so that [[ evaluation measures ]] of ASR and << IR >> are equivalent .", "h": ["evaluation measures"], "t": ["IR"]}, {"label": "CONJUNCTION", "tokens": "Specifically , weights are estimated so that evaluation measures of [[ ASR ]] and << IR >> are equivalent .", "h": ["ASR"], "t": ["IR"]}, {"label": "USED-FOR", "tokens": "We apply the proposed [[ method ]] to a << speech-based information retrieval system >> , which is a typical IR system , and show that the method works well .", "h": ["method"], "t": ["speech-based information retrieval system"]}, {"label": "HYPONYM-OF", "tokens": "We apply the proposed method to a [[ speech-based information retrieval system ]] , which is a typical << IR system >> , and show that the method works well .", "h": ["speech-based information retrieval system"], "t": ["IR system"]}, {"label": "USED-FOR", "tokens": "[[ Methods ]] developed for << spelling correction >> for languages like English -LRB- see the review by Kukich -LRB- Kukich , 1992 -RRB- -RRB- are not readily applicable to agglutinative languages .", "h": ["Methods"], "t": ["spelling correction"]}, {"label": "USED-FOR", "tokens": "Methods developed for [[ spelling correction ]] for << languages >> like English -LRB- see the review by Kukich -LRB- Kukich , 1992 -RRB- -RRB- are not readily applicable to agglutinative languages .", "h": ["spelling correction"], "t": ["languages"]}, {"label": "HYPONYM-OF", "tokens": "Methods developed for spelling correction for << languages >> like [[ English ]] -LRB- see the review by Kukich -LRB- Kukich , 1992 -RRB- -RRB- are not readily applicable to agglutinative languages .", "h": ["English"], "t": ["languages"]}, {"label": "USED-FOR", "tokens": "This poster presents an [[ approach ]] to << spelling correction >> in agglutinative languages that is based on two-level morphology and a dynamic-programming based search algorithm .", "h": ["approach"], "t": ["spelling correction"]}, {"label": "USED-FOR", "tokens": "This poster presents an approach to << spelling correction >> in [[ agglutinative languages ]] that is based on two-level morphology and a dynamic-programming based search algorithm .", "h": ["agglutinative languages"], "t": ["spelling correction"]}, {"label": "USED-FOR", "tokens": "This poster presents an approach to << spelling correction >> in agglutinative languages that is based on [[ two-level morphology ]] and a dynamic-programming based search algorithm .", "h": ["two-level morphology"], "t": ["spelling correction"]}, {"label": "CONJUNCTION", "tokens": "This poster presents an approach to spelling correction in agglutinative languages that is based on [[ two-level morphology ]] and a << dynamic-programming based search algorithm >> .", "h": ["two-level morphology"], "t": ["dynamic-programming based search algorithm"]}, {"label": "USED-FOR", "tokens": "This poster presents an approach to << spelling correction >> in agglutinative languages that is based on two-level morphology and a [[ dynamic-programming based search algorithm ]] .", "h": ["dynamic-programming based search algorithm"], "t": ["spelling correction"]}, {"label": "USED-FOR", "tokens": "After an overview of our approach , we present results from experiments with << spelling correction >> in [[ Turkish ]] .", "h": ["Turkish"], "t": ["spelling correction"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a novel [[ training method ]] for a << localized phrase-based prediction model >> for statistical machine translation -LRB- SMT -RRB- .", "h": ["training method"], "t": ["localized phrase-based prediction model"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a novel training method for a [[ localized phrase-based prediction model ]] for << statistical machine translation -LRB- SMT -RRB- >> .", "h": ["localized phrase-based prediction model"], "t": ["statistical machine translation -LRB- SMT -RRB-"]}, {"label": "USED-FOR", "tokens": "The [[ model ]] predicts blocks with orientation to handle << local phrase re-ordering >> .", "h": ["model"], "t": ["local phrase re-ordering"]}, {"label": "USED-FOR", "tokens": "We use a [[ maximum likelihood criterion ]] to train a << log-linear block bigram model >> which uses real-valued features -LRB- e.g. a language model score -RRB- as well as binary features based on the block identities themselves , e.g. block bigram features .", "h": ["maximum likelihood criterion"], "t": ["log-linear block bigram model"]}, {"label": "USED-FOR", "tokens": "We use a maximum likelihood criterion to train a << log-linear block bigram model >> which uses [[ real-valued features ]] -LRB- e.g. a language model score -RRB- as well as binary features based on the block identities themselves , e.g. block bigram features .", "h": ["real-valued features"], "t": ["log-linear block bigram model"]}, {"label": "CONJUNCTION", "tokens": "We use a maximum likelihood criterion to train a log-linear block bigram model which uses [[ real-valued features ]] -LRB- e.g. a language model score -RRB- as well as << binary features >> based on the block identities themselves , e.g. block bigram features .", "h": ["real-valued features"], "t": ["binary features"]}, {"label": "HYPONYM-OF", "tokens": "We use a maximum likelihood criterion to train a log-linear block bigram model which uses << real-valued features >> -LRB- e.g. a [[ language model score ]] -RRB- as well as binary features based on the block identities themselves , e.g. block bigram features .", "h": ["language model score"], "t": ["real-valued features"]}, {"label": "USED-FOR", "tokens": "We use a maximum likelihood criterion to train a << log-linear block bigram model >> which uses real-valued features -LRB- e.g. a language model score -RRB- as well as [[ binary features ]] based on the block identities themselves , e.g. block bigram features .", "h": ["binary features"], "t": ["log-linear block bigram model"]}, {"label": "USED-FOR", "tokens": "Our << training algorithm >> can easily handle millions of [[ features ]] .", "h": ["features"], "t": ["training algorithm"]}, {"label": "COMPARE", "tokens": "The best [[ system ]] obtains a 18.6 % improvement over the << baseline >> on a standard Arabic-English translation task .", "h": ["system"], "t": ["baseline"]}, {"label": "EVALUATE-FOR", "tokens": "The best << system >> obtains a 18.6 % improvement over the baseline on a standard [[ Arabic-English translation task ]] .", "h": ["Arabic-English translation task"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "The best system obtains a 18.6 % improvement over the << baseline >> on a standard [[ Arabic-English translation task ]] .", "h": ["Arabic-English translation task"], "t": ["baseline"]}, {"label": "USED-FOR", "tokens": "In this paper we describe a novel [[ data structure ]] for << phrase-based statistical machine translation >> which allows for the retrieval of arbitrarily long phrases while simultaneously using less memory than is required by current decoder implementations .", "h": ["data structure"], "t": ["phrase-based statistical machine translation"]}, {"label": "USED-FOR", "tokens": "In this paper we describe a novel [[ data structure ]] for phrase-based statistical machine translation which allows for the << retrieval of arbitrarily long phrases >> while simultaneously using less memory than is required by current decoder implementations .", "h": ["data structure"], "t": ["retrieval of arbitrarily long phrases"]}, {"label": "CONJUNCTION", "tokens": "We detail the [[ computational complexity ]] and << average retrieval times >> for looking up phrase translations in our suffix array-based data structure .", "h": ["computational complexity"], "t": ["average retrieval times"]}, {"label": "PART-OF", "tokens": "We detail the computational complexity and average retrieval times for looking up [[ phrase translations ]] in our << suffix array-based data structure >> .", "h": ["phrase translations"], "t": ["suffix array-based data structure"]}, {"label": "EVALUATE-FOR", "tokens": "We show how << sampling >> can be used to reduce the [[ retrieval time ]] by orders of magnitude with no loss in translation quality .", "h": ["retrieval time"], "t": ["sampling"]}, {"label": "EVALUATE-FOR", "tokens": "We show how << sampling >> can be used to reduce the retrieval time by orders of magnitude with no loss in [[ translation quality ]] .", "h": ["translation quality"], "t": ["sampling"]}, {"label": "USED-FOR", "tokens": "The major objective of this program is to develop and demonstrate robust , high performance [[ continuous speech recognition -LRB- CSR -RRB- techniques ]] focussed on application in << Spoken Language Systems -LRB- SLS -RRB- >> which will enhance the effectiveness of military and civilian computer-based systems .", "h": ["continuous speech recognition -LRB- CSR -RRB- techniques"], "t": ["Spoken Language Systems -LRB- SLS -RRB-"]}, {"label": "USED-FOR", "tokens": "The major objective of this program is to develop and demonstrate robust , high performance continuous speech recognition -LRB- CSR -RRB- techniques focussed on application in [[ Spoken Language Systems -LRB- SLS -RRB- ]] which will enhance the effectiveness of << military and civilian computer-based systems >> .", "h": ["Spoken Language Systems -LRB- SLS -RRB-"], "t": ["military and civilian computer-based systems"]}, {"label": "USED-FOR", "tokens": "A key complementary objective is to define and develop applications of robust speech recognition and understanding systems , and to help catalyze the transition of [[ spoken language technology ]] into << military and civilian systems >> , with particular focus on application of robust CSR to mobile military command and control .", "h": ["spoken language technology"], "t": ["military and civilian systems"]}, {"label": "USED-FOR", "tokens": "A key complementary objective is to define and develop applications of robust speech recognition and understanding systems , and to help catalyze the transition of spoken language technology into military and civilian systems , with particular focus on application of robust [[ CSR ]] to << mobile military command and control >> .", "h": ["CSR"], "t": ["mobile military command and control"]}, {"label": "CONJUNCTION", "tokens": "The research effort focusses on developing advanced [[ acoustic modelling ]] , << rapid search >> , and recognition-time adaptation techniques for robust large-vocabulary CSR , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "h": ["acoustic modelling"], "t": ["rapid search"]}, {"label": "USED-FOR", "tokens": "The research effort focusses on developing advanced [[ acoustic modelling ]] , rapid search , and recognition-time adaptation techniques for robust << large-vocabulary CSR >> , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "h": ["acoustic modelling"], "t": ["large-vocabulary CSR"]}, {"label": "HYPONYM-OF", "tokens": "The research effort focusses on developing advanced [[ acoustic modelling ]] , rapid search , and recognition-time adaptation techniques for robust large-vocabulary CSR , and on applying these << techniques >> to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "h": ["acoustic modelling"], "t": ["techniques"]}, {"label": "USED-FOR", "tokens": "The research effort focusses on developing advanced [[ acoustic modelling ]] , rapid search , and recognition-time adaptation techniques for robust large-vocabulary CSR , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to << military application tasks >> .", "h": ["acoustic modelling"], "t": ["military application tasks"]}, {"label": "CONJUNCTION", "tokens": "The research effort focusses on developing advanced acoustic modelling , [[ rapid search ]] , and << recognition-time adaptation techniques >> for robust large-vocabulary CSR , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "h": ["rapid search"], "t": ["recognition-time adaptation techniques"]}, {"label": "USED-FOR", "tokens": "The research effort focusses on developing advanced acoustic modelling , [[ rapid search ]] , and recognition-time adaptation techniques for robust << large-vocabulary CSR >> , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "h": ["rapid search"], "t": ["large-vocabulary CSR"]}, {"label": "HYPONYM-OF", "tokens": "The research effort focusses on developing advanced acoustic modelling , [[ rapid search ]] , and recognition-time adaptation techniques for robust large-vocabulary CSR , and on applying these << techniques >> to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "h": ["rapid search"], "t": ["techniques"]}, {"label": "USED-FOR", "tokens": "The research effort focusses on developing advanced acoustic modelling , [[ rapid search ]] , and recognition-time adaptation techniques for robust large-vocabulary CSR , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to << military application tasks >> .", "h": ["rapid search"], "t": ["military application tasks"]}, {"label": "USED-FOR", "tokens": "The research effort focusses on developing advanced acoustic modelling , rapid search , and [[ recognition-time adaptation techniques ]] for robust << large-vocabulary CSR >> , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "h": ["recognition-time adaptation techniques"], "t": ["large-vocabulary CSR"]}, {"label": "HYPONYM-OF", "tokens": "The research effort focusses on developing advanced acoustic modelling , rapid search , and [[ recognition-time adaptation techniques ]] for robust large-vocabulary CSR , and on applying these << techniques >> to the new ARPA large-vocabulary CSR corpora and to military application tasks .", "h": ["recognition-time adaptation techniques"], "t": ["techniques"]}, {"label": "USED-FOR", "tokens": "The research effort focusses on developing advanced acoustic modelling , rapid search , and [[ recognition-time adaptation techniques ]] for robust large-vocabulary CSR , and on applying these techniques to the new ARPA large-vocabulary CSR corpora and to << military application tasks >> .", "h": ["recognition-time adaptation techniques"], "t": ["military application tasks"]}, {"label": "USED-FOR", "tokens": "The research effort focusses on developing advanced acoustic modelling , rapid search , and recognition-time adaptation techniques for robust [[ large-vocabulary CSR ]] , and on applying these techniques to the new << ARPA large-vocabulary CSR corpora >> and to military application tasks .", "h": ["large-vocabulary CSR"], "t": ["ARPA large-vocabulary CSR corpora"]}, {"label": "USED-FOR", "tokens": "The research effort focusses on developing advanced acoustic modelling , rapid search , and recognition-time adaptation techniques for robust large-vocabulary CSR , and on applying these [[ techniques ]] to the new ARPA large-vocabulary CSR corpora and to << military application tasks >> .", "h": ["techniques"], "t": ["military application tasks"]}, {"label": "EVALUATE-FOR", "tokens": "The research effort focusses on developing advanced acoustic modelling , rapid search , and recognition-time adaptation techniques for robust large-vocabulary CSR , and on applying these << techniques >> to the new [[ ARPA large-vocabulary CSR corpora ]] and to military application tasks .", "h": ["ARPA large-vocabulary CSR corpora"], "t": ["techniques"]}, {"label": "USED-FOR", "tokens": "This paper examines what kind of << similarity between words >> can be represented by what kind of [[ word vectors ]] in the vector space model .", "h": ["word vectors"], "t": ["similarity between words"]}, {"label": "USED-FOR", "tokens": "This paper examines what kind of similarity between words can be represented by what kind of << word vectors >> in the [[ vector space model ]] .", "h": ["vector space model"], "t": ["word vectors"]}, {"label": "USED-FOR", "tokens": "Through two experiments , three [[ methods ]] for << constructing word vectors >> , i.e. , LSA-based , cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of similarity , i.e. , taxonomic similarity and associative similarity .", "h": ["methods"], "t": ["constructing word vectors"]}, {"label": "USED-FOR", "tokens": "Through two experiments , three methods for constructing word vectors , i.e. , [[ LSA-based , cooccurrence-based and dictionary-based methods ]] , were compared in terms of the ability to represent two kinds of << similarity >> , i.e. , taxonomic similarity and associative similarity .", "h": ["LSA-based , cooccurrence-based and dictionary-based methods"], "t": ["similarity"]}, {"label": "HYPONYM-OF", "tokens": "Through two experiments , three methods for constructing word vectors , i.e. , LSA-based , cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of << similarity >> , i.e. , [[ taxonomic similarity ]] and associative similarity .", "h": ["taxonomic similarity"], "t": ["similarity"]}, {"label": "CONJUNCTION", "tokens": "Through two experiments , three methods for constructing word vectors , i.e. , LSA-based , cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of similarity , i.e. , [[ taxonomic similarity ]] and << associative similarity >> .", "h": ["taxonomic similarity"], "t": ["associative similarity"]}, {"label": "HYPONYM-OF", "tokens": "Through two experiments , three methods for constructing word vectors , i.e. , LSA-based , cooccurrence-based and dictionary-based methods , were compared in terms of the ability to represent two kinds of << similarity >> , i.e. , taxonomic similarity and [[ associative similarity ]] .", "h": ["associative similarity"], "t": ["similarity"]}, {"label": "USED-FOR", "tokens": "The result of the comparison was that the [[ dictionary-based word vectors ]] better reflect << taxonomic similarity >> , while the LSA-based and the cooccurrence-based word vectors better reflect associative similarity .", "h": ["dictionary-based word vectors"], "t": ["taxonomic similarity"]}, {"label": "USED-FOR", "tokens": "The result of the comparison was that the dictionary-based word vectors better reflect taxonomic similarity , while the [[ LSA-based and the cooccurrence-based word vectors ]] better reflect << associative similarity >> .", "h": ["LSA-based and the cooccurrence-based word vectors"], "t": ["associative similarity"]}, {"label": "USED-FOR", "tokens": "This paper presents a << maximum entropy word alignment algorithm >> for [[ Arabic-English ]] based on supervised training data .", "h": ["Arabic-English"], "t": ["maximum entropy word alignment algorithm"]}, {"label": "USED-FOR", "tokens": "This paper presents a << maximum entropy word alignment algorithm >> for Arabic-English based on [[ supervised training data ]] .", "h": ["supervised training data"], "t": ["maximum entropy word alignment algorithm"]}, {"label": "USED-FOR", "tokens": "We demonstrate that it is feasible to create [[ training material ]] for problems in << machine translation >> and that a mixture of supervised and unsupervised methods yields superior performance .", "h": ["training material"], "t": ["machine translation"]}, {"label": "USED-FOR", "tokens": "The [[ probabilistic model ]] used in the << alignment >> directly models the link decisions .", "h": ["probabilistic model"], "t": ["alignment"]}, {"label": "USED-FOR", "tokens": "The [[ probabilistic model ]] used in the alignment directly models the << link decisions >> .", "h": ["probabilistic model"], "t": ["link decisions"]}, {"label": "USED-FOR", "tokens": "Significant improvement over traditional [[ word alignment techniques ]] is shown as well as improvement on several << machine translation tests >> .", "h": ["word alignment techniques"], "t": ["machine translation tests"]}, {"label": "COMPARE", "tokens": "Performance of the [[ algorithm ]] is contrasted with << human annotation >> performance .", "h": ["algorithm"], "t": ["human annotation"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a novel [[ Cooperative Model ]] for << natural language understanding >> in a dialogue system .", "h": ["Cooperative Model"], "t": ["natural language understanding"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a novel Cooperative Model for [[ natural language understanding ]] in a << dialogue system >> .", "h": ["natural language understanding"], "t": ["dialogue system"]}, {"label": "USED-FOR", "tokens": "We build << this >> based on both [[ Finite State Model -LRB- FSM -RRB- ]] and Statistical Learning Model -LRB- SLM -RRB- .", "h": ["Finite State Model -LRB- FSM -RRB-"], "t": ["this"]}, {"label": "CONJUNCTION", "tokens": "We build this based on both [[ Finite State Model -LRB- FSM -RRB- ]] and << Statistical Learning Model -LRB- SLM -RRB- >> .", "h": ["Finite State Model -LRB- FSM -RRB-"], "t": ["Statistical Learning Model -LRB- SLM -RRB-"]}, {"label": "USED-FOR", "tokens": "We build << this >> based on both Finite State Model -LRB- FSM -RRB- and [[ Statistical Learning Model -LRB- SLM -RRB- ]] .", "h": ["Statistical Learning Model -LRB- SLM -RRB-"], "t": ["this"]}, {"label": "USED-FOR", "tokens": "[[ FSM ]] provides two strategies for << language understanding >> and have a high accuracy but little robustness and flexibility .", "h": ["FSM"], "t": ["language understanding"]}, {"label": "USED-FOR", "tokens": "The [[ ambiguity resolution of right-side dependencies ]] is essential for << dependency parsing >> of sentences with two or more verbs .", "h": ["ambiguity resolution of right-side dependencies"], "t": ["dependency parsing"]}, {"label": "EVALUATE-FOR", "tokens": "Previous works on shift-reduce dependency parsers may not guarantee the [[ connectivity ]] of a << dependency tree >> due to their weakness at resolving the right-side dependencies .", "h": ["connectivity"], "t": ["dependency tree"]}, {"label": "USED-FOR", "tokens": "This paper proposes a << two-phase shift-reduce dependency parser >> based on [[ SVM learning ]] .", "h": ["SVM learning"], "t": ["two-phase shift-reduce dependency parser"]}, {"label": "CONJUNCTION", "tokens": "The [[ left-side dependents ]] and << right-side nominal dependents >> are detected in Phase I , and right-side verbal dependents are decided in Phase II .", "h": ["left-side dependents"], "t": ["right-side nominal dependents"]}, {"label": "CONJUNCTION", "tokens": "The left-side dependents and << right-side nominal dependents >> are detected in Phase I , and [[ right-side verbal dependents ]] are decided in Phase II .", "h": ["right-side verbal dependents"], "t": ["right-side nominal dependents"]}, {"label": "COMPARE", "tokens": "In experimental evaluation , our proposed [[ method ]] outperforms previous << shift-reduce dependency parsers >> for the Chine language , showing improvement of dependency accuracy by 10.08 % .", "h": ["method"], "t": ["shift-reduce dependency parsers"]}, {"label": "EVALUATE-FOR", "tokens": "In experimental evaluation , our proposed << method >> outperforms previous shift-reduce dependency parsers for the [[ Chine language ]] , showing improvement of dependency accuracy by 10.08 % .", "h": ["Chine language"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "In experimental evaluation , our proposed method outperforms previous << shift-reduce dependency parsers >> for the [[ Chine language ]] , showing improvement of dependency accuracy by 10.08 % .", "h": ["Chine language"], "t": ["shift-reduce dependency parsers"]}, {"label": "EVALUATE-FOR", "tokens": "In experimental evaluation , our proposed << method >> outperforms previous shift-reduce dependency parsers for the Chine language , showing improvement of [[ dependency accuracy ]] by 10.08 % .", "h": ["dependency accuracy"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "In experimental evaluation , our proposed method outperforms previous << shift-reduce dependency parsers >> for the Chine language , showing improvement of [[ dependency accuracy ]] by 10.08 % .", "h": ["dependency accuracy"], "t": ["shift-reduce dependency parsers"]}, {"label": "CONJUNCTION", "tokens": "By using [[ commands ]] or << rules >> which are defined to facilitate the construction of format expected or some mathematical expressions , elaborate and pretty documents can be successfully obtained .", "h": ["commands"], "t": ["rules"]}, {"label": "USED-FOR", "tokens": "By using [[ commands ]] or rules which are defined to facilitate the construction of format expected or some << mathematical expressions >> , elaborate and pretty documents can be successfully obtained .", "h": ["commands"], "t": ["mathematical expressions"]}, {"label": "USED-FOR", "tokens": "By using commands or [[ rules ]] which are defined to facilitate the construction of format expected or some << mathematical expressions >> , elaborate and pretty documents can be successfully obtained .", "h": ["rules"], "t": ["mathematical expressions"]}, {"label": "EVALUATE-FOR", "tokens": "This paper presents an [[ evaluation method ]] employing a latent variable model for << paraphrases >> with their contexts .", "h": ["evaluation method"], "t": ["paraphrases"]}, {"label": "USED-FOR", "tokens": "This paper presents an << evaluation method >> employing a [[ latent variable model ]] for paraphrases with their contexts .", "h": ["latent variable model"], "t": ["evaluation method"]}, {"label": "EVALUATE-FOR", "tokens": "The results also revealed an upper bound of [[ accuracy ]] of 77 % with the << method >> when using only topic information .", "h": ["accuracy"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The results also revealed an upper bound of accuracy of 77 % with the << method >> when using only [[ topic information ]] .", "h": ["topic information"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "We describe the [[ methods ]] and << hardware >> that we are using to produce a real-time demonstration of an integrated Spoken Language System .", "h": ["methods"], "t": ["hardware"]}, {"label": "USED-FOR", "tokens": "We describe the [[ methods ]] and hardware that we are using to produce a real-time demonstration of an << integrated Spoken Language System >> .", "h": ["methods"], "t": ["integrated Spoken Language System"]}, {"label": "USED-FOR", "tokens": "We describe the methods and [[ hardware ]] that we are using to produce a real-time demonstration of an << integrated Spoken Language System >> .", "h": ["hardware"], "t": ["integrated Spoken Language System"]}, {"label": "USED-FOR", "tokens": "We describe [[ algorithms ]] that greatly reduce the computation needed to compute the << N-Best sentence hypotheses >> .", "h": ["algorithms"], "t": ["N-Best sentence hypotheses"]}, {"label": "USED-FOR", "tokens": "To avoid << grammar coverage problems >> we use a [[ fully-connected first-order statistical class grammar ]] .", "h": ["fully-connected first-order statistical class grammar"], "t": ["grammar coverage problems"]}, {"label": "USED-FOR", "tokens": "The << speech-search algorithm >> is implemented on a [[ board ]] with a single Intel i860 chip , which provides a factor of 5 speed-up over a SUN 4 for straight C code .", "h": ["board"], "t": ["speech-search algorithm"]}, {"label": "PART-OF", "tokens": "The speech-search algorithm is implemented on a << board >> with a single [[ Intel i860 chip ]] , which provides a factor of 5 speed-up over a SUN 4 for straight C code .", "h": ["Intel i860 chip"], "t": ["board"]}, {"label": "COMPARE", "tokens": "The speech-search algorithm is implemented on a board with a single [[ Intel i860 chip ]] , which provides a factor of 5 speed-up over a << SUN 4 >> for straight C code .", "h": ["Intel i860 chip"], "t": ["SUN 4"]}, {"label": "USED-FOR", "tokens": "The speech-search algorithm is implemented on a board with a single [[ Intel i860 chip ]] , which provides a factor of 5 speed-up over a SUN 4 for << straight C code >> .", "h": ["Intel i860 chip"], "t": ["straight C code"]}, {"label": "USED-FOR", "tokens": "The speech-search algorithm is implemented on a board with a single Intel i860 chip , which provides a factor of 5 speed-up over a [[ SUN 4 ]] for << straight C code >> .", "h": ["SUN 4"], "t": ["straight C code"]}, {"label": "USED-FOR", "tokens": "The [[ board ]] plugs directly into the VME bus of the SUN4 , which controls the << system >> and contains the natural language system and application back end .", "h": ["board"], "t": ["system"]}, {"label": "PART-OF", "tokens": "The board plugs directly into the [[ VME bus ]] of the << SUN4 >> , which controls the system and contains the natural language system and application back end .", "h": ["VME bus"], "t": ["SUN4"]}, {"label": "PART-OF", "tokens": "The << board >> plugs directly into the VME bus of the SUN4 , which controls the system and contains the [[ natural language system ]] and application back end .", "h": ["natural language system"], "t": ["board"]}, {"label": "CONJUNCTION", "tokens": "The board plugs directly into the VME bus of the SUN4 , which controls the system and contains the [[ natural language system ]] and << application back end >> .", "h": ["natural language system"], "t": ["application back end"]}, {"label": "PART-OF", "tokens": "The << board >> plugs directly into the VME bus of the SUN4 , which controls the system and contains the natural language system and [[ application back end ]] .", "h": ["application back end"], "t": ["board"]}, {"label": "USED-FOR", "tokens": "We address the problem of << estimating location information >> of an [[ image ]] using principles from automated representation learning .", "h": ["image"], "t": ["estimating location information"]}, {"label": "USED-FOR", "tokens": "We address the problem of << estimating location information >> of an image using principles from [[ automated representation learning ]] .", "h": ["automated representation learning"], "t": ["estimating location information"]}, {"label": "USED-FOR", "tokens": "We pursue a hierarchical sparse coding approach that learns features useful in discriminating images across locations , by initializing << it >> with a [[ geometric prior ]] corresponding to transformations between image appearance space and their corresponding location grouping space using the notion of parallel transport on manifolds .", "h": ["geometric prior"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "We pursue a hierarchical sparse coding approach that learns features useful in discriminating images across locations , by initializing it with a << geometric prior >> corresponding to transformations between image appearance space and their corresponding location grouping space using the notion of [[ parallel transport on manifolds ]] .", "h": ["parallel transport on manifolds"], "t": ["geometric prior"]}, {"label": "USED-FOR", "tokens": "We then extend this [[ approach ]] to account for the availability of << heterogeneous data modalities >> such as geo-tags and videos pertaining to different locations , and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations .", "h": ["approach"], "t": ["heterogeneous data modalities"]}, {"label": "HYPONYM-OF", "tokens": "We then extend this approach to account for the availability of << heterogeneous data modalities >> such as [[ geo-tags ]] and videos pertaining to different locations , and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations .", "h": ["geo-tags"], "t": ["heterogeneous data modalities"]}, {"label": "CONJUNCTION", "tokens": "We then extend this approach to account for the availability of heterogeneous data modalities such as [[ geo-tags ]] and << videos >> pertaining to different locations , and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations .", "h": ["geo-tags"], "t": ["videos"]}, {"label": "HYPONYM-OF", "tokens": "We then extend this approach to account for the availability of << heterogeneous data modalities >> such as geo-tags and [[ videos ]] pertaining to different locations , and also study a relatively under-addressed problem of transferring knowledge available from certain locations to infer the grouping of data from novel locations .", "h": ["videos"], "t": ["heterogeneous data modalities"]}, {"label": "USED-FOR", "tokens": "We then extend this approach to account for the availability of heterogeneous data modalities such as geo-tags and videos pertaining to different locations , and also study a relatively under-addressed problem of [[ transferring knowledge ]] available from certain locations to infer the << grouping of data >> from novel locations .", "h": ["transferring knowledge"], "t": ["grouping of data"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate our << approach >> on several standard [[ datasets ]] such as im2gps , San Francisco and MediaEval2010 , and obtain state-of-the-art results .", "h": ["datasets"], "t": ["approach"]}, {"label": "HYPONYM-OF", "tokens": "We evaluate our approach on several standard << datasets >> such as [[ im2gps ]] , San Francisco and MediaEval2010 , and obtain state-of-the-art results .", "h": ["im2gps"], "t": ["datasets"]}, {"label": "CONJUNCTION", "tokens": "We evaluate our approach on several standard datasets such as [[ im2gps ]] , << San Francisco >> and MediaEval2010 , and obtain state-of-the-art results .", "h": ["im2gps"], "t": ["San Francisco"]}, {"label": "HYPONYM-OF", "tokens": "We evaluate our approach on several standard << datasets >> such as im2gps , [[ San Francisco ]] and MediaEval2010 , and obtain state-of-the-art results .", "h": ["San Francisco"], "t": ["datasets"]}, {"label": "CONJUNCTION", "tokens": "We evaluate our approach on several standard datasets such as im2gps , [[ San Francisco ]] and << MediaEval2010 >> , and obtain state-of-the-art results .", "h": ["San Francisco"], "t": ["MediaEval2010"]}, {"label": "HYPONYM-OF", "tokens": "We evaluate our approach on several standard << datasets >> such as im2gps , San Francisco and [[ MediaEval2010 ]] , and obtain state-of-the-art results .", "h": ["MediaEval2010"], "t": ["datasets"]}, {"label": "FEATURE-OF", "tokens": "Conventional << HMMs >> have [[ weak duration constraints ]] .", "h": ["weak duration constraints"], "t": ["HMMs"]}, {"label": "USED-FOR", "tokens": "In noisy conditions , the mismatch between corrupted speech signals and << models >> trained on [[ clean speech ]] may cause the decoder to produce word matches with unrealistic durations .", "h": ["clean speech"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "In noisy conditions , the mismatch between corrupted speech signals and models trained on clean speech may cause the [[ decoder ]] to produce << word matches >> with unrealistic durations .", "h": ["decoder"], "t": ["word matches"]}, {"label": "FEATURE-OF", "tokens": "In noisy conditions , the mismatch between corrupted speech signals and models trained on clean speech may cause the decoder to produce << word matches >> with [[ unrealistic durations ]] .", "h": ["unrealistic durations"], "t": ["word matches"]}, {"label": "USED-FOR", "tokens": "This paper presents a simple way to incorporate << word duration constraints >> by [[ unrolling HMMs ]] to form a lattice where word duration probabilities can be applied directly to state transitions .", "h": ["unrolling HMMs"], "t": ["word duration constraints"]}, {"label": "USED-FOR", "tokens": "This paper presents a simple way to incorporate word duration constraints by [[ unrolling HMMs ]] to form a << lattice >> where word duration probabilities can be applied directly to state transitions .", "h": ["unrolling HMMs"], "t": ["lattice"]}, {"label": "USED-FOR", "tokens": "This paper presents a simple way to incorporate word duration constraints by unrolling HMMs to form a lattice where [[ word duration probabilities ]] can be applied directly to << state transitions >> .", "h": ["word duration probabilities"], "t": ["state transitions"]}, {"label": "CONJUNCTION", "tokens": "The expanded << HMMs >> are compatible with conventional [[ Viterbi decoding ]] .", "h": ["Viterbi decoding"], "t": ["HMMs"]}, {"label": "USED-FOR", "tokens": "Experiments on [[ connected-digit recognition ]] show that when using explicit duration constraints the << decoder >> generates word matches with more reasonable durations , and word error rates are significantly reduced across a broad range of noise conditions .", "h": ["connected-digit recognition"], "t": ["decoder"]}, {"label": "USED-FOR", "tokens": "Experiments on connected-digit recognition show that when using explicit [[ duration constraints ]] the << decoder >> generates word matches with more reasonable durations , and word error rates are significantly reduced across a broad range of noise conditions .", "h": ["duration constraints"], "t": ["decoder"]}, {"label": "USED-FOR", "tokens": "Experiments on connected-digit recognition show that when using explicit duration constraints the [[ decoder ]] generates << word matches >> with more reasonable durations , and word error rates are significantly reduced across a broad range of noise conditions .", "h": ["decoder"], "t": ["word matches"]}, {"label": "FEATURE-OF", "tokens": "One of the claimed benefits of [[ Tree Adjoining Grammars ]] is that they have an << extended domain of locality -LRB- EDOL -RRB- >> .", "h": ["Tree Adjoining Grammars"], "t": ["extended domain of locality -LRB- EDOL -RRB-"]}, {"label": "USED-FOR", "tokens": "We consider how this can be exploited to limit the need for [[ feature structure unification ]] during << parsing >> .", "h": ["feature structure unification"], "t": ["parsing"]}, {"label": "HYPONYM-OF", "tokens": "We compare two wide-coverage << lexicalized grammars of English >> , [[ LEXSYS ]] and XTAG , finding that the two grammars exploit EDOL in different ways .", "h": ["LEXSYS"], "t": ["lexicalized grammars of English"]}, {"label": "COMPARE", "tokens": "We compare two wide-coverage lexicalized grammars of English , [[ LEXSYS ]] and << XTAG >> , finding that the two grammars exploit EDOL in different ways .", "h": ["LEXSYS"], "t": ["XTAG"]}, {"label": "HYPONYM-OF", "tokens": "We compare two wide-coverage << lexicalized grammars of English >> , LEXSYS and [[ XTAG ]] , finding that the two grammars exploit EDOL in different ways .", "h": ["XTAG"], "t": ["lexicalized grammars of English"]}, {"label": "USED-FOR", "tokens": "We compare two wide-coverage lexicalized grammars of English , LEXSYS and XTAG , finding that the two [[ grammars ]] exploit << EDOL >> in different ways .", "h": ["grammars"], "t": ["EDOL"]}, {"label": "HYPONYM-OF", "tokens": "[[ Identity uncertainty ]] is a pervasive problem in << real-world data analysis >> .", "h": ["Identity uncertainty"], "t": ["real-world data analysis"]}, {"label": "USED-FOR", "tokens": "Our << approach >> is based on the use of a [[ relational probability model ]] to define a generative model for the domain , including models of author and title corruption and a probabilistic citation grammar .", "h": ["relational probability model"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "Our approach is based on the use of a [[ relational probability model ]] to define a << generative model >> for the domain , including models of author and title corruption and a probabilistic citation grammar .", "h": ["relational probability model"], "t": ["generative model"]}, {"label": "USED-FOR", "tokens": "Our approach is based on the use of a relational probability model to define a [[ generative model ]] for the << domain >> , including models of author and title corruption and a probabilistic citation grammar .", "h": ["generative model"], "t": ["domain"]}, {"label": "PART-OF", "tokens": "Our approach is based on the use of a << relational probability model >> to define a generative model for the domain , including [[ models of author and title corruption ]] and a probabilistic citation grammar .", "h": ["models of author and title corruption"], "t": ["relational probability model"]}, {"label": "CONJUNCTION", "tokens": "Our approach is based on the use of a relational probability model to define a generative model for the domain , including [[ models of author and title corruption ]] and a << probabilistic citation grammar >> .", "h": ["models of author and title corruption"], "t": ["probabilistic citation grammar"]}, {"label": "PART-OF", "tokens": "Our approach is based on the use of a << relational probability model >> to define a generative model for the domain , including models of author and title corruption and a [[ probabilistic citation grammar ]] .", "h": ["probabilistic citation grammar"], "t": ["relational probability model"]}, {"label": "USED-FOR", "tokens": "<< Identity uncertainty >> is handled by extending standard [[ models ]] to incorporate probabilities over the possible mappings between terms in the language and objects in the domain .", "h": ["models"], "t": ["Identity uncertainty"]}, {"label": "USED-FOR", "tokens": "<< Inference >> is based on [[ Markov chain Monte Carlo ]] , augmented with specific methods for generating efficient proposals when the domain contains many objects .", "h": ["Markov chain Monte Carlo"], "t": ["Inference"]}, {"label": "USED-FOR", "tokens": "<< Inference >> is based on Markov chain Monte Carlo , augmented with specific [[ methods ]] for generating efficient proposals when the domain contains many objects .", "h": ["methods"], "t": ["Inference"]}, {"label": "EVALUATE-FOR", "tokens": "Results on several [[ citation data sets ]] show that the << method >> outperforms current algorithms for citation matching .", "h": ["citation data sets"], "t": ["method"]}, {"label": "COMPARE", "tokens": "Results on several citation data sets show that the [[ method ]] outperforms << current algorithms >> for citation matching .", "h": ["method"], "t": ["current algorithms"]}, {"label": "USED-FOR", "tokens": "Results on several citation data sets show that the [[ method ]] outperforms current algorithms for << citation matching >> .", "h": ["method"], "t": ["citation matching"]}, {"label": "USED-FOR", "tokens": "Results on several citation data sets show that the method outperforms [[ current algorithms ]] for << citation matching >> .", "h": ["current algorithms"], "t": ["citation matching"]}, {"label": "USED-FOR", "tokens": "The declarative , relational nature of the model also means that our [[ algorithm ]] can determine << object characteristics >> such as author names by combining multiple citations of multiple papers .", "h": ["algorithm"], "t": ["object characteristics"]}, {"label": "HYPONYM-OF", "tokens": "The declarative , relational nature of the model also means that our algorithm can determine << object characteristics >> such as [[ author names ]] by combining multiple citations of multiple papers .", "h": ["author names"], "t": ["object characteristics"]}, {"label": "USED-FOR", "tokens": "The paper proposes and empirically motivates an integration of [[ supervised learning ]] with unsupervised learning to deal with << human biases in summarization >> .", "h": ["supervised learning"], "t": ["human biases in summarization"]}, {"label": "CONJUNCTION", "tokens": "The paper proposes and empirically motivates an integration of << supervised learning >> with [[ unsupervised learning ]] to deal with human biases in summarization .", "h": ["unsupervised learning"], "t": ["supervised learning"]}, {"label": "USED-FOR", "tokens": "The paper proposes and empirically motivates an integration of supervised learning with [[ unsupervised learning ]] to deal with << human biases in summarization >> .", "h": ["unsupervised learning"], "t": ["human biases in summarization"]}, {"label": "FEATURE-OF", "tokens": "In particular , we explore the use of << probabilistic decision tree >> within the [[ clustering framework ]] to account for the variation as well as regularity in human created summaries .", "h": ["clustering framework"], "t": ["probabilistic decision tree"]}, {"label": "USED-FOR", "tokens": "The << corpus of human created extracts >> is created from a [[ newspaper corpus ]] and used as a test set .", "h": ["newspaper corpus"], "t": ["corpus of human created extracts"]}, {"label": "CONJUNCTION", "tokens": "We build probabilistic decision trees of different flavors and integrate each of << them >> with the [[ clustering framework ]] .", "h": ["clustering framework"], "t": ["them"]}, {"label": "PART-OF", "tokens": "In this study , we propose a knowledge-independent method for aligning terms and thus extracting translations from a << small , domain-specific corpus >> consisting of [[ parallel English and Chinese court judgments ]] from Hong Kong .", "h": ["parallel English and Chinese court judgments"], "t": ["small , domain-specific corpus"]}, {"label": "USED-FOR", "tokens": "With a [[ sentence-aligned corpus ]] , << translation equivalences >> are suggested by analysing the frequency profiles of parallel concordances .", "h": ["sentence-aligned corpus"], "t": ["translation equivalences"]}, {"label": "PART-OF", "tokens": "With a sentence-aligned corpus , translation equivalences are suggested by analysing the [[ frequency profiles ]] of << parallel concordances >> .", "h": ["frequency profiles"], "t": ["parallel concordances"]}, {"label": "COMPARE", "tokens": "The [[ method ]] overcomes the limitations of conventional << statistical methods >> which require large corpora to be effective , and lexical approaches which depend on existing bilingual dictionaries .", "h": ["method"], "t": ["statistical methods"]}, {"label": "COMPARE", "tokens": "The [[ method ]] overcomes the limitations of conventional statistical methods which require large corpora to be effective , and << lexical approaches >> which depend on existing bilingual dictionaries .", "h": ["method"], "t": ["lexical approaches"]}, {"label": "USED-FOR", "tokens": "The method overcomes the limitations of conventional << statistical methods >> which require [[ large corpora ]] to be effective , and lexical approaches which depend on existing bilingual dictionaries .", "h": ["large corpora"], "t": ["statistical methods"]}, {"label": "USED-FOR", "tokens": "The method overcomes the limitations of conventional statistical methods which require large corpora to be effective , and << lexical approaches >> which depend on existing [[ bilingual dictionaries ]] .", "h": ["bilingual dictionaries"], "t": ["lexical approaches"]}, {"label": "CONJUNCTION", "tokens": "Pilot testing on a parallel corpus of about 113K Chinese words and 120K English words gives an encouraging 85 % [[ precision ]] and 45 % << recall >> .", "h": ["precision"], "t": ["recall"]}, {"label": "USED-FOR", "tokens": "Future work includes fine-tuning the algorithm upon the analysis of the errors , and acquiring a [[ translation lexicon ]] for << legal terminology >> by filtering out general terms .", "h": ["translation lexicon"], "t": ["legal terminology"]}, {"label": "USED-FOR", "tokens": "Traditional [[ machine learning techniques ]] have been applied to this << problem >> with reasonable success , but they have been shown to work well only when there is a good match between the training and test data with respect to topic .", "h": ["machine learning techniques"], "t": ["problem"]}, {"label": "FEATURE-OF", "tokens": "This paper demonstrates that match with respect to domain and time is also important , and presents preliminary experiments with << training data >> labeled with [[ emoticons ]] , which has the potential of being independent of domain , topic and time .", "h": ["emoticons"], "t": ["training data"]}, {"label": "USED-FOR", "tokens": "We present a novel [[ algorithm ]] for estimating the broad << 3D geometric structure of outdoor video scenes >> .", "h": ["algorithm"], "t": ["3D geometric structure of outdoor video scenes"]}, {"label": "USED-FOR", "tokens": "Leveraging [[ spatio-temporal video segmentation ]] , we decompose a << dynamic scene >> captured by a video into geometric classes , based on predictions made by region-classifiers that are trained on appearance and motion features .", "h": ["spatio-temporal video segmentation"], "t": ["dynamic scene"]}, {"label": "PART-OF", "tokens": "Leveraging spatio-temporal video segmentation , we decompose a << dynamic scene >> captured by a video into [[ geometric classes ]] , based on predictions made by region-classifiers that are trained on appearance and motion features .", "h": ["geometric classes"], "t": ["dynamic scene"]}, {"label": "USED-FOR", "tokens": "Leveraging spatio-temporal video segmentation , we decompose a dynamic scene captured by a video into << geometric classes >> , based on predictions made by [[ region-classifiers ]] that are trained on appearance and motion features .", "h": ["region-classifiers"], "t": ["geometric classes"]}, {"label": "USED-FOR", "tokens": "Leveraging spatio-temporal video segmentation , we decompose a dynamic scene captured by a video into geometric classes , based on predictions made by << region-classifiers >> that are trained on [[ appearance and motion features ]] .", "h": ["appearance and motion features"], "t": ["region-classifiers"]}, {"label": "EVALUATE-FOR", "tokens": "We built a novel , extensive [[ dataset ]] on geometric context of video to evaluate our << method >> , consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames .", "h": ["dataset"], "t": ["method"]}, {"label": "FEATURE-OF", "tokens": "We built a novel , extensive << dataset >> on [[ geometric context of video ]] to evaluate our method , consisting of over 100 ground-truth annotated outdoor videos with over 20,000 frames .", "h": ["geometric context of video"], "t": ["dataset"]}, {"label": "PART-OF", "tokens": "We built a novel , extensive << dataset >> on geometric context of video to evaluate our method , consisting of over 100 ground-truth [[ annotated outdoor videos ]] with over 20,000 frames .", "h": ["annotated outdoor videos"], "t": ["dataset"]}, {"label": "USED-FOR", "tokens": "To further scale beyond this dataset , we propose a [[ semi-supervised learning framework ]] to expand the pool of << labeled data >> with high confidence predictions obtained from unlabeled data .", "h": ["semi-supervised learning framework"], "t": ["labeled data"]}, {"label": "USED-FOR", "tokens": "To further scale beyond this dataset , we propose a << semi-supervised learning framework >> to expand the pool of labeled data with [[ high confidence predictions ]] obtained from unlabeled data .", "h": ["high confidence predictions"], "t": ["semi-supervised learning framework"]}, {"label": "USED-FOR", "tokens": "To further scale beyond this dataset , we propose a semi-supervised learning framework to expand the pool of labeled data with << high confidence predictions >> obtained from [[ unlabeled data ]] .", "h": ["unlabeled data"], "t": ["high confidence predictions"]}, {"label": "USED-FOR", "tokens": "Our [[ system ]] produces an accurate prediction of << geometric context of video >> achieving 96 % accuracy across main geometric classes .", "h": ["system"], "t": ["geometric context of video"]}, {"label": "EVALUATE-FOR", "tokens": "Our << system >> produces an accurate prediction of geometric context of video achieving 96 % [[ accuracy ]] across main geometric classes .", "h": ["accuracy"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "This paper describes a [[ system ]] -LRB- RAREAS -RRB- which synthesizes << marine weather forecasts >> directly from formatted weather data .", "h": ["system"], "t": ["marine weather forecasts"]}, {"label": "USED-FOR", "tokens": "This paper describes a << system >> -LRB- RAREAS -RRB- which synthesizes marine weather forecasts directly from [[ formatted weather data ]] .", "h": ["formatted weather data"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "Such << synthesis >> appears feasible in certain [[ natural sublanguages with stereotyped text structure ]] .", "h": ["natural sublanguages with stereotyped text structure"], "t": ["synthesis"]}, {"label": "USED-FOR", "tokens": "<< RAREAS >> draws on several kinds of [[ linguistic and non-linguistic knowledge ]] and mirrors a forecaster 's apparent tendency to ascribe less precise temporal adverbs to more remote meteorological events .", "h": ["linguistic and non-linguistic knowledge"], "t": ["RAREAS"]}, {"label": "USED-FOR", "tokens": "The << approach >> can easily be adapted to synthesize [[ bilingual or multi-lingual texts ]] .", "h": ["bilingual or multi-lingual texts"], "t": ["approach"]}, {"label": "HYPONYM-OF", "tokens": "We go , on to describe [[ FlexP ]] , a << bottom-up pattern-matching parser >> that we have designed and implemented to provide these flexibilities for restricted natural language input to a limited-domain computer system .", "h": ["FlexP"], "t": ["bottom-up pattern-matching parser"]}, {"label": "USED-FOR", "tokens": "We go , on to describe FlexP , a [[ bottom-up pattern-matching parser ]] that we have designed and implemented to provide these << flexibilities >> for restricted natural language input to a limited-domain computer system .", "h": ["bottom-up pattern-matching parser"], "t": ["flexibilities"]}, {"label": "FEATURE-OF", "tokens": "We go , on to describe FlexP , a bottom-up pattern-matching parser that we have designed and implemented to provide these [[ flexibilities ]] for << restricted natural language >> input to a limited-domain computer system .", "h": ["flexibilities"], "t": ["restricted natural language"]}, {"label": "PART-OF", "tokens": "We go , on to describe FlexP , a bottom-up pattern-matching parser that we have designed and implemented to provide these [[ flexibilities ]] for restricted natural language input to a << limited-domain computer system >> .", "h": ["flexibilities"], "t": ["limited-domain computer system"]}, {"label": "USED-FOR", "tokens": "We go , on to describe FlexP , a << bottom-up pattern-matching parser >> that we have designed and implemented to provide these flexibilities for [[ restricted natural language ]] input to a limited-domain computer system .", "h": ["restricted natural language"], "t": ["bottom-up pattern-matching parser"]}, {"label": "USED-FOR", "tokens": "Traditional << information retrieval techniques >> use a [[ histogram of keywords ]] as the document representation but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance .", "h": ["histogram of keywords"], "t": ["information retrieval techniques"]}, {"label": "USED-FOR", "tokens": "Traditional information retrieval techniques use a [[ histogram of keywords ]] as the << document representation >> but oral communication may offer additional indices such as the time and place of the rejoinder and the attendance .", "h": ["histogram of keywords"], "t": ["document representation"]}, {"label": "HYPONYM-OF", "tokens": "An alternative index could be the << activity >> such as [[ discussing ]] , planning , informing , story-telling , etc. .", "h": ["discussing"], "t": ["activity"]}, {"label": "CONJUNCTION", "tokens": "An alternative index could be the activity such as [[ discussing ]] , << planning >> , informing , story-telling , etc. .", "h": ["discussing"], "t": ["planning"]}, {"label": "HYPONYM-OF", "tokens": "An alternative index could be the << activity >> such as discussing , [[ planning ]] , informing , story-telling , etc. .", "h": ["planning"], "t": ["activity"]}, {"label": "CONJUNCTION", "tokens": "An alternative index could be the activity such as discussing , [[ planning ]] , << informing >> , story-telling , etc. .", "h": ["planning"], "t": ["informing"]}, {"label": "HYPONYM-OF", "tokens": "An alternative index could be the << activity >> such as discussing , planning , [[ informing ]] , story-telling , etc. .", "h": ["informing"], "t": ["activity"]}, {"label": "CONJUNCTION", "tokens": "An alternative index could be the activity such as discussing , planning , [[ informing ]] , << story-telling >> , etc. .", "h": ["informing"], "t": ["story-telling"]}, {"label": "HYPONYM-OF", "tokens": "An alternative index could be the << activity >> such as discussing , planning , informing , [[ story-telling ]] , etc. .", "h": ["story-telling"], "t": ["activity"]}, {"label": "USED-FOR", "tokens": "This paper addresses the problem of the << automatic detection >> of those [[ activities ]] in meeting situation and everyday rejoinders .", "h": ["activities"], "t": ["automatic detection"]}, {"label": "FEATURE-OF", "tokens": "The format of the << corpus >> adopts the [[ Child Language Data Exchange System -LRB- CHILDES -RRB- ]] .", "h": ["Child Language Data Exchange System -LRB- CHILDES -RRB-"], "t": ["corpus"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we describe [[ data collection ]] , << transcription >> , word segmentation , and part-of-speech annotation of this corpus .", "h": ["data collection"], "t": ["transcription"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe [[ data collection ]] , transcription , word segmentation , and part-of-speech annotation of this << corpus >> .", "h": ["data collection"], "t": ["corpus"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we describe data collection , [[ transcription ]] , << word segmentation >> , and part-of-speech annotation of this corpus .", "h": ["transcription"], "t": ["word segmentation"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe data collection , [[ transcription ]] , word segmentation , and part-of-speech annotation of this << corpus >> .", "h": ["transcription"], "t": ["corpus"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we describe data collection , transcription , [[ word segmentation ]] , and << part-of-speech annotation >> of this corpus .", "h": ["word segmentation"], "t": ["part-of-speech annotation"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe data collection , transcription , [[ word segmentation ]] , and part-of-speech annotation of this << corpus >> .", "h": ["word segmentation"], "t": ["corpus"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe data collection , transcription , word segmentation , and [[ part-of-speech annotation ]] of this << corpus >> .", "h": ["part-of-speech annotation"], "t": ["corpus"]}, {"label": "USED-FOR", "tokens": "This paper shows how << dictionary word sense definitions >> can be analysed by applying a [[ hierarchy of phrasal patterns ]] .", "h": ["hierarchy of phrasal patterns"], "t": ["dictionary word sense definitions"]}, {"label": "PART-OF", "tokens": "An experimental << system >> embodying this [[ mechanism ]] has been implemented for processing definitions from the Longman Dictionary of Contemporary English .", "h": ["mechanism"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "A property of this dictionary , exploited by the system , is that << it >> uses a [[ restricted vocabulary ]] in its word sense definitions .", "h": ["restricted vocabulary"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "A property of this dictionary , exploited by the system , is that it uses a [[ restricted vocabulary ]] in its << word sense definitions >> .", "h": ["restricted vocabulary"], "t": ["word sense definitions"]}, {"label": "USED-FOR", "tokens": "The structures generated by the experimental system are intended to be used for the << classification of new word senses >> in terms of the senses of words in the [[ restricted vocabulary ]] .", "h": ["restricted vocabulary"], "t": ["classification of new word senses"]}, {"label": "FEATURE-OF", "tokens": "Thus the work reported addresses two [[ robustness problems ]] faced by current experimental << natural language processing systems >> : coping with an incomplete lexicon and with incomplete knowledge of phrasal constructions .", "h": ["robustness problems"], "t": ["natural language processing systems"]}, {"label": "HYPONYM-OF", "tokens": "Thus the work reported addresses two << robustness problems >> faced by current experimental natural language processing systems : coping with an [[ incomplete lexicon ]] and with incomplete knowledge of phrasal constructions .", "h": ["incomplete lexicon"], "t": ["robustness problems"]}, {"label": "HYPONYM-OF", "tokens": "Thus the work reported addresses two << robustness problems >> faced by current experimental natural language processing systems : coping with an incomplete lexicon and with [[ incomplete knowledge of phrasal constructions ]] .", "h": ["incomplete knowledge of phrasal constructions"], "t": ["robustness problems"]}, {"label": "USED-FOR", "tokens": "This paper presents a << word segmentation system >> in France Telecom R&D Beijing , which uses a unified [[ approach ]] to word breaking and OOV identification .", "h": ["approach"], "t": ["word segmentation system"]}, {"label": "USED-FOR", "tokens": "This paper presents a word segmentation system in France Telecom R&D Beijing , which uses a unified [[ approach ]] to << word breaking >> and OOV identification .", "h": ["approach"], "t": ["word breaking"]}, {"label": "USED-FOR", "tokens": "This paper presents a word segmentation system in France Telecom R&D Beijing , which uses a unified [[ approach ]] to word breaking and << OOV identification >> .", "h": ["approach"], "t": ["OOV identification"]}, {"label": "CONJUNCTION", "tokens": "This paper presents a word segmentation system in France Telecom R&D Beijing , which uses a unified approach to [[ word breaking ]] and << OOV identification >> .", "h": ["word breaking"], "t": ["OOV identification"]}, {"label": "HYPONYM-OF", "tokens": "The system participated in all the tracks of the << segmentation bakeoff >> -- [[ PK-open ]] , PK-closed , AS-open , AS-closed , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["PK-open"], "t": ["segmentation bakeoff"]}, {"label": "CONJUNCTION", "tokens": "The system participated in all the tracks of the segmentation bakeoff -- [[ PK-open ]] , << PK-closed >> , AS-open , AS-closed , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["PK-open"], "t": ["PK-closed"]}, {"label": "HYPONYM-OF", "tokens": "The system participated in all the tracks of the << segmentation bakeoff >> -- PK-open , [[ PK-closed ]] , AS-open , AS-closed , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["PK-closed"], "t": ["segmentation bakeoff"]}, {"label": "CONJUNCTION", "tokens": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , [[ PK-closed ]] , << AS-open >> , AS-closed , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["PK-closed"], "t": ["AS-open"]}, {"label": "HYPONYM-OF", "tokens": "The system participated in all the tracks of the << segmentation bakeoff >> -- PK-open , PK-closed , [[ AS-open ]] , AS-closed , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["AS-open"], "t": ["segmentation bakeoff"]}, {"label": "CONJUNCTION", "tokens": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , PK-closed , [[ AS-open ]] , << AS-closed >> , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["AS-open"], "t": ["AS-closed"]}, {"label": "HYPONYM-OF", "tokens": "The system participated in all the tracks of the << segmentation bakeoff >> -- PK-open , PK-closed , AS-open , [[ AS-closed ]] , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["AS-closed"], "t": ["segmentation bakeoff"]}, {"label": "CONJUNCTION", "tokens": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , PK-closed , AS-open , [[ AS-closed ]] , << HK-open >> , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["AS-closed"], "t": ["HK-open"]}, {"label": "HYPONYM-OF", "tokens": "The system participated in all the tracks of the << segmentation bakeoff >> -- PK-open , PK-closed , AS-open , AS-closed , [[ HK-open ]] , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["HK-open"], "t": ["segmentation bakeoff"]}, {"label": "CONJUNCTION", "tokens": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , PK-closed , AS-open , AS-closed , [[ HK-open ]] , << HK-closed >> , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["HK-open"], "t": ["HK-closed"]}, {"label": "HYPONYM-OF", "tokens": "The system participated in all the tracks of the << segmentation bakeoff >> -- PK-open , PK-closed , AS-open , AS-closed , HK-open , [[ HK-closed ]] , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["HK-closed"], "t": ["segmentation bakeoff"]}, {"label": "CONJUNCTION", "tokens": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , PK-closed , AS-open , AS-closed , HK-open , [[ HK-closed ]] , << MSR-open >> and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["HK-closed"], "t": ["MSR-open"]}, {"label": "HYPONYM-OF", "tokens": "The system participated in all the tracks of the << segmentation bakeoff >> -- PK-open , PK-closed , AS-open , AS-closed , HK-open , HK-closed , [[ MSR-open ]] and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["MSR-open"], "t": ["segmentation bakeoff"]}, {"label": "CONJUNCTION", "tokens": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , PK-closed , AS-open , AS-closed , HK-open , HK-closed , [[ MSR-open ]] and << MSR - closed >> -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["MSR-open"], "t": ["MSR - closed"]}, {"label": "HYPONYM-OF", "tokens": "The system participated in all the tracks of the << segmentation bakeoff >> -- PK-open , PK-closed , AS-open , AS-closed , HK-open , HK-closed , MSR-open and [[ MSR - closed ]] -- and achieved the state-of-the-art performance in MSR-open , MSR-close and PK-open tracks .", "h": ["MSR - closed"], "t": ["segmentation bakeoff"]}, {"label": "CONJUNCTION", "tokens": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , PK-closed , AS-open , AS-closed , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in [[ MSR-open ]] , << MSR-close >> and PK-open tracks .", "h": ["MSR-open"], "t": ["MSR-close"]}, {"label": "CONJUNCTION", "tokens": "The system participated in all the tracks of the segmentation bakeoff -- PK-open , PK-closed , AS-open , AS-closed , HK-open , HK-closed , MSR-open and MSR - closed -- and achieved the state-of-the-art performance in MSR-open , [[ MSR-close ]] and << PK-open >> tracks .", "h": ["MSR-close"], "t": ["PK-open"]}, {"label": "USED-FOR", "tokens": "This paper describes a [[ method ]] of << interactively visualizing and directing the process of translating >> a sentence .", "h": ["method"], "t": ["interactively visualizing and directing the process of translating"]}, {"label": "USED-FOR", "tokens": "The [[ method ]] allows a user to explore a << model >> of syntax-based statistical machine translation -LRB- MT -RRB- , to understand the model 's strengths and weaknesses , and to compare it to other MT systems .", "h": ["method"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "The method allows a user to explore a [[ model ]] of << syntax-based statistical machine translation -LRB- MT -RRB- >> , to understand the model 's strengths and weaknesses , and to compare it to other MT systems .", "h": ["model"], "t": ["syntax-based statistical machine translation -LRB- MT -RRB-"]}, {"label": "COMPARE", "tokens": "The method allows a user to explore a model of syntax-based statistical machine translation -LRB- MT -RRB- , to understand the model 's strengths and weaknesses , and to compare [[ it ]] to other << MT systems >> .", "h": ["it"], "t": ["MT systems"]}, {"label": "USED-FOR", "tokens": "Using this [[ visualization method ]] , we can find and address conceptual and practical problems in an << MT system >> .", "h": ["visualization method"], "t": ["MT system"]}, {"label": "USED-FOR", "tokens": "A [[ method ]] of << sense resolution >> is proposed that is based on WordNet , an on-line lexical database that incorporates semantic relations -LRB- synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["method"], "t": ["sense resolution"]}, {"label": "USED-FOR", "tokens": "A << method >> of sense resolution is proposed that is based on [[ WordNet ]] , an on-line lexical database that incorporates semantic relations -LRB- synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["WordNet"], "t": ["method"]}, {"label": "HYPONYM-OF", "tokens": "A method of sense resolution is proposed that is based on [[ WordNet ]] , an << on-line lexical database >> that incorporates semantic relations -LRB- synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["WordNet"], "t": ["on-line lexical database"]}, {"label": "PART-OF", "tokens": "A method of sense resolution is proposed that is based on << WordNet >> , an on-line lexical database that incorporates [[ semantic relations ]] -LRB- synonymy , antonymy , hyponymy , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["semantic relations"], "t": ["WordNet"]}, {"label": "HYPONYM-OF", "tokens": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates << semantic relations >> -LRB- [[ synonymy ]] , antonymy , hyponymy , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["synonymy"], "t": ["semantic relations"]}, {"label": "CONJUNCTION", "tokens": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates semantic relations -LRB- [[ synonymy ]] , << antonymy >> , hyponymy , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["synonymy"], "t": ["antonymy"]}, {"label": "HYPONYM-OF", "tokens": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates << semantic relations >> -LRB- synonymy , [[ antonymy ]] , hyponymy , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["antonymy"], "t": ["semantic relations"]}, {"label": "CONJUNCTION", "tokens": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates semantic relations -LRB- synonymy , [[ antonymy ]] , << hyponymy >> , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["antonymy"], "t": ["hyponymy"]}, {"label": "HYPONYM-OF", "tokens": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates << semantic relations >> -LRB- synonymy , antonymy , [[ hyponymy ]] , meronymy , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["hyponymy"], "t": ["semantic relations"]}, {"label": "CONJUNCTION", "tokens": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates semantic relations -LRB- synonymy , antonymy , [[ hyponymy ]] , << meronymy >> , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["hyponymy"], "t": ["meronymy"]}, {"label": "HYPONYM-OF", "tokens": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates << semantic relations >> -LRB- synonymy , antonymy , hyponymy , [[ meronymy ]] , causal and troponymic entailment -RRB- as labeled pointers between word senses .", "h": ["meronymy"], "t": ["semantic relations"]}, {"label": "CONJUNCTION", "tokens": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates semantic relations -LRB- synonymy , antonymy , hyponymy , [[ meronymy ]] , << causal and troponymic entailment >> -RRB- as labeled pointers between word senses .", "h": ["meronymy"], "t": ["causal and troponymic entailment"]}, {"label": "HYPONYM-OF", "tokens": "A method of sense resolution is proposed that is based on WordNet , an on-line lexical database that incorporates << semantic relations >> -LRB- synonymy , antonymy , hyponymy , meronymy , [[ causal and troponymic entailment ]] -RRB- as labeled pointers between word senses .", "h": ["causal and troponymic entailment"], "t": ["semantic relations"]}, {"label": "USED-FOR", "tokens": "With [[ WordNet ]] , it is easy to retrieve sets of << semantically related words >> , a facility that will be used for sense resolution during text processing , as follows .", "h": ["WordNet"], "t": ["semantically related words"]}, {"label": "USED-FOR", "tokens": "With WordNet , it is easy to retrieve sets of [[ semantically related words ]] , a facility that will be used for << sense resolution >> during text processing , as follows .", "h": ["semantically related words"], "t": ["sense resolution"]}, {"label": "USED-FOR", "tokens": "With WordNet , it is easy to retrieve sets of semantically related words , a facility that will be used for [[ sense resolution ]] during << text processing >> , as follows .", "h": ["sense resolution"], "t": ["text processing"]}, {"label": "USED-FOR", "tokens": "Or , -LRB- 2 -RRB- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; [[ WordNet ]] will then be used to estimate the << semantic distance >> from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this procedure could have practical applications to problems of information retrieval , mechanical translation , intelligent tutoring systems , and elsewhere .", "h": ["WordNet"], "t": ["semantic distance"]}, {"label": "USED-FOR", "tokens": "Or , -LRB- 2 -RRB- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this [[ procedure ]] could have practical applications to problems of << information retrieval >> , mechanical translation , intelligent tutoring systems , and elsewhere .", "h": ["procedure"], "t": ["information retrieval"]}, {"label": "USED-FOR", "tokens": "Or , -LRB- 2 -RRB- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this [[ procedure ]] could have practical applications to problems of information retrieval , << mechanical translation >> , intelligent tutoring systems , and elsewhere .", "h": ["procedure"], "t": ["mechanical translation"]}, {"label": "USED-FOR", "tokens": "Or , -LRB- 2 -RRB- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this [[ procedure ]] could have practical applications to problems of information retrieval , mechanical translation , << intelligent tutoring systems >> , and elsewhere .", "h": ["procedure"], "t": ["intelligent tutoring systems"]}, {"label": "CONJUNCTION", "tokens": "Or , -LRB- 2 -RRB- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this procedure could have practical applications to problems of [[ information retrieval ]] , << mechanical translation >> , intelligent tutoring systems , and elsewhere .", "h": ["information retrieval"], "t": ["mechanical translation"]}, {"label": "CONJUNCTION", "tokens": "Or , -LRB- 2 -RRB- the context of the polysemous word will be used as a key to search a large corpus ; all words found to occur in that context will be noted ; WordNet will then be used to estimate the semantic distance from those words to the alternative senses of the polysemous word ; and that sense will be chosen that is closest in meaning to other words occurring in the same context If successful , this procedure could have practical applications to problems of information retrieval , [[ mechanical translation ]] , << intelligent tutoring systems >> , and elsewhere .", "h": ["mechanical translation"], "t": ["intelligent tutoring systems"]}, {"label": "USED-FOR", "tokens": "The [[ interlingual approach ]] to << MT >> has been repeatedly advocated by researchers originally interested in natural language understanding who take machine translation to be one possible application .", "h": ["interlingual approach"], "t": ["MT"]}, {"label": "USED-FOR", "tokens": "In contrast , our project , the [[ Mu-project ]] , adopts the transfer approach as the basic framework of << MT >> .", "h": ["Mu-project"], "t": ["MT"]}, {"label": "USED-FOR", "tokens": "In contrast , our project , the << Mu-project >> , adopts the [[ transfer approach ]] as the basic framework of MT .", "h": ["transfer approach"], "t": ["Mu-project"]}, {"label": "PART-OF", "tokens": "This paper describes the detailed construction of the [[ transfer phase ]] of our << system >> from Japanese to English , and gives some examples of problems which seem difficult to treat in the interlingual approach .", "h": ["transfer phase"], "t": ["system"]}, {"label": "PART-OF", "tokens": "The basic design principles of the [[ transfer phase ]] of our << system >> have already been mentioned in -LRB- 1 -RRB- -LRB- 2 -RRB- .", "h": ["transfer phase"], "t": ["system"]}, {"label": "PART-OF", "tokens": "Some of the << principles >> which are relevant to the topic of this paper are : -LRB- a -RRB- [[ Multiple Layer of Grammars ]] -LRB- b -RRB- Multiple Layer Presentation -LRB- c -RRB- Lexicon Driven Processing -LRB- d -RRB- Form-Oriented Dictionary Description .", "h": ["Multiple Layer of Grammars"], "t": ["principles"]}, {"label": "CONJUNCTION", "tokens": "Some of the principles which are relevant to the topic of this paper are : -LRB- a -RRB- [[ Multiple Layer of Grammars ]] -LRB- b -RRB- << Multiple Layer Presentation >> -LRB- c -RRB- Lexicon Driven Processing -LRB- d -RRB- Form-Oriented Dictionary Description .", "h": ["Multiple Layer of Grammars"], "t": ["Multiple Layer Presentation"]}, {"label": "PART-OF", "tokens": "Some of the << principles >> which are relevant to the topic of this paper are : -LRB- a -RRB- Multiple Layer of Grammars -LRB- b -RRB- [[ Multiple Layer Presentation ]] -LRB- c -RRB- Lexicon Driven Processing -LRB- d -RRB- Form-Oriented Dictionary Description .", "h": ["Multiple Layer Presentation"], "t": ["principles"]}, {"label": "CONJUNCTION", "tokens": "Some of the principles which are relevant to the topic of this paper are : -LRB- a -RRB- Multiple Layer of Grammars -LRB- b -RRB- [[ Multiple Layer Presentation ]] -LRB- c -RRB- << Lexicon Driven Processing >> -LRB- d -RRB- Form-Oriented Dictionary Description .", "h": ["Multiple Layer Presentation"], "t": ["Lexicon Driven Processing"]}, {"label": "PART-OF", "tokens": "Some of the << principles >> which are relevant to the topic of this paper are : -LRB- a -RRB- Multiple Layer of Grammars -LRB- b -RRB- Multiple Layer Presentation -LRB- c -RRB- [[ Lexicon Driven Processing ]] -LRB- d -RRB- Form-Oriented Dictionary Description .", "h": ["Lexicon Driven Processing"], "t": ["principles"]}, {"label": "CONJUNCTION", "tokens": "Some of the principles which are relevant to the topic of this paper are : -LRB- a -RRB- Multiple Layer of Grammars -LRB- b -RRB- Multiple Layer Presentation -LRB- c -RRB- [[ Lexicon Driven Processing ]] -LRB- d -RRB- << Form-Oriented Dictionary Description >> .", "h": ["Lexicon Driven Processing"], "t": ["Form-Oriented Dictionary Description"]}, {"label": "PART-OF", "tokens": "Some of the << principles >> which are relevant to the topic of this paper are : -LRB- a -RRB- Multiple Layer of Grammars -LRB- b -RRB- Multiple Layer Presentation -LRB- c -RRB- Lexicon Driven Processing -LRB- d -RRB- [[ Form-Oriented Dictionary Description ]] .", "h": ["Form-Oriented Dictionary Description"], "t": ["principles"]}, {"label": "PART-OF", "tokens": "This paper also shows how these [[ principles ]] are realized in the current << system >> .", "h": ["principles"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "In this paper discourse segments are defined and a [[ method ]] for << discourse segmentation >> primarily based on abduction of temporal relations between segments is proposed .", "h": ["method"], "t": ["discourse segmentation"]}, {"label": "USED-FOR", "tokens": "In this paper discourse segments are defined and a method for << discourse segmentation >> primarily based on [[ abduction of temporal relations ]] between segments is proposed .", "h": ["abduction of temporal relations"], "t": ["discourse segmentation"]}, {"label": "USED-FOR", "tokens": "This << method >> is precise and computationally feasible and is supported by previous work in the area of [[ temporal anaphora resolution ]] .", "h": ["temporal anaphora resolution"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "This paper describes to what extent << deep processing >> may benefit from [[ shallow techniques ]] and it presents a NLP system which integrates a linguistic PoS tagger and chunker as a preprocessing module of a broad coverage unification based grammar of Spanish .", "h": ["shallow techniques"], "t": ["deep processing"]}, {"label": "PART-OF", "tokens": "This paper describes to what extent deep processing may benefit from shallow techniques and it presents a NLP system which integrates a [[ linguistic PoS tagger and chunker ]] as a preprocessing module of a << broad coverage unification based grammar of Spanish >> .", "h": ["linguistic PoS tagger and chunker"], "t": ["broad coverage unification based grammar of Spanish"]}, {"label": "USED-FOR", "tokens": "This paper describes to what extent deep processing may benefit from shallow techniques and it presents a << NLP system >> which integrates a linguistic PoS tagger and chunker as a preprocessing module of a [[ broad coverage unification based grammar of Spanish ]] .", "h": ["broad coverage unification based grammar of Spanish"], "t": ["NLP system"]}, {"label": "USED-FOR", "tokens": "Experiments show that the efficiency of the overall analysis improves significantly and that our [[ system ]] also provides robustness to the << linguistic processing >> while maintaining both the accuracy and the precision of the grammar .", "h": ["system"], "t": ["linguistic processing"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments show that the efficiency of the overall analysis improves significantly and that our << system >> also provides [[ robustness ]] to the linguistic processing while maintaining both the accuracy and the precision of the grammar .", "h": ["robustness"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments show that the efficiency of the overall analysis improves significantly and that our << system >> also provides robustness to the linguistic processing while maintaining both the [[ accuracy ]] and the precision of the grammar .", "h": ["accuracy"], "t": ["system"]}, {"label": "CONJUNCTION", "tokens": "Experiments show that the efficiency of the overall analysis improves significantly and that our system also provides robustness to the linguistic processing while maintaining both the [[ accuracy ]] and the << precision >> of the grammar .", "h": ["accuracy"], "t": ["precision"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments show that the efficiency of the overall analysis improves significantly and that our << system >> also provides robustness to the linguistic processing while maintaining both the accuracy and the [[ precision ]] of the grammar .", "h": ["precision"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "[[ Joint image filters ]] can leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for << suppressing noise >> or enhancing spatial resolution .", "h": ["Joint image filters"], "t": ["suppressing noise"]}, {"label": "USED-FOR", "tokens": "[[ Joint image filters ]] can leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or << enhancing spatial resolution >> .", "h": ["Joint image filters"], "t": ["enhancing spatial resolution"]}, {"label": "USED-FOR", "tokens": "<< Joint image filters >> can leverage the [[ guidance image ]] as a prior and transfer the structural details from the guidance image to the target image for suppressing noise or enhancing spatial resolution .", "h": ["guidance image"], "t": ["Joint image filters"]}, {"label": "CONJUNCTION", "tokens": "Joint image filters can leverage the guidance image as a prior and transfer the structural details from the guidance image to the target image for [[ suppressing noise ]] or << enhancing spatial resolution >> .", "h": ["suppressing noise"], "t": ["enhancing spatial resolution"]}, {"label": "USED-FOR", "tokens": "<< Existing methods >> rely on various kinds of [[ explicit filter construction ]] or hand-designed objective functions .", "h": ["explicit filter construction"], "t": ["Existing methods"]}, {"label": "CONJUNCTION", "tokens": "Existing methods rely on various kinds of [[ explicit filter construction ]] or << hand-designed objective functions >> .", "h": ["explicit filter construction"], "t": ["hand-designed objective functions"]}, {"label": "USED-FOR", "tokens": "<< Existing methods >> rely on various kinds of explicit filter construction or [[ hand-designed objective functions ]] .", "h": ["hand-designed objective functions"], "t": ["Existing methods"]}, {"label": "USED-FOR", "tokens": "It is thus difficult to understand , improve , and accelerate << them >> in a [[ coherent framework ]] .", "h": ["coherent framework"], "t": ["them"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a [[ learning-based approach ]] to construct a << joint filter >> based on Convolution-al Neural Networks .", "h": ["learning-based approach"], "t": ["joint filter"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a << learning-based approach >> to construct a joint filter based on [[ Convolution-al Neural Networks ]] .", "h": ["Convolution-al Neural Networks"], "t": ["learning-based approach"]}, {"label": "COMPARE", "tokens": "In contrast to existing [[ methods ]] that consider only the guidance image , our << method >> can selectively transfer salient structures that are consistent in both guidance and target images .", "h": ["methods"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "In contrast to existing << methods >> that consider only the [[ guidance image ]] , our method can selectively transfer salient structures that are consistent in both guidance and target images .", "h": ["guidance image"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "In contrast to existing methods that consider only the guidance image , our [[ method ]] can selectively << transfer salient structures >> that are consistent in both guidance and target images .", "h": ["method"], "t": ["transfer salient structures"]}, {"label": "USED-FOR", "tokens": "We show that the [[ model ]] trained on a certain type of data , e.g. , RGB and depth images , generalizes well for other << modalities >> , e.g. , Flash/Non-Flash and RGB/NIR images .", "h": ["model"], "t": ["modalities"]}, {"label": "USED-FOR", "tokens": "We show that the << model >> trained on a certain type of [[ data ]] , e.g. , RGB and depth images , generalizes well for other modalities , e.g. , Flash/Non-Flash and RGB/NIR images .", "h": ["data"], "t": ["model"]}, {"label": "HYPONYM-OF", "tokens": "We show that the model trained on a certain type of << data >> , e.g. , [[ RGB and depth images ]] , generalizes well for other modalities , e.g. , Flash/Non-Flash and RGB/NIR images .", "h": ["RGB and depth images"], "t": ["data"]}, {"label": "HYPONYM-OF", "tokens": "We show that the model trained on a certain type of data , e.g. , RGB and depth images , generalizes well for other << modalities >> , e.g. , [[ Flash/Non-Flash and RGB/NIR images ]] .", "h": ["Flash/Non-Flash and RGB/NIR images"], "t": ["modalities"]}, {"label": "COMPARE", "tokens": "We validate the effectiveness of the proposed [[ joint filter ]] through extensive comparisons with << state-of-the-art methods >> .", "h": ["joint filter"], "t": ["state-of-the-art methods"]}, {"label": "USED-FOR", "tokens": "In our current research into the design of << cognitively well-motivated interfaces >> relying primarily on the [[ display of graphical information ]] , we have observed that graphical information alone does not provide sufficient support to users - particularly when situations arise that do not simply conform to the users ' expectations .", "h": ["display of graphical information"], "t": ["cognitively well-motivated interfaces"]}, {"label": "USED-FOR", "tokens": "To solve this problem , we are working towards the integration of [[ natural language generation ]] to augment the << interaction >>", "h": ["natural language generation"], "t": ["interaction"]}, {"label": "USED-FOR", "tokens": "A central problem of word sense disambiguation -LRB- WSD -RRB- is the lack of [[ manually sense-tagged data ]] required for << supervised learning >> .", "h": ["manually sense-tagged data"], "t": ["supervised learning"]}, {"label": "USED-FOR", "tokens": "In this paper , we evaluate an [[ approach ]] to automatically acquire << sense-tagged training data >> from English-Chinese parallel corpora , which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task .", "h": ["approach"], "t": ["sense-tagged training data"]}, {"label": "USED-FOR", "tokens": "In this paper , we evaluate an [[ approach ]] to automatically acquire sense-tagged training data from English-Chinese parallel corpora , which are then used for disambiguating the << nouns >> in the SENSEVAL-2 English lexical sample task .", "h": ["approach"], "t": ["nouns"]}, {"label": "USED-FOR", "tokens": "In this paper , we evaluate an approach to automatically acquire << sense-tagged training data >> from [[ English-Chinese parallel corpora ]] , which are then used for disambiguating the nouns in the SENSEVAL-2 English lexical sample task .", "h": ["English-Chinese parallel corpora"], "t": ["sense-tagged training data"]}, {"label": "PART-OF", "tokens": "In this paper , we evaluate an approach to automatically acquire sense-tagged training data from English-Chinese parallel corpora , which are then used for disambiguating the [[ nouns ]] in the << SENSEVAL-2 English lexical sample task >> .", "h": ["nouns"], "t": ["SENSEVAL-2 English lexical sample task"]}, {"label": "USED-FOR", "tokens": "Our investigation reveals that this [[ method ]] of << acquiring sense-tagged data >> is promising .", "h": ["method"], "t": ["acquiring sense-tagged data"]}, {"label": "FEATURE-OF", "tokens": "On a subset of the most difficult SENSEVAL-2 nouns , the accuracy difference between the two approaches is only 14.0 % , and the difference could narrow further to 6.5 % if we disregard the advantage that << manually sense-tagged data >> have in their [[ sense coverage ]] .", "h": ["sense coverage"], "t": ["manually sense-tagged data"]}, {"label": "FEATURE-OF", "tokens": "Our analysis also highlights the importance of the issue of [[ domain dependence ]] in << evaluating WSD programs >> .", "h": ["domain dependence"], "t": ["evaluating WSD programs"]}, {"label": "PART-OF", "tokens": "This paper presents an analysis of << temporal anaphora >> in sentences which contain [[ quantification over events ]] , within the framework of Discourse Representation Theory .", "h": ["quantification over events"], "t": ["temporal anaphora"]}, {"label": "USED-FOR", "tokens": "This paper presents an analysis of << temporal anaphora >> in sentences which contain quantification over events , within the framework of [[ Discourse Representation Theory ]] .", "h": ["Discourse Representation Theory"], "t": ["temporal anaphora"]}, {"label": "PART-OF", "tokens": "The analysis in -LRB- Partee , 1984 -RRB- of quantified sentences , introduced by a temporal connective , gives the wrong truth-conditions when the [[ temporal connective ]] in the << subordinate clause >> is before or after .", "h": ["temporal connective"], "t": ["subordinate clause"]}, {"label": "USED-FOR", "tokens": "This << problem >> has been previously analyzed in -LRB- de Swart , 1991 -RRB- as an instance of the proportion problem and given a solution from a [[ Generalized Quantifier approach ]] .", "h": ["Generalized Quantifier approach"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "By using a careful distinction between the different notions of reference time based on -LRB- Kamp and Reyle , 1993 -RRB- , we propose a [[ solution ]] to this << problem >> , within the framework of DRT .", "h": ["solution"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "By using a careful distinction between the different notions of reference time based on -LRB- Kamp and Reyle , 1993 -RRB- , we propose a solution to this << problem >> , within the framework of [[ DRT ]] .", "h": ["DRT"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "We show some applications of this [[ solution ]] to additional << temporal anaphora phenomena >> in quantified sentences .", "h": ["solution"], "t": ["temporal anaphora phenomena"]}, {"label": "USED-FOR", "tokens": "We show some applications of this << solution >> to additional temporal anaphora phenomena in [[ quantified sentences ]] .", "h": ["quantified sentences"], "t": ["solution"]}, {"label": "USED-FOR", "tokens": "In this paper , we explore [[ correlation of dependency relation paths ]] to rank candidate answers in << answer extraction >> .", "h": ["correlation of dependency relation paths"], "t": ["answer extraction"]}, {"label": "USED-FOR", "tokens": "Using the [[ correlation measure ]] , we compare << dependency relations >> of a candidate answer and mapped question phrases in sentence with the corresponding relations in question .", "h": ["correlation measure"], "t": ["dependency relations"]}, {"label": "PART-OF", "tokens": "Different from previous studies , we propose an approximate phrase mapping algorithm and incorporate the [[ mapping score ]] into the << correlation measure >> .", "h": ["mapping score"], "t": ["correlation measure"]}, {"label": "PART-OF", "tokens": "The [[ correlations ]] are further incorporated into a << Maximum Entropy-based ranking model >> which estimates path weights from training .", "h": ["correlations"], "t": ["Maximum Entropy-based ranking model"]}, {"label": "COMPARE", "tokens": "Experimental results show that our [[ method ]] significantly outperforms state-of-the-art << syntactic relation-based methods >> by up to 20 % in MRR .", "h": ["method"], "t": ["syntactic relation-based methods"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results show that our << method >> significantly outperforms state-of-the-art syntactic relation-based methods by up to 20 % in [[ MRR ]] .", "h": ["MRR"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results show that our method significantly outperforms state-of-the-art << syntactic relation-based methods >> by up to 20 % in [[ MRR ]] .", "h": ["MRR"], "t": ["syntactic relation-based methods"]}, {"label": "USED-FOR", "tokens": "[[ Evaluation ]] is also crucial to assessing competing claims and identifying promising technical << approaches >> .", "h": ["Evaluation"], "t": ["approaches"]}, {"label": "EVALUATE-FOR", "tokens": "Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems -LRB- SLS -RRB- program to agree on a [[ methodology ]] for comparative << evaluation of SLS systems >> , and that methodology has been put into practice several times in comparative tests of several SLS systems .", "h": ["methodology"], "t": ["evaluation of SLS systems"]}, {"label": "EVALUATE-FOR", "tokens": "Recently considerable progress has been made by a number of groups involved in the DARPA Spoken Language Systems -LRB- SLS -RRB- program to agree on a methodology for comparative evaluation of SLS systems , and that [[ methodology ]] has been put into practice several times in comparative tests of several << SLS systems >> .", "h": ["methodology"], "t": ["SLS systems"]}, {"label": "HYPONYM-OF", "tokens": "These [[ evaluations ]] are probably the only << NL evaluations >> other than the series of Message Understanding Conferences -LRB- Sundheim , 1989 ; Sundheim , 1991 -RRB- to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems -LRB- Palmer et al. , 1989 ; Neal et al. , 1991 -RRB- .", "h": ["evaluations"], "t": ["NL evaluations"]}, {"label": "CONJUNCTION", "tokens": "These [[ evaluations ]] are probably the only NL evaluations other than the series of << Message Understanding Conferences >> -LRB- Sundheim , 1989 ; Sundheim , 1991 -RRB- to have been developed and used by a group of researchers at different sites , although several excellent workshops have been held to study some of these problems -LRB- Palmer et al. , 1989 ; Neal et al. , 1991 -RRB- .", "h": ["evaluations"], "t": ["Message Understanding Conferences"]}, {"label": "EVALUATE-FOR", "tokens": "This paper describes a practical [[ `` black-box '' methodology ]] for << automatic evaluation of question-answering NL systems >> .", "h": ["`` black-box '' methodology"], "t": ["automatic evaluation of question-answering NL systems"]}, {"label": "USED-FOR", "tokens": "While each new application domain will require some development of special resources , the heart of the methodology is domain-independent , and << it >> can be used with either [[ speech or text input ]] .", "h": ["speech or text input"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "In this paper we present a novel [[ autonomous pipeline ]] to build a << personalized parametric model -LRB- pose-driven avatar -RRB- >> using a single depth sensor .", "h": ["autonomous pipeline"], "t": ["personalized parametric model -LRB- pose-driven avatar -RRB-"]}, {"label": "USED-FOR", "tokens": "In this paper we present a novel << autonomous pipeline >> to build a personalized parametric model -LRB- pose-driven avatar -RRB- using a [[ single depth sensor ]] .", "h": ["single depth sensor"], "t": ["autonomous pipeline"]}, {"label": "USED-FOR", "tokens": "We fit each incomplete scan using << template fitting techniques >> with a generic [[ human template ]] , and register all scans to every pose using global consistency constraints .", "h": ["human template"], "t": ["template fitting techniques"]}, {"label": "USED-FOR", "tokens": "After registration , these [[ watertight models ]] with different poses are used to train a << parametric model >> in a fashion similar to the SCAPE method .", "h": ["watertight models"], "t": ["parametric model"]}, {"label": "USED-FOR", "tokens": "After registration , these watertight models with different poses are used to train a << parametric model >> in a fashion similar to the [[ SCAPE method ]] .", "h": ["SCAPE method"], "t": ["parametric model"]}, {"label": "USED-FOR", "tokens": "Once the parametric model is built , [[ it ]] can be used as an << anim-itable avatar >> or more interestingly synthesizing dynamic 3D models from single-view depth videos .", "h": ["it"], "t": ["anim-itable avatar"]}, {"label": "USED-FOR", "tokens": "Once the parametric model is built , [[ it ]] can be used as an anim-itable avatar or more interestingly synthesizing << dynamic 3D models >> from single-view depth videos .", "h": ["it"], "t": ["dynamic 3D models"]}, {"label": "USED-FOR", "tokens": "Once the parametric model is built , it can be used as an anim-itable avatar or more interestingly synthesizing << dynamic 3D models >> from [[ single-view depth videos ]] .", "h": ["single-view depth videos"], "t": ["dynamic 3D models"]}, {"label": "USED-FOR", "tokens": "Experimental results demonstrate the effectiveness of our [[ system ]] to produce << dynamic models >> .", "h": ["system"], "t": ["dynamic models"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a novel [[ algorithm ]] to detect/compensate << on-line interference effects >> when integrating Global Navigation Satellite System -LRB- GNSS -RRB- and Inertial Navigation System -LRB- INS -RRB- .", "h": ["algorithm"], "t": ["on-line interference effects"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we propose a novel algorithm to detect/compensate on-line interference effects when integrating [[ Global Navigation Satellite System -LRB- GNSS -RRB- ]] and << Inertial Navigation System -LRB- INS -RRB- >> .", "h": ["Global Navigation Satellite System -LRB- GNSS -RRB-"], "t": ["Inertial Navigation System -LRB- INS -RRB-"]}, {"label": "USED-FOR", "tokens": "The << GNSS/INS coupling >> is usually performed by an [[ Extended Kalman Filter -LRB- EKF -RRB- ]] which yields an accurate and robust localization .", "h": ["Extended Kalman Filter -LRB- EKF -RRB-"], "t": ["GNSS/INS coupling"]}, {"label": "USED-FOR", "tokens": "The GNSS/INS coupling is usually performed by an [[ Extended Kalman Filter -LRB- EKF -RRB- ]] which yields an << accurate and robust localization >> .", "h": ["Extended Kalman Filter -LRB- EKF -RRB-"], "t": ["accurate and robust localization"]}, {"label": "FEATURE-OF", "tokens": "We first study the impact of the GNSS noise inflation on the << covariance >> of the [[ EKF outputs ]] so as to compute a least square estimate of the potential variance jumps .", "h": ["EKF outputs"], "t": ["covariance"]}, {"label": "USED-FOR", "tokens": "We first study the impact of the GNSS noise inflation on the covariance of the EKF outputs so as to compute a [[ least square estimate ]] of the potential << variance jumps >> .", "h": ["least square estimate"], "t": ["variance jumps"]}, {"label": "USED-FOR", "tokens": "Then , this [[ estimation ]] is used in a << Bayesian test >> which decides whether interference are corrupting the GNSS signal or not .", "h": ["estimation"], "t": ["Bayesian test"]}, {"label": "EVALUATE-FOR", "tokens": "The results show the performance of the proposed << approach >> on [[ simulated data ]] .", "h": ["simulated data"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "We propose a [[ unified variational formulation ]] for << joint motion estimation and segmentation >> with explicit occlusion handling .", "h": ["unified variational formulation"], "t": ["joint motion estimation and segmentation"]}, {"label": "USED-FOR", "tokens": "We propose a << unified variational formulation >> for joint motion estimation and segmentation with [[ explicit occlusion handling ]] .", "h": ["explicit occlusion handling"], "t": ["unified variational formulation"]}, {"label": "USED-FOR", "tokens": "We use a [[ convex formulation ]] of the << multi-label Potts model >> with label costs and show that the asymmetric map-uniqueness criterion can be integrated into our formulation by means of convex constraints .", "h": ["convex formulation"], "t": ["multi-label Potts model"]}, {"label": "PART-OF", "tokens": "We use a convex formulation of the multi-label Potts model with label costs and show that the [[ asymmetric map-uniqueness criterion ]] can be integrated into our << formulation >> by means of convex constraints .", "h": ["asymmetric map-uniqueness criterion"], "t": ["formulation"]}, {"label": "USED-FOR", "tokens": "We use a convex formulation of the multi-label Potts model with label costs and show that the asymmetric map-uniqueness criterion can be integrated into our << formulation >> by means of [[ convex constraints ]] .", "h": ["convex constraints"], "t": ["formulation"]}, {"label": "USED-FOR", "tokens": "By using a fast [[ primal-dual algorithm ]] we are able to handle several hundred << motion segments >> .", "h": ["primal-dual algorithm"], "t": ["motion segments"]}, {"label": "USED-FOR", "tokens": "Two main classes of [[ approaches ]] have been studied to perform << monocular nonrigid 3D reconstruction >> : Template-based methods and Non-rigid Structure from Motion techniques .", "h": ["approaches"], "t": ["monocular nonrigid 3D reconstruction"]}, {"label": "HYPONYM-OF", "tokens": "Two main classes of << approaches >> have been studied to perform monocular nonrigid 3D reconstruction : [[ Template-based methods ]] and Non-rigid Structure from Motion techniques .", "h": ["Template-based methods"], "t": ["approaches"]}, {"label": "CONJUNCTION", "tokens": "Two main classes of approaches have been studied to perform monocular nonrigid 3D reconstruction : [[ Template-based methods ]] and << Non-rigid Structure from Motion techniques >> .", "h": ["Template-based methods"], "t": ["Non-rigid Structure from Motion techniques"]}, {"label": "HYPONYM-OF", "tokens": "Two main classes of << approaches >> have been studied to perform monocular nonrigid 3D reconstruction : Template-based methods and [[ Non-rigid Structure from Motion techniques ]] .", "h": ["Non-rigid Structure from Motion techniques"], "t": ["approaches"]}, {"label": "USED-FOR", "tokens": "While the first [[ ones ]] have been applied to reconstruct << poorly-textured surfaces >> , they assume the availability of a 3D shape model prior to reconstruction .", "h": ["ones"], "t": ["poorly-textured surfaces"]}, {"label": "USED-FOR", "tokens": "While the first ones have been applied to reconstruct poorly-textured surfaces , << they >> assume the availability of a [[ 3D shape model ]] prior to reconstruction .", "h": ["3D shape model"], "t": ["they"]}, {"label": "USED-FOR", "tokens": "In this paper , we introduce a [[ template-free approach ]] to reconstructing a << poorly-textured , deformable surface >> .", "h": ["template-free approach"], "t": ["poorly-textured , deformable surface"]}, {"label": "USED-FOR", "tokens": "To this end , we leverage [[ surface isometry ]] and formulate << 3D reconstruction >> as the joint problem of non-rigid image registration and depth estimation .", "h": ["surface isometry"], "t": ["3D reconstruction"]}, {"label": "USED-FOR", "tokens": "To this end , we leverage surface isometry and formulate << 3D reconstruction >> as the [[ joint problem of non-rigid image registration and depth estimation ]] .", "h": ["joint problem of non-rigid image registration and depth estimation"], "t": ["3D reconstruction"]}, {"label": "COMPARE", "tokens": "Our experiments demonstrate that our [[ approach ]] yields much more accurate 3D reconstructions than << state-of-the-art techniques >> .", "h": ["approach"], "t": ["state-of-the-art techniques"]}, {"label": "EVALUATE-FOR", "tokens": "Our experiments demonstrate that our << approach >> yields much more accurate [[ 3D reconstructions ]] than state-of-the-art techniques .", "h": ["3D reconstructions"], "t": ["approach"]}, {"label": "EVALUATE-FOR", "tokens": "Our experiments demonstrate that our approach yields much more accurate [[ 3D reconstructions ]] than << state-of-the-art techniques >> .", "h": ["3D reconstructions"], "t": ["state-of-the-art techniques"]}, {"label": "HYPONYM-OF", "tokens": "Many << computer vision applications >> , such as [[ image classification ]] and video indexing , are usually multi-label classification problems in which an instance can be assigned to more than one category .", "h": ["image classification"], "t": ["computer vision applications"]}, {"label": "CONJUNCTION", "tokens": "Many computer vision applications , such as [[ image classification ]] and << video indexing >> , are usually multi-label classification problems in which an instance can be assigned to more than one category .", "h": ["image classification"], "t": ["video indexing"]}, {"label": "HYPONYM-OF", "tokens": "Many << computer vision applications >> , such as image classification and [[ video indexing ]] , are usually multi-label classification problems in which an instance can be assigned to more than one category .", "h": ["video indexing"], "t": ["computer vision applications"]}, {"label": "USED-FOR", "tokens": "Many << computer vision applications >> , such as image classification and video indexing , are usually [[ multi-label classification problems ]] in which an instance can be assigned to more than one category .", "h": ["multi-label classification problems"], "t": ["computer vision applications"]}, {"label": "FEATURE-OF", "tokens": "In this paper , we present a novel << multi-label classification approach >> with [[ hypergraph regu-larization ]] that addresses the correlations among different categories .", "h": ["hypergraph regu-larization"], "t": ["multi-label classification approach"]}, {"label": "USED-FOR", "tokens": "Then , an improved [[ SVM like learning system ]] incorporating the hypergraph regularization , called Rank-HLapSVM , is proposed to handle the << multi-label classification problems >> .", "h": ["SVM like learning system"], "t": ["multi-label classification problems"]}, {"label": "PART-OF", "tokens": "Then , an improved << SVM like learning system >> incorporating the [[ hypergraph regularization ]] , called Rank-HLapSVM , is proposed to handle the multi-label classification problems .", "h": ["hypergraph regularization"], "t": ["SVM like learning system"]}, {"label": "HYPONYM-OF", "tokens": "Then , an improved << SVM like learning system >> incorporating the hypergraph regularization , called [[ Rank-HLapSVM ]] , is proposed to handle the multi-label classification problems .", "h": ["Rank-HLapSVM"], "t": ["SVM like learning system"]}, {"label": "USED-FOR", "tokens": "We find that the corresponding << optimization problem >> can be efficiently solved by the [[ dual coordinate descent method ]] .", "h": ["dual coordinate descent method"], "t": ["optimization problem"]}, {"label": "EVALUATE-FOR", "tokens": "Many promising experimental results on the [[ real datasets ]] including ImageCLEF and Me-diaMill demonstrate the effectiveness and efficiency of the proposed << algorithm >> .", "h": ["real datasets"], "t": ["algorithm"]}, {"label": "HYPONYM-OF", "tokens": "Many promising experimental results on the << real datasets >> including [[ ImageCLEF ]] and Me-diaMill demonstrate the effectiveness and efficiency of the proposed algorithm .", "h": ["ImageCLEF"], "t": ["real datasets"]}, {"label": "CONJUNCTION", "tokens": "Many promising experimental results on the real datasets including [[ ImageCLEF ]] and << Me-diaMill >> demonstrate the effectiveness and efficiency of the proposed algorithm .", "h": ["ImageCLEF"], "t": ["Me-diaMill"]}, {"label": "HYPONYM-OF", "tokens": "Many promising experimental results on the << real datasets >> including ImageCLEF and [[ Me-diaMill ]] demonstrate the effectiveness and efficiency of the proposed algorithm .", "h": ["Me-diaMill"], "t": ["real datasets"]}, {"label": "USED-FOR", "tokens": "We derive a [[ convex optimization problem ]] for the task of << segmenting sequential data >> , which explicitly treats presence of outliers .", "h": ["convex optimization problem"], "t": ["segmenting sequential data"]}, {"label": "USED-FOR", "tokens": "We derive a convex optimization problem for the task of [[ segmenting sequential data ]] , which explicitly treats presence of << outliers >> .", "h": ["segmenting sequential data"], "t": ["outliers"]}, {"label": "USED-FOR", "tokens": "We describe two [[ algorithms ]] for solving this << problem >> , one exact and one a top-down novel approach , and we derive a consistency results for the case of two segments and no outliers .", "h": ["algorithms"], "t": ["problem"]}, {"label": "FEATURE-OF", "tokens": "<< Robustness >> to [[ outliers ]] is evaluated on two real-world tasks related to speech segmentation .", "h": ["outliers"], "t": ["Robustness"]}, {"label": "EVALUATE-FOR", "tokens": "<< Robustness >> to outliers is evaluated on two [[ real-world tasks ]] related to speech segmentation .", "h": ["real-world tasks"], "t": ["Robustness"]}, {"label": "EVALUATE-FOR", "tokens": "<< Robustness >> to outliers is evaluated on two real-world tasks related to [[ speech segmentation ]] .", "h": ["speech segmentation"], "t": ["Robustness"]}, {"label": "FEATURE-OF", "tokens": "Robustness to outliers is evaluated on two << real-world tasks >> related to [[ speech segmentation ]] .", "h": ["speech segmentation"], "t": ["real-world tasks"]}, {"label": "COMPARE", "tokens": "Our [[ algorithms ]] outperform << baseline seg-mentation algorithms >> .", "h": ["algorithms"], "t": ["baseline seg-mentation algorithms"]}, {"label": "USED-FOR", "tokens": "This paper examines the properties of << feature-based partial descriptions >> built on top of [[ Halliday 's systemic networks ]] .", "h": ["Halliday 's systemic networks"], "t": ["feature-based partial descriptions"]}, {"label": "USED-FOR", "tokens": "We show that the crucial operation of [[ consistency checking ]] for such << descriptions >> is NP-complete , and therefore probably intractable , but proceed to develop algorithms which can sometimes alleviate the unpleasant consequences of this intractability .", "h": ["consistency checking"], "t": ["descriptions"]}, {"label": "HYPONYM-OF", "tokens": "We describe [[ Yoopick ]] , a << combinatorial sports prediction market >> that implements a flexible betting language , and in turn facilitates fine-grained probabilistic estimation of outcomes .", "h": ["Yoopick"], "t": ["combinatorial sports prediction market"]}, {"label": "USED-FOR", "tokens": "We describe [[ Yoopick ]] , a combinatorial sports prediction market that implements a flexible betting language , and in turn facilitates << fine-grained probabilistic estimation of outcomes >> .", "h": ["Yoopick"], "t": ["fine-grained probabilistic estimation of outcomes"]}, {"label": "USED-FOR", "tokens": "We describe << Yoopick >> , a combinatorial sports prediction market that implements a [[ flexible betting language ]] , and in turn facilitates fine-grained probabilistic estimation of outcomes .", "h": ["flexible betting language"], "t": ["Yoopick"]}, {"label": "USED-FOR", "tokens": "The goal of this paper is to discover a set of [[ discriminative patches ]] which can serve as a fully << unsupervised mid-level visual representation >> .", "h": ["discriminative patches"], "t": ["unsupervised mid-level visual representation"]}, {"label": "USED-FOR", "tokens": "We pose this as an << unsupervised discriminative clustering problem >> on a huge dataset of [[ image patches ]] .", "h": ["image patches"], "t": ["unsupervised discriminative clustering problem"]}, {"label": "USED-FOR", "tokens": "We use an iterative procedure which alternates between clustering and training discriminative classifiers , while applying careful [[ cross-validation ]] at each step to prevent << overfitting >> .", "h": ["cross-validation"], "t": ["overfitting"]}, {"label": "USED-FOR", "tokens": "The paper experimentally demonstrates the effectiveness of [[ discriminative patches ]] as an << unsupervised mid-level visual representation >> , suggesting that it could be used in place of visual words for many tasks .", "h": ["discriminative patches"], "t": ["unsupervised mid-level visual representation"]}, {"label": "USED-FOR", "tokens": "The paper experimentally demonstrates the effectiveness of << discriminative patches >> as an unsupervised mid-level visual representation , suggesting that [[ it ]] could be used in place of visual words for many tasks .", "h": ["it"], "t": ["discriminative patches"]}, {"label": "USED-FOR", "tokens": "Furthermore , [[ discrim-inative patches ]] can also be used in a << supervised regime >> , such as scene classification , where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset .", "h": ["discrim-inative patches"], "t": ["supervised regime"]}, {"label": "HYPONYM-OF", "tokens": "Furthermore , discrim-inative patches can also be used in a << supervised regime >> , such as [[ scene classification ]] , where they demonstrate state-of-the-art performance on the MIT Indoor-67 dataset .", "h": ["scene classification"], "t": ["supervised regime"]}, {"label": "EVALUATE-FOR", "tokens": "Furthermore , discrim-inative patches can also be used in a supervised regime , such as scene classification , where << they >> demonstrate state-of-the-art performance on the [[ MIT Indoor-67 dataset ]] .", "h": ["MIT Indoor-67 dataset"], "t": ["they"]}, {"label": "USED-FOR", "tokens": "We investigate the utility of an [[ algorithm ]] for << translation lexicon acquisition -LRB- SABLE -RRB- >> , used previously on a very large corpus to acquire general translation lexicons , when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons .", "h": ["algorithm"], "t": ["translation lexicon acquisition -LRB- SABLE -RRB-"]}, {"label": "USED-FOR", "tokens": "We investigate the utility of an [[ algorithm ]] for translation lexicon acquisition -LRB- SABLE -RRB- , used previously on a very large corpus to acquire << general translation lexicons >> , when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons .", "h": ["algorithm"], "t": ["general translation lexicons"]}, {"label": "USED-FOR", "tokens": "We investigate the utility of an algorithm for translation lexicon acquisition -LRB- SABLE -RRB- , used previously on a very large corpus to acquire general translation lexicons , when that [[ algorithm ]] is applied to a much smaller corpus to produce candidates for << domain-specific translation lexicons >> .", "h": ["algorithm"], "t": ["domain-specific translation lexicons"]}, {"label": "USED-FOR", "tokens": "This paper describes a [[ computational model ]] of << word segmentation >> and presents simulation results on realistic acquisition .", "h": ["computational model"], "t": ["word segmentation"]}, {"label": "USED-FOR", "tokens": "This paper describes a [[ computational model ]] of word segmentation and presents simulation results on << realistic acquisition >> .", "h": ["computational model"], "t": ["realistic acquisition"]}, {"label": "USED-FOR", "tokens": "In particular , we explore the capacity and limitations of << statistical learning mechanisms >> that have recently gained prominence in [[ cognitive psychology ]] and linguistics .", "h": ["cognitive psychology"], "t": ["statistical learning mechanisms"]}, {"label": "CONJUNCTION", "tokens": "In particular , we explore the capacity and limitations of statistical learning mechanisms that have recently gained prominence in [[ cognitive psychology ]] and << linguistics >> .", "h": ["cognitive psychology"], "t": ["linguistics"]}, {"label": "USED-FOR", "tokens": "In particular , we explore the capacity and limitations of << statistical learning mechanisms >> that have recently gained prominence in cognitive psychology and [[ linguistics ]] .", "h": ["linguistics"], "t": ["statistical learning mechanisms"]}, {"label": "USED-FOR", "tokens": "In the [[ model-based policy search approach ]] to << reinforcement learning -LRB- RL -RRB- >> , policies are found using a model -LRB- or `` simulator '' -RRB- of the Markov decision process .", "h": ["model-based policy search approach"], "t": ["reinforcement learning -LRB- RL -RRB-"]}, {"label": "USED-FOR", "tokens": "In the model-based policy search approach to reinforcement learning -LRB- RL -RRB- , << policies >> are found using a model -LRB- or `` simulator '' -RRB- of the [[ Markov decision process ]] .", "h": ["Markov decision process"], "t": ["policies"]}, {"label": "USED-FOR", "tokens": "However , for << high-dimensional continuous-state tasks >> , it can be extremely difficult to build an accurate [[ model ]] , and thus often the algorithm returns a policy that works in simulation but not in real-life .", "h": ["model"], "t": ["high-dimensional continuous-state tasks"]}, {"label": "USED-FOR", "tokens": "However , for high-dimensional continuous-state tasks , it can be extremely difficult to build an accurate model , and thus often the [[ algorithm ]] returns a << policy >> that works in simulation but not in real-life .", "h": ["algorithm"], "t": ["policy"]}, {"label": "USED-FOR", "tokens": "The other extreme , << model-free RL >> , tends to require infeasibly large numbers of [[ real-life trials ]] .", "h": ["real-life trials"], "t": ["model-free RL"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a << hybrid algorithm >> that requires only an [[ approximate model ]] , and only a small number of real-life trials .", "h": ["approximate model"], "t": ["hybrid algorithm"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a hybrid algorithm that requires only an << approximate model >> , and only a small number of [[ real-life trials ]] .", "h": ["real-life trials"], "t": ["approximate model"]}, {"label": "USED-FOR", "tokens": "The key idea is to successively `` ground '' the << policy evaluations >> using [[ real-life trials ]] , but to rely on the approximate model to suggest local changes .", "h": ["real-life trials"], "t": ["policy evaluations"]}, {"label": "CONJUNCTION", "tokens": "Empirical results also demonstrate that -- when given only a [[ crude model ]] and a small number of << real-life trials >> -- our algorithm can obtain near-optimal performance in the real system .", "h": ["crude model"], "t": ["real-life trials"]}, {"label": "USED-FOR", "tokens": "Empirical results also demonstrate that -- when given only a [[ crude model ]] and a small number of real-life trials -- our << algorithm >> can obtain near-optimal performance in the real system .", "h": ["crude model"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "Empirical results also demonstrate that -- when given only a crude model and a small number of [[ real-life trials ]] -- our << algorithm >> can obtain near-optimal performance in the real system .", "h": ["real-life trials"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "Although every << natural language system >> needs a [[ computational lexicon ]] , each system puts different amounts and types of information into its lexicon according to its individual needs .", "h": ["computational lexicon"], "t": ["natural language system"]}, {"label": "HYPONYM-OF", "tokens": "This paper presents our experience in planning and building [[ COMPLEX ]] , a << computational lexicon >> designed to be a repository of shared lexical information for use by Natural Language Processing -LRB- NLP -RRB- systems .", "h": ["COMPLEX"], "t": ["computational lexicon"]}, {"label": "USED-FOR", "tokens": "This paper presents our experience in planning and building [[ COMPLEX ]] , a computational lexicon designed to be a repository of shared lexical information for use by << Natural Language Processing -LRB- NLP -RRB- systems >> .", "h": ["COMPLEX"], "t": ["Natural Language Processing -LRB- NLP -RRB- systems"]}, {"label": "PART-OF", "tokens": "Sentence planning is a set of inter-related but distinct << tasks >> , one of which is [[ sentence scoping ]] , i.e. the choice of syntactic structure for elementary speech acts and the decision of how to combine them into one or more sentences .", "h": ["sentence scoping"], "t": ["tasks"]}, {"label": "USED-FOR", "tokens": "Sentence planning is a set of inter-related but distinct tasks , one of which is sentence scoping , i.e. the choice of [[ syntactic structure ]] for elementary << speech acts >> and the decision of how to combine them into one or more sentences .", "h": ["syntactic structure"], "t": ["speech acts"]}, {"label": "HYPONYM-OF", "tokens": "In this paper , we present [[ SPoT ]] , a << sentence planner >> , and a new methodology for automatically training SPoT on the basis of feedback provided by human judges .", "h": ["SPoT"], "t": ["sentence planner"]}, {"label": "USED-FOR", "tokens": "In this paper , we present SPoT , a sentence planner , and a new [[ methodology ]] for automatically training << SPoT >> on the basis of feedback provided by human judges .", "h": ["methodology"], "t": ["SPoT"]}, {"label": "USED-FOR", "tokens": "First , a very simple , [[ randomized sentence-plan-generator -LRB- SPG -RRB- ]] generates a potentially large list of possible << sentence plans >> for a given text-plan input .", "h": ["randomized sentence-plan-generator -LRB- SPG -RRB-"], "t": ["sentence plans"]}, {"label": "USED-FOR", "tokens": "First , a very simple , << randomized sentence-plan-generator -LRB- SPG -RRB- >> generates a potentially large list of possible sentence plans for a given [[ text-plan input ]] .", "h": ["text-plan input"], "t": ["randomized sentence-plan-generator -LRB- SPG -RRB-"]}, {"label": "USED-FOR", "tokens": "Second , the [[ sentence-plan-ranker -LRB- SPR -RRB- ]] ranks the list of output << sentence plans >> , and then selects the top-ranked plan .", "h": ["sentence-plan-ranker -LRB- SPR -RRB-"], "t": ["sentence plans"]}, {"label": "USED-FOR", "tokens": "The << SPR >> uses [[ ranking rules ]] automatically learned from training data .", "h": ["ranking rules"], "t": ["SPR"]}, {"label": "USED-FOR", "tokens": "We show that the trained [[ SPR ]] learns to select a << sentence plan >> whose rating on average is only 5 % worse than the top human-ranked sentence plan .", "h": ["SPR"], "t": ["sentence plan"]}, {"label": "COMPARE", "tokens": "We show that the trained SPR learns to select a [[ sentence plan ]] whose rating on average is only 5 % worse than the << top human-ranked sentence plan >> .", "h": ["sentence plan"], "t": ["top human-ranked sentence plan"]}, {"label": "USED-FOR", "tokens": "We discuss [[ maximum a posteriori estimation ]] of << continuous density hidden Markov models -LRB- CDHMM -RRB- >> .", "h": ["maximum a posteriori estimation"], "t": ["continuous density hidden Markov models -LRB- CDHMM -RRB-"]}, {"label": "HYPONYM-OF", "tokens": "The classical << MLE reestimation algorithms >> , namely the [[ forward-backward algorithm ]] and the segmental k-means algorithm , are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities .", "h": ["forward-backward algorithm"], "t": ["MLE reestimation algorithms"]}, {"label": "HYPONYM-OF", "tokens": "The classical << MLE reestimation algorithms >> , namely the forward-backward algorithm and the [[ segmental k-means algorithm ]] , are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities .", "h": ["segmental k-means algorithm"], "t": ["MLE reestimation algorithms"]}, {"label": "CONJUNCTION", "tokens": "The classical MLE reestimation algorithms , namely the << forward-backward algorithm >> and the [[ segmental k-means algorithm ]] , are expanded and reestimation formulas are given for HMM with Gaussian mixture observation densities .", "h": ["segmental k-means algorithm"], "t": ["forward-backward algorithm"]}, {"label": "USED-FOR", "tokens": "The classical MLE reestimation algorithms , namely the forward-backward algorithm and the segmental k-means algorithm , are expanded and [[ reestimation formulas ]] are given for << HMM with Gaussian mixture observation densities >> .", "h": ["reestimation formulas"], "t": ["HMM with Gaussian mixture observation densities"]}, {"label": "USED-FOR", "tokens": "Because of its adaptive nature , [[ Bayesian learning ]] serves as a unified approach for the following four << speech recognition applications >> , namely parameter smoothing , speaker adaptation , speaker group modeling and corrective training .", "h": ["Bayesian learning"], "t": ["speech recognition applications"]}, {"label": "HYPONYM-OF", "tokens": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four << speech recognition applications >> , namely [[ parameter smoothing ]] , speaker adaptation , speaker group modeling and corrective training .", "h": ["parameter smoothing"], "t": ["speech recognition applications"]}, {"label": "CONJUNCTION", "tokens": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four speech recognition applications , namely [[ parameter smoothing ]] , << speaker adaptation >> , speaker group modeling and corrective training .", "h": ["parameter smoothing"], "t": ["speaker adaptation"]}, {"label": "HYPONYM-OF", "tokens": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four << speech recognition applications >> , namely parameter smoothing , [[ speaker adaptation ]] , speaker group modeling and corrective training .", "h": ["speaker adaptation"], "t": ["speech recognition applications"]}, {"label": "CONJUNCTION", "tokens": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four speech recognition applications , namely parameter smoothing , [[ speaker adaptation ]] , << speaker group modeling >> and corrective training .", "h": ["speaker adaptation"], "t": ["speaker group modeling"]}, {"label": "HYPONYM-OF", "tokens": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four << speech recognition applications >> , namely parameter smoothing , speaker adaptation , [[ speaker group modeling ]] and corrective training .", "h": ["speaker group modeling"], "t": ["speech recognition applications"]}, {"label": "CONJUNCTION", "tokens": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four speech recognition applications , namely parameter smoothing , speaker adaptation , [[ speaker group modeling ]] and << corrective training >> .", "h": ["speaker group modeling"], "t": ["corrective training"]}, {"label": "HYPONYM-OF", "tokens": "Because of its adaptive nature , Bayesian learning serves as a unified approach for the following four << speech recognition applications >> , namely parameter smoothing , speaker adaptation , speaker group modeling and [[ corrective training ]] .", "h": ["corrective training"], "t": ["speech recognition applications"]}, {"label": "EVALUATE-FOR", "tokens": "New experimental results on all four [[ applications ]] are provided to show the effectiveness of the << MAP estimation approach >> .", "h": ["applications"], "t": ["MAP estimation approach"]}, {"label": "COMPARE", "tokens": "This paper describes a characters-based Chinese collocation system and discusses the advantages of [[ it ]] over a traditional << word-based system >> .", "h": ["it"], "t": ["word-based system"]}, {"label": "FEATURE-OF", "tokens": "Since wordbreaks are not conventionally marked in Chinese text corpora , a << character-based collocation system >> has the dual advantages of [[ avoiding pre-processing distortion ]] and directly accessing sub-lexical information .", "h": ["avoiding pre-processing distortion"], "t": ["character-based collocation system"]}, {"label": "CONJUNCTION", "tokens": "Since wordbreaks are not conventionally marked in Chinese text corpora , a character-based collocation system has the dual advantages of [[ avoiding pre-processing distortion ]] and directly << accessing sub-lexical information >> .", "h": ["avoiding pre-processing distortion"], "t": ["accessing sub-lexical information"]}, {"label": "FEATURE-OF", "tokens": "Since wordbreaks are not conventionally marked in Chinese text corpora , a << character-based collocation system >> has the dual advantages of avoiding pre-processing distortion and directly [[ accessing sub-lexical information ]] .", "h": ["accessing sub-lexical information"], "t": ["character-based collocation system"]}, {"label": "USED-FOR", "tokens": "Furthermore , << word-based collocational properties >> can be obtained through an [[ auxiliary module of automatic segmentation ]] .", "h": ["auxiliary module of automatic segmentation"], "t": ["word-based collocational properties"]}, {"label": "USED-FOR", "tokens": "This paper describes a [[ method ]] for << utterance classification >> that does not require manual transcription of training data .", "h": ["method"], "t": ["utterance classification"]}, {"label": "USED-FOR", "tokens": "The [[ method ]] combines domain independent acoustic models with off-the-shelf classifiers to give << utterance classification >> performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .", "h": ["method"], "t": ["utterance classification"]}, {"label": "PART-OF", "tokens": "The << method >> combines [[ domain independent acoustic models ]] with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .", "h": ["domain independent acoustic models"], "t": ["method"]}, {"label": "PART-OF", "tokens": "The << method >> combines domain independent acoustic models with off-the-shelf [[ classifiers ]] to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .", "h": ["classifiers"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "The method combines << domain independent acoustic models >> with off-the-shelf [[ classifiers ]] to give utterance classification performance that is surprisingly close to what can be achieved using conventional word-trigram recognition requiring manual transcription .", "h": ["classifiers"], "t": ["domain independent acoustic models"]}, {"label": "USED-FOR", "tokens": "The << method >> combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional [[ word-trigram recognition ]] requiring manual transcription .", "h": ["word-trigram recognition"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The method combines domain independent acoustic models with off-the-shelf classifiers to give utterance classification performance that is surprisingly close to what can be achieved using conventional << word-trigram recognition >> requiring [[ manual transcription ]] .", "h": ["manual transcription"], "t": ["word-trigram recognition"]}, {"label": "PART-OF", "tokens": "In our << method >> , [[ unsupervised training ]] is first used to train a phone n-gram model for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .", "h": ["unsupervised training"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "In our method , [[ unsupervised training ]] is first used to train a << phone n-gram model >> for a particular domain ; the output of recognition with this model is then passed to a phone-string classifier .", "h": ["unsupervised training"], "t": ["phone n-gram model"]}, {"label": "USED-FOR", "tokens": "In our method , [[ unsupervised training ]] is first used to train a phone n-gram model for a particular << domain >> ; the output of recognition with this model is then passed to a phone-string classifier .", "h": ["unsupervised training"], "t": ["domain"]}, {"label": "EVALUATE-FOR", "tokens": "The [[ classification accuracy ]] of the << method >> is evaluated on three different spoken language system domains .", "h": ["classification accuracy"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "The classification accuracy of the << method >> is evaluated on three different [[ spoken language system domains ]] .", "h": ["spoken language system domains"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "The [[ Interval Algebra -LRB- IA -RRB- ]] and a subset of the << Region Connection Calculus -LRB- RCC -RRB- >> , namely RCC-8 , are the dominant Artificial Intelligence approaches for representing and reasoning about qualitative temporal and topological relations respectively .", "h": ["Interval Algebra -LRB- IA -RRB-"], "t": ["Region Connection Calculus -LRB- RCC -RRB-"]}, {"label": "HYPONYM-OF", "tokens": "The [[ Interval Algebra -LRB- IA -RRB- ]] and a subset of the Region Connection Calculus -LRB- RCC -RRB- , namely RCC-8 , are the dominant << Artificial Intelligence approaches >> for representing and reasoning about qualitative temporal and topological relations respectively .", "h": ["Interval Algebra -LRB- IA -RRB-"], "t": ["Artificial Intelligence approaches"]}, {"label": "HYPONYM-OF", "tokens": "The Interval Algebra -LRB- IA -RRB- and a subset of the [[ Region Connection Calculus -LRB- RCC -RRB- ]] , namely RCC-8 , are the dominant << Artificial Intelligence approaches >> for representing and reasoning about qualitative temporal and topological relations respectively .", "h": ["Region Connection Calculus -LRB- RCC -RRB-"], "t": ["Artificial Intelligence approaches"]}, {"label": "HYPONYM-OF", "tokens": "The Interval Algebra -LRB- IA -RRB- and a subset of the << Region Connection Calculus -LRB- RCC -RRB- >> , namely [[ RCC-8 ]] , are the dominant Artificial Intelligence approaches for representing and reasoning about qualitative temporal and topological relations respectively .", "h": ["RCC-8"], "t": ["Region Connection Calculus -LRB- RCC -RRB-"]}, {"label": "USED-FOR", "tokens": "The Interval Algebra -LRB- IA -RRB- and a subset of the Region Connection Calculus -LRB- RCC -RRB- , namely RCC-8 , are the dominant [[ Artificial Intelligence approaches ]] for << representing and reasoning about qualitative temporal and topological relations >> respectively .", "h": ["Artificial Intelligence approaches"], "t": ["representing and reasoning about qualitative temporal and topological relations"]}, {"label": "USED-FOR", "tokens": "Such << qualitative information >> can be formulated as a [[ Qualitative Constraint Network -LRB- QCN -RRB- ]] .", "h": ["Qualitative Constraint Network -LRB- QCN -RRB-"], "t": ["qualitative information"]}, {"label": "USED-FOR", "tokens": "In this paper , we focus on the minimal labeling problem -LRB- MLP -RRB- and we propose an [[ algorithm ]] to efficiently derive all the feasible base relations of a << QCN >> .", "h": ["algorithm"], "t": ["QCN"]}, {"label": "PART-OF", "tokens": "Our << algorithm >> considers [[ chordal QCNs ]] and a new form of partial consistency which we define as \u25c6 G-consistency .", "h": ["chordal QCNs"], "t": ["algorithm"]}, {"label": "CONJUNCTION", "tokens": "Our algorithm considers [[ chordal QCNs ]] and a new form of << partial consistency >> which we define as \u25c6 G-consistency .", "h": ["chordal QCNs"], "t": ["partial consistency"]}, {"label": "PART-OF", "tokens": "Our << algorithm >> considers chordal QCNs and a new form of [[ partial consistency ]] which we define as \u25c6 G-consistency .", "h": ["partial consistency"], "t": ["algorithm"]}, {"label": "HYPONYM-OF", "tokens": "Our algorithm considers chordal QCNs and a new form of << partial consistency >> which we define as [[ \u25c6 G-consistency ]] .", "h": ["\u25c6 G-consistency"], "t": ["partial consistency"]}, {"label": "EVALUATE-FOR", "tokens": "Experi-mentations with [[ QCNs of IA and RCC-8 ]] show the importance and efficiency of this new << approach >> .", "h": ["QCNs of IA and RCC-8"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "In this paper a [[ morphological component ]] with a limited capability to automatically interpret -LRB- and generate -RRB- << derived words >> is presented .", "h": ["morphological component"], "t": ["derived words"]}, {"label": "USED-FOR", "tokens": "The << system >> combines an extended [[ two-level morphology ]] -LSB- Trost , 1991a ; Trost , 1991b -RSB- with a feature-based word grammar building on a hierarchical lexicon .", "h": ["two-level morphology"], "t": ["system"]}, {"label": "CONJUNCTION", "tokens": "The system combines an extended [[ two-level morphology ]] -LSB- Trost , 1991a ; Trost , 1991b -RSB- with a << feature-based word grammar >> building on a hierarchical lexicon .", "h": ["two-level morphology"], "t": ["feature-based word grammar"]}, {"label": "USED-FOR", "tokens": "The system combines an extended two-level morphology -LSB- Trost , 1991a ; Trost , 1991b -RSB- with a << feature-based word grammar >> building on a [[ hierarchical lexicon ]] .", "h": ["hierarchical lexicon"], "t": ["feature-based word grammar"]}, {"label": "FEATURE-OF", "tokens": "<< Polymorphemic stems >> not explicitly stored in the lexicon are given a [[ compositional interpretation ]] .", "h": ["compositional interpretation"], "t": ["Polymorphemic stems"]}, {"label": "USED-FOR", "tokens": "The << system >> is implemented in [[ CommonLisp ]] and has been tested on examples from German derivation .", "h": ["CommonLisp"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "The << system >> is implemented in CommonLisp and has been tested on examples from [[ German derivation ]] .", "h": ["German derivation"], "t": ["system"]}, {"label": "CONJUNCTION", "tokens": "Four problems render vector space model -LRB- VSM -RRB- - based text classification approach ineffective : 1 -RRB- Many words within song lyrics actually contribute little to sentiment ; 2 -RRB- Nouns and verbs used to express sentiment are ambiguous ; 3 -RRB- [[ Negations ]] and << modifiers >> around the sentiment keywords make particular contributions to sentiment ; 4 -RRB- Song lyric is usually very short .", "h": ["Negations"], "t": ["modifiers"]}, {"label": "USED-FOR", "tokens": "Four problems render vector space model -LRB- VSM -RRB- - based text classification approach ineffective : 1 -RRB- Many words within song lyrics actually contribute little to sentiment ; 2 -RRB- Nouns and verbs used to express sentiment are ambiguous ; 3 -RRB- [[ Negations ]] and modifiers around the sentiment keywords make particular contributions to << sentiment >> ; 4 -RRB- Song lyric is usually very short .", "h": ["Negations"], "t": ["sentiment"]}, {"label": "USED-FOR", "tokens": "Four problems render vector space model -LRB- VSM -RRB- - based text classification approach ineffective : 1 -RRB- Many words within song lyrics actually contribute little to sentiment ; 2 -RRB- Nouns and verbs used to express sentiment are ambiguous ; 3 -RRB- Negations and [[ modifiers ]] around the sentiment keywords make particular contributions to << sentiment >> ; 4 -RRB- Song lyric is usually very short .", "h": ["modifiers"], "t": ["sentiment"]}, {"label": "USED-FOR", "tokens": "To address these problems , the [[ sentiment vector space model -LRB- s-VSM -RRB- ]] is proposed to represent << song lyric document >> .", "h": ["sentiment vector space model -LRB- s-VSM -RRB-"], "t": ["song lyric document"]}, {"label": "COMPARE", "tokens": "The preliminary experiments prove that the [[ s-VSM model ]] outperforms the << VSM model >> in the lyric-based song sentiment classification task .", "h": ["s-VSM model"], "t": ["VSM model"]}, {"label": "EVALUATE-FOR", "tokens": "The preliminary experiments prove that the << s-VSM model >> outperforms the VSM model in the [[ lyric-based song sentiment classification task ]] .", "h": ["lyric-based song sentiment classification task"], "t": ["s-VSM model"]}, {"label": "EVALUATE-FOR", "tokens": "The preliminary experiments prove that the s-VSM model outperforms the << VSM model >> in the [[ lyric-based song sentiment classification task ]] .", "h": ["lyric-based song sentiment classification task"], "t": ["VSM model"]}, {"label": "USED-FOR", "tokens": "We present an efficient [[ algorithm ]] for the << redundancy elimination problem >> : Given an underspecified semantic representation -LRB- USR -RRB- of a scope ambiguity , compute an USR with fewer mutually equivalent readings .", "h": ["algorithm"], "t": ["redundancy elimination problem"]}, {"label": "USED-FOR", "tokens": "We present an efficient algorithm for the redundancy elimination problem : Given an [[ underspecified semantic representation -LRB- USR -RRB- ]] of a << scope ambiguity >> , compute an USR with fewer mutually equivalent readings .", "h": ["underspecified semantic representation -LRB- USR -RRB-"], "t": ["scope ambiguity"]}, {"label": "USED-FOR", "tokens": "We present an efficient algorithm for the redundancy elimination problem : Given an underspecified semantic representation -LRB- USR -RRB- of a scope ambiguity , compute an << USR >> with fewer mutually [[ equivalent readings ]] .", "h": ["equivalent readings"], "t": ["USR"]}, {"label": "USED-FOR", "tokens": "The [[ algorithm ]] operates on << underspecified chart representations >> which are derived from dominance graphs ; it can be applied to the USRs computed by large-scale grammars .", "h": ["algorithm"], "t": ["underspecified chart representations"]}, {"label": "USED-FOR", "tokens": "The algorithm operates on << underspecified chart representations >> which are derived from [[ dominance graphs ]] ; it can be applied to the USRs computed by large-scale grammars .", "h": ["dominance graphs"], "t": ["underspecified chart representations"]}, {"label": "USED-FOR", "tokens": "The algorithm operates on underspecified chart representations which are derived from dominance graphs ; [[ it ]] can be applied to the << USRs >> computed by large-scale grammars .", "h": ["it"], "t": ["USRs"]}, {"label": "USED-FOR", "tokens": "The algorithm operates on underspecified chart representations which are derived from dominance graphs ; it can be applied to the << USRs >> computed by [[ large-scale grammars ]] .", "h": ["large-scale grammars"], "t": ["USRs"]}, {"label": "USED-FOR", "tokens": "We evaluate the algorithm on a corpus , and show that [[ it ]] reduces the << degree of ambiguity >> significantly while taking negligible runtime .", "h": ["it"], "t": ["degree of ambiguity"]}, {"label": "USED-FOR", "tokens": "Currently several << grammatical formalisms >> converge towards being declarative and towards utilizing [[ context-free phrase-structure grammar ]] as a backbone , e.g. LFG and PATR-II .", "h": ["context-free phrase-structure grammar"], "t": ["grammatical formalisms"]}, {"label": "HYPONYM-OF", "tokens": "Currently several << grammatical formalisms >> converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone , e.g. [[ LFG ]] and PATR-II .", "h": ["LFG"], "t": ["grammatical formalisms"]}, {"label": "CONJUNCTION", "tokens": "Currently several grammatical formalisms converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone , e.g. [[ LFG ]] and << PATR-II >> .", "h": ["LFG"], "t": ["PATR-II"]}, {"label": "HYPONYM-OF", "tokens": "Currently several << grammatical formalisms >> converge towards being declarative and towards utilizing context-free phrase-structure grammar as a backbone , e.g. LFG and [[ PATR-II ]] .", "h": ["PATR-II"], "t": ["grammatical formalisms"]}, {"label": "FEATURE-OF", "tokens": "Typically the processing of these << formalisms >> is organized within a [[ chart-parsing framework ]] .", "h": ["chart-parsing framework"], "t": ["formalisms"]}, {"label": "PART-OF", "tokens": "The aim of this paper is to provide a survey and a practical comparison of fundamental [[ rule-invocation strategies ]] within << context-free chart parsing >> .", "h": ["rule-invocation strategies"], "t": ["context-free chart parsing"]}, {"label": "USED-FOR", "tokens": "The present paper focusses on << terminology structuring >> by [[ lexical methods ]] , which match terms on the basis on their content words , taking morphological variants into account .", "h": ["lexical methods"], "t": ["terminology structuring"]}, {"label": "HYPONYM-OF", "tokens": "Experiments are done on a ` flat ' list of terms obtained from an originally << hierarchically-structured terminology >> : the French version of the [[ US National Library of Medicine MeSH thesaurus ]] .", "h": ["US National Library of Medicine MeSH thesaurus"], "t": ["hierarchically-structured terminology"]}, {"label": "COMPARE", "tokens": "We compare the [[ lexically-induced relations ]] with the original << MeSH relations >> : after a quantitative evaluation of their congruence through recall and precision metrics , we perform a qualitative , human analysis ofthe ` new ' relations not present in the MeSH .", "h": ["lexically-induced relations"], "t": ["MeSH relations"]}, {"label": "EVALUATE-FOR", "tokens": "We compare the lexically-induced relations with the original << MeSH relations >> : after a quantitative evaluation of their congruence through [[ recall and precision metrics ]] , we perform a qualitative , human analysis ofthe ` new ' relations not present in the MeSH .", "h": ["recall and precision metrics"], "t": ["MeSH relations"]}, {"label": "USED-FOR", "tokens": "In order to boost the translation quality of << EBMT >> based on a [[ small-sized bilingual corpus ]] , we use an out-of-domain bilingual corpus and , in addition , the language model of an in-domain monolingual corpus .", "h": ["small-sized bilingual corpus"], "t": ["EBMT"]}, {"label": "USED-FOR", "tokens": "In order to boost the translation quality of << EBMT >> based on a small-sized bilingual corpus , we use an [[ out-of-domain bilingual corpus ]] and , in addition , the language model of an in-domain monolingual corpus .", "h": ["out-of-domain bilingual corpus"], "t": ["EBMT"]}, {"label": "USED-FOR", "tokens": "In order to boost the translation quality of << EBMT >> based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and , in addition , the [[ language model ]] of an in-domain monolingual corpus .", "h": ["language model"], "t": ["EBMT"]}, {"label": "USED-FOR", "tokens": "In order to boost the translation quality of EBMT based on a small-sized bilingual corpus , we use an out-of-domain bilingual corpus and , in addition , the << language model >> of an [[ in-domain monolingual corpus ]] .", "h": ["in-domain monolingual corpus"], "t": ["language model"]}, {"label": "USED-FOR", "tokens": "The two [[ evaluation measures ]] of the BLEU score and the NIST score demonstrated the effect of using an << out-of-domain bilingual corpus >> and the possibility of using the language model .", "h": ["evaluation measures"], "t": ["out-of-domain bilingual corpus"]}, {"label": "EVALUATE-FOR", "tokens": "The two [[ evaluation measures ]] of the BLEU score and the NIST score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the << language model >> .", "h": ["evaluation measures"], "t": ["language model"]}, {"label": "HYPONYM-OF", "tokens": "The two << evaluation measures >> of the [[ BLEU score ]] and the NIST score demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model .", "h": ["BLEU score"], "t": ["evaluation measures"]}, {"label": "CONJUNCTION", "tokens": "The two evaluation measures of the [[ BLEU score ]] and the << NIST score >> demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model .", "h": ["BLEU score"], "t": ["NIST score"]}, {"label": "HYPONYM-OF", "tokens": "The two << evaluation measures >> of the BLEU score and the [[ NIST score ]] demonstrated the effect of using an out-of-domain bilingual corpus and the possibility of using the language model .", "h": ["NIST score"], "t": ["evaluation measures"]}, {"label": "USED-FOR", "tokens": "[[ Diagrams ]] are common tools for representing << complex concepts >> , relationships and events , often when it would be difficult to portray the same information with natural images .", "h": ["Diagrams"], "t": ["complex concepts"]}, {"label": "USED-FOR", "tokens": "[[ Diagrams ]] are common tools for representing complex concepts , << relationships >> and events , often when it would be difficult to portray the same information with natural images .", "h": ["Diagrams"], "t": ["relationships"]}, {"label": "USED-FOR", "tokens": "[[ Diagrams ]] are common tools for representing complex concepts , relationships and << events >> , often when it would be difficult to portray the same information with natural images .", "h": ["Diagrams"], "t": ["events"]}, {"label": "CONJUNCTION", "tokens": "Diagrams are common tools for representing [[ complex concepts ]] , << relationships >> and events , often when it would be difficult to portray the same information with natural images .", "h": ["complex concepts"], "t": ["relationships"]}, {"label": "CONJUNCTION", "tokens": "Diagrams are common tools for representing complex concepts , [[ relationships ]] and << events >> , often when it would be difficult to portray the same information with natural images .", "h": ["relationships"], "t": ["events"]}, {"label": "PART-OF", "tokens": "[[ Understanding natural images ]] has been extensively studied in << computer vision >> , while diagram understanding has received little attention .", "h": ["Understanding natural images"], "t": ["computer vision"]}, {"label": "COMPARE", "tokens": "[[ Understanding natural images ]] has been extensively studied in computer vision , while << diagram understanding >> has received little attention .", "h": ["Understanding natural images"], "t": ["diagram understanding"]}, {"label": "USED-FOR", "tokens": "In this paper , we study the problem of diagram interpretation and reasoning , the challenging [[ task ]] of identifying the << structure of a diagram >> and the semantics of its constituents and their relationships .", "h": ["task"], "t": ["structure of a diagram"]}, {"label": "USED-FOR", "tokens": "We introduce [[ Diagram Parse Graphs -LRB- DPG -RRB- ]] as our representation to model the << structure of diagrams >> .", "h": ["Diagram Parse Graphs -LRB- DPG -RRB-"], "t": ["structure of diagrams"]}, {"label": "USED-FOR", "tokens": "We define [[ syntactic parsing of diagrams ]] as learning to infer << DPGs >> for diagrams and study semantic interpretation and reasoning of diagrams in the context of diagram question answering .", "h": ["syntactic parsing of diagrams"], "t": ["DPGs"]}, {"label": "USED-FOR", "tokens": "We define syntactic parsing of diagrams as learning to infer DPGs for diagrams and study [[ semantic interpretation and reasoning of diagrams ]] in the context of << diagram question answering >> .", "h": ["semantic interpretation and reasoning of diagrams"], "t": ["diagram question answering"]}, {"label": "USED-FOR", "tokens": "We devise an [[ LSTM-based method ]] for << syntactic parsing of diagrams >> and introduce a DPG-based attention model for diagram question answering .", "h": ["LSTM-based method"], "t": ["syntactic parsing of diagrams"]}, {"label": "USED-FOR", "tokens": "We devise an LSTM-based method for syntactic parsing of diagrams and introduce a [[ DPG-based attention model ]] for << diagram question answering >> .", "h": ["DPG-based attention model"], "t": ["diagram question answering"]}, {"label": "FEATURE-OF", "tokens": "We compile a new << dataset >> of [[ diagrams ]] with exhaustive annotations of constituents and relationships for over 5,000 diagrams and 15,000 questions and answers .", "h": ["diagrams"], "t": ["dataset"]}, {"label": "USED-FOR", "tokens": "Our results show the significance of our [[ models ]] for << syntactic parsing and question answering in diagrams >> using DPGs .", "h": ["models"], "t": ["syntactic parsing and question answering in diagrams"]}, {"label": "USED-FOR", "tokens": "Our results show the significance of our << models >> for syntactic parsing and question answering in diagrams using [[ DPGs ]] .", "h": ["DPGs"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "Previous [[ change detection methods ]] , focusing on << detecting large-scale significant changes >> , can not do this well .", "h": ["change detection methods"], "t": ["detecting large-scale significant changes"]}, {"label": "USED-FOR", "tokens": "This paper proposes a feasible [[ end-to-end approach ]] to this challenging << problem >> .", "h": ["end-to-end approach"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "Given two times observations , we formulate << fine-grained change detection >> as a [[ joint optimization problem ]] of three related factors , i.e. , normal-aware lighting difference , camera geometry correction flow , and real scene change mask .", "h": ["joint optimization problem"], "t": ["fine-grained change detection"]}, {"label": "FEATURE-OF", "tokens": "Given two times observations , we formulate fine-grained change detection as a << joint optimization problem >> of three related [[ factors ]] , i.e. , normal-aware lighting difference , camera geometry correction flow , and real scene change mask .", "h": ["factors"], "t": ["joint optimization problem"]}, {"label": "HYPONYM-OF", "tokens": "Given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related << factors >> , i.e. , [[ normal-aware lighting difference ]] , camera geometry correction flow , and real scene change mask .", "h": ["normal-aware lighting difference"], "t": ["factors"]}, {"label": "CONJUNCTION", "tokens": "Given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related factors , i.e. , [[ normal-aware lighting difference ]] , << camera geometry correction flow >> , and real scene change mask .", "h": ["normal-aware lighting difference"], "t": ["camera geometry correction flow"]}, {"label": "HYPONYM-OF", "tokens": "Given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related << factors >> , i.e. , normal-aware lighting difference , [[ camera geometry correction flow ]] , and real scene change mask .", "h": ["camera geometry correction flow"], "t": ["factors"]}, {"label": "CONJUNCTION", "tokens": "Given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related factors , i.e. , normal-aware lighting difference , [[ camera geometry correction flow ]] , and << real scene change mask >> .", "h": ["camera geometry correction flow"], "t": ["real scene change mask"]}, {"label": "HYPONYM-OF", "tokens": "Given two times observations , we formulate fine-grained change detection as a joint optimization problem of three related << factors >> , i.e. , normal-aware lighting difference , camera geometry correction flow , and [[ real scene change mask ]] .", "h": ["real scene change mask"], "t": ["factors"]}, {"label": "USED-FOR", "tokens": "We solve the three << factors >> in a [[ coarse-to-fine manner ]] and achieve reliable change decision by rank minimization .", "h": ["coarse-to-fine manner"], "t": ["factors"]}, {"label": "USED-FOR", "tokens": "We solve the three factors in a coarse-to-fine manner and achieve reliable << change decision >> by [[ rank minimization ]] .", "h": ["rank minimization"], "t": ["change decision"]}, {"label": "EVALUATE-FOR", "tokens": "We build three [[ real-world datasets ]] to benchmark << fine-grained change detection of misaligned scenes >> under varied multiple lighting conditions .", "h": ["real-world datasets"], "t": ["fine-grained change detection of misaligned scenes"]}, {"label": "FEATURE-OF", "tokens": "We build three real-world datasets to benchmark << fine-grained change detection of misaligned scenes >> under [[ varied multiple lighting conditions ]] .", "h": ["varied multiple lighting conditions"], "t": ["fine-grained change detection of misaligned scenes"]}, {"label": "COMPARE", "tokens": "Extensive experiments show the superior performance of our [[ approach ]] over state-of-the-art << change detection methods >> and its ability to distinguish real scene changes from false ones caused by lighting variations .", "h": ["approach"], "t": ["change detection methods"]}, {"label": "USED-FOR", "tokens": "Extensive experiments show the superior performance of our [[ approach ]] over state-of-the-art change detection methods and its ability to distinguish << real scene changes >> from false ones caused by lighting variations .", "h": ["approach"], "t": ["real scene changes"]}, {"label": "EVALUATE-FOR", "tokens": "[[ Automatic evaluation metrics ]] for << Machine Translation -LRB- MT -RRB- systems >> , such as BLEU or NIST , are now well established .", "h": ["Automatic evaluation metrics"], "t": ["Machine Translation -LRB- MT -RRB- systems"]}, {"label": "HYPONYM-OF", "tokens": "<< Automatic evaluation metrics >> for Machine Translation -LRB- MT -RRB- systems , such as [[ BLEU ]] or NIST , are now well established .", "h": ["BLEU"], "t": ["Automatic evaluation metrics"]}, {"label": "CONJUNCTION", "tokens": "Automatic evaluation metrics for Machine Translation -LRB- MT -RRB- systems , such as [[ BLEU ]] or << NIST >> , are now well established .", "h": ["BLEU"], "t": ["NIST"]}, {"label": "HYPONYM-OF", "tokens": "<< Automatic evaluation metrics >> for Machine Translation -LRB- MT -RRB- systems , such as BLEU or [[ NIST ]] , are now well established .", "h": ["NIST"], "t": ["Automatic evaluation metrics"]}, {"label": "USED-FOR", "tokens": "Yet , [[ they ]] are scarcely used for the << assessment of language pairs >> like English-Chinese or English-Japanese , because of the word segmentation problem .", "h": ["they"], "t": ["assessment of language pairs"]}, {"label": "HYPONYM-OF", "tokens": "Yet , they are scarcely used for the assessment of << language pairs >> like [[ English-Chinese ]] or English-Japanese , because of the word segmentation problem .", "h": ["English-Chinese"], "t": ["language pairs"]}, {"label": "CONJUNCTION", "tokens": "Yet , they are scarcely used for the assessment of language pairs like [[ English-Chinese ]] or << English-Japanese >> , because of the word segmentation problem .", "h": ["English-Chinese"], "t": ["English-Japanese"]}, {"label": "HYPONYM-OF", "tokens": "Yet , they are scarcely used for the assessment of << language pairs >> like English-Chinese or [[ English-Japanese ]] , because of the word segmentation problem .", "h": ["English-Japanese"], "t": ["language pairs"]}, {"label": "USED-FOR", "tokens": "This study establishes the equivalence between the standard use of [[ BLEU ]] in << word n-grams >> and its application at the character level .", "h": ["BLEU"], "t": ["word n-grams"]}, {"label": "USED-FOR", "tokens": "This study establishes the equivalence between the standard use of [[ BLEU ]] in word n-grams and its application at the << character level >> .", "h": ["BLEU"], "t": ["character level"]}, {"label": "CONJUNCTION", "tokens": "This study establishes the equivalence between the standard use of BLEU in [[ word n-grams ]] and its application at the << character level >> .", "h": ["word n-grams"], "t": ["character level"]}, {"label": "USED-FOR", "tokens": "The use of [[ BLEU ]] at the << character level >> eliminates the word segmentation problem : it makes it possible to directly compare commercial systems outputting unsegmented texts with , for instance , statistical MT systems which usually segment their outputs .", "h": ["BLEU"], "t": ["character level"]}, {"label": "USED-FOR", "tokens": "The use of [[ BLEU ]] at the character level eliminates the << word segmentation problem >> : it makes it possible to directly compare commercial systems outputting unsegmented texts with , for instance , statistical MT systems which usually segment their outputs .", "h": ["BLEU"], "t": ["word segmentation problem"]}, {"label": "EVALUATE-FOR", "tokens": "The use of BLEU at the character level eliminates the word segmentation problem : [[ it ]] makes it possible to directly compare << commercial systems >> outputting unsegmented texts with , for instance , statistical MT systems which usually segment their outputs .", "h": ["it"], "t": ["commercial systems"]}, {"label": "EVALUATE-FOR", "tokens": "The use of BLEU at the character level eliminates the word segmentation problem : [[ it ]] makes it possible to directly compare commercial systems outputting unsegmented texts with , for instance , << statistical MT systems >> which usually segment their outputs .", "h": ["it"], "t": ["statistical MT systems"]}, {"label": "COMPARE", "tokens": "The use of BLEU at the character level eliminates the word segmentation problem : it makes it possible to directly compare [[ commercial systems ]] outputting unsegmented texts with , for instance , << statistical MT systems >> which usually segment their outputs .", "h": ["commercial systems"], "t": ["statistical MT systems"]}, {"label": "USED-FOR", "tokens": "This paper proposes a series of modifications to the [[ left corner parsing algorithm ]] for << context-free grammars >> .", "h": ["left corner parsing algorithm"], "t": ["context-free grammars"]}, {"label": "USED-FOR", "tokens": "It is argued that the resulting [[ algorithm ]] is both efficient and flexible and is , therefore , a good choice for the << parser >> used in a natural language interface .", "h": ["algorithm"], "t": ["parser"]}, {"label": "USED-FOR", "tokens": "It is argued that the resulting algorithm is both efficient and flexible and is , therefore , a good choice for the [[ parser ]] used in a << natural language interface >> .", "h": ["parser"], "t": ["natural language interface"]}, {"label": "USED-FOR", "tokens": "This paper presents a novel << statistical singing voice conversion -LRB- SVC -RRB- technique >> with [[ direct waveform modification ]] based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms .", "h": ["direct waveform modification"], "t": ["statistical singing voice conversion -LRB- SVC -RRB- technique"]}, {"label": "USED-FOR", "tokens": "This paper presents a novel statistical singing voice conversion -LRB- SVC -RRB- technique with << direct waveform modification >> based on the [[ spectrum differential ]] that can convert voice timbre of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms .", "h": ["spectrum differential"], "t": ["direct waveform modification"]}, {"label": "USED-FOR", "tokens": "This paper presents a novel statistical singing voice conversion -LRB- SVC -RRB- technique with direct waveform modification based on the [[ spectrum differential ]] that can convert << voice timbre >> of a source singer into that of a target singer without using a vocoder to generate converted singing voice waveforms .", "h": ["spectrum differential"], "t": ["voice timbre"]}, {"label": "USED-FOR", "tokens": "This paper presents a novel statistical singing voice conversion -LRB- SVC -RRB- technique with direct waveform modification based on the spectrum differential that can convert voice timbre of a source singer into that of a target singer without using a [[ vocoder ]] to generate << converted singing voice waveforms >> .", "h": ["vocoder"], "t": ["converted singing voice waveforms"]}, {"label": "USED-FOR", "tokens": "[[ SVC ]] makes it possible to convert << singing voice characteristics >> of an arbitrary source singer into those of an arbitrary target singer .", "h": ["SVC"], "t": ["singing voice characteristics"]}, {"label": "EVALUATE-FOR", "tokens": "However , [[ speech quality ]] of the << converted singing voice >> is significantly degraded compared to that of a natural singing voice due to various factors , such as analysis and modeling errors in the vocoder-based framework .", "h": ["speech quality"], "t": ["converted singing voice"]}, {"label": "EVALUATE-FOR", "tokens": "However , [[ speech quality ]] of the converted singing voice is significantly degraded compared to that of a << natural singing voice >> due to various factors , such as analysis and modeling errors in the vocoder-based framework .", "h": ["speech quality"], "t": ["natural singing voice"]}, {"label": "COMPARE", "tokens": "However , speech quality of the [[ converted singing voice ]] is significantly degraded compared to that of a << natural singing voice >> due to various factors , such as analysis and modeling errors in the vocoder-based framework .", "h": ["converted singing voice"], "t": ["natural singing voice"]}, {"label": "USED-FOR", "tokens": "The << differential spectral feature >> is directly estimated using a [[ differential Gaussian mixture model -LRB- GMM -RRB- ]] that is analytically derived from the traditional GMM used as a conversion model in the conventional SVC .", "h": ["differential Gaussian mixture model -LRB- GMM -RRB-"], "t": ["differential spectral feature"]}, {"label": "USED-FOR", "tokens": "The differential spectral feature is directly estimated using a << differential Gaussian mixture model -LRB- GMM -RRB- >> that is analytically derived from the traditional [[ GMM ]] used as a conversion model in the conventional SVC .", "h": ["GMM"], "t": ["differential Gaussian mixture model -LRB- GMM -RRB-"]}, {"label": "USED-FOR", "tokens": "The differential spectral feature is directly estimated using a differential Gaussian mixture model -LRB- GMM -RRB- that is analytically derived from the traditional [[ GMM ]] used as a << conversion model >> in the conventional SVC .", "h": ["GMM"], "t": ["conversion model"]}, {"label": "USED-FOR", "tokens": "The differential spectral feature is directly estimated using a differential Gaussian mixture model -LRB- GMM -RRB- that is analytically derived from the traditional GMM used as a [[ conversion model ]] in the conventional << SVC >> .", "h": ["conversion model"], "t": ["SVC"]}, {"label": "COMPARE", "tokens": "The experimental results demonstrate that the proposed [[ method ]] makes it possible to significantly improve speech quality in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional << SVC >> .", "h": ["method"], "t": ["SVC"]}, {"label": "EVALUATE-FOR", "tokens": "The experimental results demonstrate that the proposed << method >> makes it possible to significantly improve [[ speech quality ]] in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional SVC .", "h": ["speech quality"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "The experimental results demonstrate that the proposed method makes it possible to significantly improve [[ speech quality ]] in the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional << SVC >> .", "h": ["speech quality"], "t": ["SVC"]}, {"label": "EVALUATE-FOR", "tokens": "The experimental results demonstrate that the proposed << method >> makes it possible to significantly improve speech quality in the converted singing voice while preserving the [[ conversion accuracy of singer identity ]] compared to the conventional SVC .", "h": ["conversion accuracy of singer identity"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The experimental results demonstrate that the proposed method makes it possible to significantly improve speech quality in the converted singing voice while preserving the [[ conversion accuracy of singer identity ]] compared to the conventional << SVC >> .", "h": ["conversion accuracy of singer identity"], "t": ["SVC"]}, {"label": "USED-FOR", "tokens": "During late-2013 through early-2014 NIST coordinated a special << i-vector challenge >> based on data used in previous [[ NIST Speaker Recognition Evaluations -LRB- SREs -RRB- ]] .", "h": ["NIST Speaker Recognition Evaluations -LRB- SREs -RRB-"], "t": ["i-vector challenge"]}, {"label": "USED-FOR", "tokens": "Unlike evaluations in the SRE series , the i-vector challenge was run entirely online and used [[ fixed-length feature vectors ]] projected into a << low-dimensional space -LRB- i-vectors -RRB- >> rather than audio recordings .", "h": ["fixed-length feature vectors"], "t": ["low-dimensional space -LRB- i-vectors -RRB-"]}, {"label": "USED-FOR", "tokens": "Unlike evaluations in the SRE series , the << i-vector challenge >> was run entirely online and used fixed-length feature vectors projected into a [[ low-dimensional space -LRB- i-vectors -RRB- ]] rather than audio recordings .", "h": ["low-dimensional space -LRB- i-vectors -RRB-"], "t": ["i-vector challenge"]}, {"label": "COMPARE", "tokens": "Unlike evaluations in the SRE series , the i-vector challenge was run entirely online and used fixed-length feature vectors projected into a << low-dimensional space -LRB- i-vectors -RRB- >> rather than [[ audio recordings ]] .", "h": ["audio recordings"], "t": ["low-dimensional space -LRB- i-vectors -RRB-"]}, {"label": "COMPARE", "tokens": "Compared to the 2012 [[ SRE ]] , the << i-vector challenge >> saw an increase in the number of participants by nearly a factor of two , and a two orders of magnitude increase in the number of systems submitted for evaluation .", "h": ["SRE"], "t": ["i-vector challenge"]}, {"label": "COMPARE", "tokens": "Initial results indicate the [[ leading system ]] achieved an approximate 37 % improvement relative to the << baseline system >> .", "h": ["leading system"], "t": ["baseline system"]}, {"label": "USED-FOR", "tokens": "Theoretical research in the area of << machine translation >> usually involves the search for and creation of an appropriate [[ formalism ]] .", "h": ["formalism"], "t": ["machine translation"]}, {"label": "PART-OF", "tokens": "In this paper , we will introduce the [[ anaphoric component ]] of the << Mimo formalism >> .", "h": ["anaphoric component"], "t": ["Mimo formalism"]}, {"label": "USED-FOR", "tokens": "In [[ Mimo ]] , the << translation of anaphoric relations >> is compositional .", "h": ["Mimo"], "t": ["translation of anaphoric relations"]}, {"label": "USED-FOR", "tokens": "The [[ anaphoric component ]] is used to define << linguistic phenomena >> such as wh-movement , the passive and the binding of reflexives and pronouns mono-lingually .", "h": ["anaphoric component"], "t": ["linguistic phenomena"]}, {"label": "HYPONYM-OF", "tokens": "The anaphoric component is used to define << linguistic phenomena >> such as [[ wh-movement ]] , the passive and the binding of reflexives and pronouns mono-lingually .", "h": ["wh-movement"], "t": ["linguistic phenomena"]}, {"label": "CONJUNCTION", "tokens": "The anaphoric component is used to define linguistic phenomena such as [[ wh-movement ]] , << the passive and the binding of reflexives and pronouns >> mono-lingually .", "h": ["wh-movement"], "t": ["the passive and the binding of reflexives and pronouns"]}, {"label": "HYPONYM-OF", "tokens": "The anaphoric component is used to define << linguistic phenomena >> such as wh-movement , [[ the passive and the binding of reflexives and pronouns ]] mono-lingually .", "h": ["the passive and the binding of reflexives and pronouns"], "t": ["linguistic phenomena"]}, {"label": "CONJUNCTION", "tokens": "The [[ efficiency ]] and << quality >> is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD 's .", "h": ["efficiency"], "t": ["quality"]}, {"label": "FEATURE-OF", "tokens": "The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a << database >> of 40000 [[ images of popular music CD 's ]] .", "h": ["images of popular music CD 's"], "t": ["database"]}, {"label": "USED-FOR", "tokens": "The [[ scheme ]] builds upon popular techniques of indexing descriptors extracted from local regions , and is robust to << background clutter >> and occlusion .", "h": ["scheme"], "t": ["background clutter"]}, {"label": "USED-FOR", "tokens": "The [[ scheme ]] builds upon popular techniques of indexing descriptors extracted from local regions , and is robust to background clutter and << occlusion >> .", "h": ["scheme"], "t": ["occlusion"]}, {"label": "USED-FOR", "tokens": "The << scheme >> builds upon popular techniques of [[ indexing descriptors ]] extracted from local regions , and is robust to background clutter and occlusion .", "h": ["indexing descriptors"], "t": ["scheme"]}, {"label": "USED-FOR", "tokens": "The scheme builds upon popular techniques of << indexing descriptors >> extracted from [[ local regions ]] , and is robust to background clutter and occlusion .", "h": ["local regions"], "t": ["indexing descriptors"]}, {"label": "CONJUNCTION", "tokens": "The scheme builds upon popular techniques of indexing descriptors extracted from local regions , and is robust to [[ background clutter ]] and << occlusion >> .", "h": ["background clutter"], "t": ["occlusion"]}, {"label": "USED-FOR", "tokens": "The << local region descriptors >> are hierarchically quantized in a [[ vocabulary tree ]] .", "h": ["vocabulary tree"], "t": ["local region descriptors"]}, {"label": "CONJUNCTION", "tokens": "The [[ quantization ]] and the << indexing >> are therefore fully integrated , essentially being one and the same .", "h": ["quantization"], "t": ["indexing"]}, {"label": "EVALUATE-FOR", "tokens": "The [[ recognition quality ]] is evaluated through retrieval on a database with ground truth , showing the power of the << vocabulary tree approach >> , going as high as 1 million images .", "h": ["recognition quality"], "t": ["vocabulary tree approach"]}, {"label": "EVALUATE-FOR", "tokens": "The << recognition quality >> is evaluated through [[ retrieval ]] on a database with ground truth , showing the power of the vocabulary tree approach , going as high as 1 million images .", "h": ["retrieval"], "t": ["recognition quality"]}, {"label": "USED-FOR", "tokens": "The recognition quality is evaluated through << retrieval >> on a [[ database with ground truth ]] , showing the power of the vocabulary tree approach , going as high as 1 million images .", "h": ["database with ground truth"], "t": ["retrieval"]}, {"label": "USED-FOR", "tokens": "This paper presents a [[ method ]] for << blind estimation of reverberation times >> in reverberant enclosures .", "h": ["method"], "t": ["blind estimation of reverberation times"]}, {"label": "FEATURE-OF", "tokens": "This paper presents a method for << blind estimation of reverberation times >> in [[ reverberant enclosures ]] .", "h": ["reverberant enclosures"], "t": ["blind estimation of reverberation times"]}, {"label": "USED-FOR", "tokens": "The proposed << algorithm >> is based on a [[ statistical model of short-term log-energy sequences ]] for echo-free speech .", "h": ["statistical model of short-term log-energy sequences"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "The proposed algorithm is based on a [[ statistical model of short-term log-energy sequences ]] for << echo-free speech >> .", "h": ["statistical model of short-term log-energy sequences"], "t": ["echo-free speech"]}, {"label": "USED-FOR", "tokens": "The [[ method ]] has been successfully applied to << robust automatic speech recognition >> in reverberant environments by model selection .", "h": ["method"], "t": ["robust automatic speech recognition"]}, {"label": "FEATURE-OF", "tokens": "The method has been successfully applied to << robust automatic speech recognition >> in [[ reverberant environments ]] by model selection .", "h": ["reverberant environments"], "t": ["robust automatic speech recognition"]}, {"label": "USED-FOR", "tokens": "The << method >> has been successfully applied to robust automatic speech recognition in reverberant environments by [[ model selection ]] .", "h": ["model selection"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "For this application , the << reverberation time >> is first estimated from the [[ reverberated speech utterance ]] to be recognized .", "h": ["reverberated speech utterance"], "t": ["reverberation time"]}, {"label": "USED-FOR", "tokens": "The [[ estimation ]] is then used to select the best << acoustic model >> out of a library of models trained in various artificial re-verberant conditions .", "h": ["estimation"], "t": ["acoustic model"]}, {"label": "PART-OF", "tokens": "The estimation is then used to select the best [[ acoustic model ]] out of a library of << models >> trained in various artificial re-verberant conditions .", "h": ["acoustic model"], "t": ["models"]}, {"label": "FEATURE-OF", "tokens": "The estimation is then used to select the best acoustic model out of a library of << models >> trained in various [[ artificial re-verberant conditions ]] .", "h": ["artificial re-verberant conditions"], "t": ["models"]}, {"label": "EVALUATE-FOR", "tokens": "[[ Speech recognition ]] experiments in simulated and real reverberant environments show the efficiency of our << approach >> which outperforms standard channel normaliza-tion techniques .", "h": ["Speech recognition"], "t": ["approach"]}, {"label": "EVALUATE-FOR", "tokens": "[[ Speech recognition ]] experiments in simulated and real reverberant environments show the efficiency of our approach which outperforms standard << channel normaliza-tion techniques >> .", "h": ["Speech recognition"], "t": ["channel normaliza-tion techniques"]}, {"label": "FEATURE-OF", "tokens": "<< Speech recognition >> experiments in [[ simulated and real reverberant environments ]] show the efficiency of our approach which outperforms standard channel normaliza-tion techniques .", "h": ["simulated and real reverberant environments"], "t": ["Speech recognition"]}, {"label": "COMPARE", "tokens": "Speech recognition experiments in simulated and real reverberant environments show the efficiency of our << approach >> which outperforms standard [[ channel normaliza-tion techniques ]] .", "h": ["channel normaliza-tion techniques"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "For one thing , [[ learning methodology ]] applicable in << general domains >> does not readily lend itself in the linguistic domain .", "h": ["learning methodology"], "t": ["general domains"]}, {"label": "COMPARE", "tokens": "For one thing , learning methodology applicable in [[ general domains ]] does not readily lend itself in the << linguistic domain >> .", "h": ["general domains"], "t": ["linguistic domain"]}, {"label": "USED-FOR", "tokens": "For another , [[ linguistic representation ]] used by << language processing systems >> is not geared to learning .", "h": ["linguistic representation"], "t": ["language processing systems"]}, {"label": "USED-FOR", "tokens": "We introduced a new [[ linguistic representation ]] , the Dynamic Hierarchical Phrasal Lexicon -LRB- DHPL -RRB- -LSB- Zernik88 -RSB- , to facilitate << language acquisition >> .", "h": ["linguistic representation"], "t": ["language acquisition"]}, {"label": "HYPONYM-OF", "tokens": "We introduced a new << linguistic representation >> , the [[ Dynamic Hierarchical Phrasal Lexicon -LRB- DHPL -RRB- ]] -LSB- Zernik88 -RSB- , to facilitate language acquisition .", "h": ["Dynamic Hierarchical Phrasal Lexicon -LRB- DHPL -RRB-"], "t": ["linguistic representation"]}, {"label": "USED-FOR", "tokens": "We introduced a new linguistic representation , the [[ Dynamic Hierarchical Phrasal Lexicon -LRB- DHPL -RRB- ]] -LSB- Zernik88 -RSB- , to facilitate << language acquisition >> .", "h": ["Dynamic Hierarchical Phrasal Lexicon -LRB- DHPL -RRB-"], "t": ["language acquisition"]}, {"label": "PART-OF", "tokens": "From this , a [[ language learning model ]] was implemented in the program << RINA >> , which enhances its own lexical hierarchy by processing examples in context .", "h": ["language learning model"], "t": ["RINA"]}, {"label": "PART-OF", "tokens": "We identified two tasks : First , how [[ linguistic concepts ]] are acquired from training examples and organized in a << hierarchy >> ; this task was discussed in previous papers -LSB- Zernik87 -RSB- .", "h": ["linguistic concepts"], "t": ["hierarchy"]}, {"label": "USED-FOR", "tokens": "Second , we show in this paper how a [[ lexical hierarchy ]] is used in predicting new << linguistic concepts >> .", "h": ["lexical hierarchy"], "t": ["linguistic concepts"]}, {"label": "USED-FOR", "tokens": "This paper presents a novel [[ ensemble learning approach ]] to resolving << German pronouns >> .", "h": ["ensemble learning approach"], "t": ["German pronouns"]}, {"label": "COMPARE", "tokens": "Experiments show that this [[ approach ]] is superior to a single << decision-tree classifier >> .", "h": ["approach"], "t": ["decision-tree classifier"]}, {"label": "USED-FOR", "tokens": "Furthermore , we present a [[ standalone system ]] that resolves << pronouns >> in unannotated text by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process .", "h": ["standalone system"], "t": ["pronouns"]}, {"label": "USED-FOR", "tokens": "Furthermore , we present a [[ standalone system ]] that resolves pronouns in << unannotated text >> by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process .", "h": ["standalone system"], "t": ["unannotated text"]}, {"label": "PART-OF", "tokens": "Furthermore , we present a standalone system that resolves [[ pronouns ]] in << unannotated text >> by using a fully automatic sequence of preprocessing modules that mimics the manual annotation process .", "h": ["pronouns"], "t": ["unannotated text"]}, {"label": "USED-FOR", "tokens": "Furthermore , we present a << standalone system >> that resolves pronouns in unannotated text by using a fully automatic sequence of [[ preprocessing modules ]] that mimics the manual annotation process .", "h": ["preprocessing modules"], "t": ["standalone system"]}, {"label": "USED-FOR", "tokens": "Furthermore , we present a standalone system that resolves pronouns in unannotated text by using a fully automatic sequence of [[ preprocessing modules ]] that mimics the << manual annotation process >> .", "h": ["preprocessing modules"], "t": ["manual annotation process"]}, {"label": "EVALUATE-FOR", "tokens": "Although the << system >> performs well within a limited [[ textual domain ]] , further research is needed to make it effective for open-domain question answering and text summarisation .", "h": ["textual domain"], "t": ["system"]}, {"label": "COMPARE", "tokens": "Although the system performs well within a limited [[ textual domain ]] , further research is needed to make it effective for << open-domain question answering >> and text summarisation .", "h": ["textual domain"], "t": ["open-domain question answering"]}, {"label": "USED-FOR", "tokens": "Although the system performs well within a limited textual domain , further research is needed to make [[ it ]] effective for << open-domain question answering >> and text summarisation .", "h": ["it"], "t": ["open-domain question answering"]}, {"label": "USED-FOR", "tokens": "Although the system performs well within a limited textual domain , further research is needed to make [[ it ]] effective for open-domain question answering and << text summarisation >> .", "h": ["it"], "t": ["text summarisation"]}, {"label": "CONJUNCTION", "tokens": "Although the system performs well within a limited textual domain , further research is needed to make it effective for [[ open-domain question answering ]] and << text summarisation >> .", "h": ["open-domain question answering"], "t": ["text summarisation"]}, {"label": "USED-FOR", "tokens": "In this paper , we compare the performance of a state-of-the-art [[ statistical parser ]] -LRB- Bikel , 2004 -RRB- in << parsing written and spoken language >> and in generating sub-categorization cues from written and spoken language .", "h": ["statistical parser"], "t": ["parsing written and spoken language"]}, {"label": "USED-FOR", "tokens": "In this paper , we compare the performance of a state-of-the-art [[ statistical parser ]] -LRB- Bikel , 2004 -RRB- in parsing written and spoken language and in << generating sub-categorization cues >> from written and spoken language .", "h": ["statistical parser"], "t": ["generating sub-categorization cues"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we compare the performance of a state-of-the-art statistical parser -LRB- Bikel , 2004 -RRB- in [[ parsing written and spoken language ]] and in << generating sub-categorization cues >> from written and spoken language .", "h": ["parsing written and spoken language"], "t": ["generating sub-categorization cues"]}, {"label": "USED-FOR", "tokens": "In this paper , we compare the performance of a state-of-the-art statistical parser -LRB- Bikel , 2004 -RRB- in parsing written and spoken language and in << generating sub-categorization cues >> from [[ written and spoken language ]] .", "h": ["written and spoken language"], "t": ["generating sub-categorization cues"]}, {"label": "USED-FOR", "tokens": "Although [[ Bikel 's parser ]] achieves a higher accuracy for << parsing written language >> , it achieves a higher accuracy when extracting subcategorization cues from spoken language .", "h": ["Bikel 's parser"], "t": ["parsing written language"]}, {"label": "EVALUATE-FOR", "tokens": "Although << Bikel 's parser >> achieves a higher [[ accuracy ]] for parsing written language , it achieves a higher accuracy when extracting subcategorization cues from spoken language .", "h": ["accuracy"], "t": ["Bikel 's parser"]}, {"label": "USED-FOR", "tokens": "Although Bikel 's parser achieves a higher accuracy for parsing written language , [[ it ]] achieves a higher accuracy when extracting << subcategorization cues >> from spoken language .", "h": ["it"], "t": ["subcategorization cues"]}, {"label": "EVALUATE-FOR", "tokens": "Although Bikel 's parser achieves a higher accuracy for parsing written language , << it >> achieves a higher [[ accuracy ]] when extracting subcategorization cues from spoken language .", "h": ["accuracy"], "t": ["it"]}, {"label": "PART-OF", "tokens": "Although Bikel 's parser achieves a higher accuracy for parsing written language , it achieves a higher accuracy when extracting [[ subcategorization cues ]] from << spoken language >> .", "h": ["subcategorization cues"], "t": ["spoken language"]}, {"label": "USED-FOR", "tokens": "Our experiments also show that current [[ technology ]] for << extracting subcategorization frames >> initially designed for written texts works equally well for spoken language .", "h": ["technology"], "t": ["extracting subcategorization frames"]}, {"label": "USED-FOR", "tokens": "Our experiments also show that current [[ technology ]] for extracting subcategorization frames initially designed for written texts works equally well for << spoken language >> .", "h": ["technology"], "t": ["spoken language"]}, {"label": "USED-FOR", "tokens": "Our experiments also show that current technology for [[ extracting subcategorization frames ]] initially designed for << written texts >> works equally well for spoken language .", "h": ["extracting subcategorization frames"], "t": ["written texts"]}, {"label": "COMPARE", "tokens": "Our experiments also show that current technology for extracting subcategorization frames initially designed for [[ written texts ]] works equally well for << spoken language >> .", "h": ["written texts"], "t": ["spoken language"]}, {"label": "USED-FOR", "tokens": "Additionally , we explore the utility of [[ punctuation ]] in helping << parsing >> and extraction of subcategorization cues .", "h": ["punctuation"], "t": ["parsing"]}, {"label": "USED-FOR", "tokens": "Additionally , we explore the utility of [[ punctuation ]] in helping parsing and << extraction of subcategorization cues >> .", "h": ["punctuation"], "t": ["extraction of subcategorization cues"]}, {"label": "CONJUNCTION", "tokens": "Our experiments show that punctuation is of little help in [[ parsing spoken language ]] and << extracting subcategorization cues >> from spoken language .", "h": ["parsing spoken language"], "t": ["extracting subcategorization cues"]}, {"label": "PART-OF", "tokens": "Our experiments show that punctuation is of little help in parsing spoken language and extracting [[ subcategorization cues ]] from << spoken language >> .", "h": ["subcategorization cues"], "t": ["spoken language"]}, {"label": "USED-FOR", "tokens": "Our experiments show that punctuation is of little help in parsing spoken language and << extracting subcategorization cues >> from [[ spoken language ]] .", "h": ["spoken language"], "t": ["extracting subcategorization cues"]}, {"label": "USED-FOR", "tokens": "This paper proposes an [[ alignment adaptation approach ]] to improve << domain-specific -LRB- in-domain -RRB- word alignment >> .", "h": ["alignment adaptation approach"], "t": ["domain-specific -LRB- in-domain -RRB- word alignment"]}, {"label": "USED-FOR", "tokens": "The basic idea of [[ alignment adaptation ]] is to use out-of-domain corpus to improve << in-domain word alignment >> results .", "h": ["alignment adaptation"], "t": ["in-domain word alignment"]}, {"label": "USED-FOR", "tokens": "The basic idea of << alignment adaptation >> is to use [[ out-of-domain corpus ]] to improve in-domain word alignment results .", "h": ["out-of-domain corpus"], "t": ["alignment adaptation"]}, {"label": "USED-FOR", "tokens": "In this paper , we first train two << statistical word alignment models >> with the [[ large-scale out-of-domain corpus ]] and the small-scale in-domain corpus respectively , and then interpolate these two models to improve the domain-specific word alignment .", "h": ["large-scale out-of-domain corpus"], "t": ["statistical word alignment models"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we first train two statistical word alignment models with the [[ large-scale out-of-domain corpus ]] and the << small-scale in-domain corpus >> respectively , and then interpolate these two models to improve the domain-specific word alignment .", "h": ["large-scale out-of-domain corpus"], "t": ["small-scale in-domain corpus"]}, {"label": "USED-FOR", "tokens": "In this paper , we first train two << statistical word alignment models >> with the large-scale out-of-domain corpus and the [[ small-scale in-domain corpus ]] respectively , and then interpolate these two models to improve the domain-specific word alignment .", "h": ["small-scale in-domain corpus"], "t": ["statistical word alignment models"]}, {"label": "USED-FOR", "tokens": "In this paper , we first train two statistical word alignment models with the large-scale out-of-domain corpus and the small-scale in-domain corpus respectively , and then interpolate these two [[ models ]] to improve the << domain-specific word alignment >> .", "h": ["models"], "t": ["domain-specific word alignment"]}, {"label": "USED-FOR", "tokens": "Experimental results show that our [[ approach ]] improves << domain-specific word alignment >> in terms of both precision and recall , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .", "h": ["approach"], "t": ["domain-specific word alignment"]}, {"label": "COMPARE", "tokens": "Experimental results show that our [[ approach ]] improves domain-specific word alignment in terms of both precision and recall , achieving a relative error rate reduction of 6.56 % as compared with the << state-of-the-art technologies >> .", "h": ["approach"], "t": ["state-of-the-art technologies"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results show that our << approach >> improves domain-specific word alignment in terms of both [[ precision ]] and recall , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .", "h": ["precision"], "t": ["approach"]}, {"label": "CONJUNCTION", "tokens": "Experimental results show that our approach improves domain-specific word alignment in terms of both [[ precision ]] and << recall >> , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .", "h": ["precision"], "t": ["recall"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results show that our << approach >> improves domain-specific word alignment in terms of both precision and [[ recall ]] , achieving a relative error rate reduction of 6.56 % as compared with the state-of-the-art technologies .", "h": ["recall"], "t": ["approach"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results show that our << approach >> improves domain-specific word alignment in terms of both precision and recall , achieving a [[ relative error rate reduction ]] of 6.56 % as compared with the state-of-the-art technologies .", "h": ["relative error rate reduction"], "t": ["approach"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results show that our approach improves domain-specific word alignment in terms of both precision and recall , achieving a [[ relative error rate reduction ]] of 6.56 % as compared with the << state-of-the-art technologies >> .", "h": ["relative error rate reduction"], "t": ["state-of-the-art technologies"]}, {"label": "EVALUATE-FOR", "tokens": "With performance above 97 % [[ accuracy ]] for newspaper text , << part of speech -LRB- pos -RRB- tagging >> might be considered a solved problem .", "h": ["accuracy"], "t": ["part of speech -LRB- pos -RRB- tagging"]}, {"label": "EVALUATE-FOR", "tokens": "With performance above 97 % accuracy for [[ newspaper text ]] , << part of speech -LRB- pos -RRB- tagging >> might be considered a solved problem .", "h": ["newspaper text"], "t": ["part of speech -LRB- pos -RRB- tagging"]}, {"label": "USED-FOR", "tokens": "Previous studies have shown that allowing the [[ parser ]] to resolve << pos tag ambiguity >> does not improve performance .", "h": ["parser"], "t": ["pos tag ambiguity"]}, {"label": "USED-FOR", "tokens": "However , for << grammar formalisms >> which use more [[ fine-grained grammatical categories ]] , for example tag and ccg , tagging accuracy is much lower .", "h": ["fine-grained grammatical categories"], "t": ["grammar formalisms"]}, {"label": "HYPONYM-OF", "tokens": "However , for grammar formalisms which use more << fine-grained grammatical categories >> , for example [[ tag ]] and ccg , tagging accuracy is much lower .", "h": ["tag"], "t": ["fine-grained grammatical categories"]}, {"label": "HYPONYM-OF", "tokens": "However , for grammar formalisms which use more << fine-grained grammatical categories >> , for example tag and [[ ccg ]] , tagging accuracy is much lower .", "h": ["ccg"], "t": ["fine-grained grammatical categories"]}, {"label": "EVALUATE-FOR", "tokens": "However , for << grammar formalisms >> which use more fine-grained grammatical categories , for example tag and ccg , [[ tagging accuracy ]] is much lower .", "h": ["tagging accuracy"], "t": ["grammar formalisms"]}, {"label": "USED-FOR", "tokens": "In fact , for these << formalisms >> , premature ambiguity resolution makes [[ parsing ]] infeasible .", "h": ["parsing"], "t": ["formalisms"]}, {"label": "USED-FOR", "tokens": "We describe a [[ multi-tagging approach ]] which maintains a suitable level of lexical category ambiguity for accurate and efficient << ccg parsing >> .", "h": ["multi-tagging approach"], "t": ["ccg parsing"]}, {"label": "FEATURE-OF", "tokens": "We describe a << multi-tagging approach >> which maintains a suitable level of [[ lexical category ambiguity ]] for accurate and efficient ccg parsing .", "h": ["lexical category ambiguity"], "t": ["multi-tagging approach"]}, {"label": "USED-FOR", "tokens": "We extend this [[ multi-tagging approach ]] to the << pos level >> to overcome errors introduced by automatically assigned pos tags .", "h": ["multi-tagging approach"], "t": ["pos level"]}, {"label": "FEATURE-OF", "tokens": "Although pos tagging accuracy seems high , maintaining some [[ pos tag ambiguity ]] in the << language processing pipeline >> results in more accurate ccg supertagging .", "h": ["pos tag ambiguity"], "t": ["language processing pipeline"]}, {"label": "USED-FOR", "tokens": "Although pos tagging accuracy seems high , maintaining some [[ pos tag ambiguity ]] in the language processing pipeline results in more accurate << ccg supertagging >> .", "h": ["pos tag ambiguity"], "t": ["ccg supertagging"]}, {"label": "USED-FOR", "tokens": "We previously presented a [[ framework ]] for << segmentation of complex scenes >> using multiple physical hypotheses for simple image regions .", "h": ["framework"], "t": ["segmentation of complex scenes"]}, {"label": "USED-FOR", "tokens": "We previously presented a << framework >> for segmentation of complex scenes using multiple [[ physical hypotheses ]] for simple image regions .", "h": ["physical hypotheses"], "t": ["framework"]}, {"label": "USED-FOR", "tokens": "We previously presented a framework for segmentation of complex scenes using multiple [[ physical hypotheses ]] for << simple image regions >> .", "h": ["physical hypotheses"], "t": ["simple image regions"]}, {"label": "USED-FOR", "tokens": "A consequence of that [[ framework ]] was a proposal for a new << approach >> to the segmentation of complex scenes into regions corresponding to coherent surfaces rather than merely regions of similar color .", "h": ["framework"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "A consequence of that framework was a proposal for a new [[ approach ]] to the << segmentation of complex scenes >> into regions corresponding to coherent surfaces rather than merely regions of similar color .", "h": ["approach"], "t": ["segmentation of complex scenes"]}, {"label": "COMPARE", "tokens": "A consequence of that framework was a proposal for a new approach to the segmentation of complex scenes into regions corresponding to [[ coherent surfaces ]] rather than merely << regions of similar color >> .", "h": ["coherent surfaces"], "t": ["regions of similar color"]}, {"label": "USED-FOR", "tokens": "Herein we present an implementation of this new approach and show example [[ segmentations ]] for << scenes >> containing multi-colored piece-wise uniform objects .", "h": ["segmentations"], "t": ["scenes"]}, {"label": "FEATURE-OF", "tokens": "Herein we present an implementation of this new approach and show example segmentations for << scenes >> containing [[ multi-colored piece-wise uniform objects ]] .", "h": ["multi-colored piece-wise uniform objects"], "t": ["scenes"]}, {"label": "COMPARE", "tokens": "Using our [[ approach ]] we are able to intelligently segment scenes with objects of greater complexity than previous << physics-based segmentation algorithms >> .", "h": ["approach"], "t": ["physics-based segmentation algorithms"]}, {"label": "HYPONYM-OF", "tokens": "[[ SmartKom ]] is a << multimodal dialog system >> that combines speech , gesture , and mimics input and output .", "h": ["SmartKom"], "t": ["multimodal dialog system"]}, {"label": "USED-FOR", "tokens": "SmartKom is a << multimodal dialog system >> that combines [[ speech ]] , gesture , and mimics input and output .", "h": ["speech"], "t": ["multimodal dialog system"]}, {"label": "CONJUNCTION", "tokens": "SmartKom is a multimodal dialog system that combines [[ speech ]] , << gesture >> , and mimics input and output .", "h": ["speech"], "t": ["gesture"]}, {"label": "USED-FOR", "tokens": "SmartKom is a << multimodal dialog system >> that combines speech , [[ gesture ]] , and mimics input and output .", "h": ["gesture"], "t": ["multimodal dialog system"]}, {"label": "CONJUNCTION", "tokens": "[[ Spontaneous speech understanding ]] is combined with the << video-based recognition of natural gestures >> .", "h": ["Spontaneous speech understanding"], "t": ["video-based recognition of natural gestures"]}, {"label": "USED-FOR", "tokens": "One of the major scientific goals of [[ SmartKom ]] is to design new << computational methods >> for the seamless integration and mutual disambiguation of multimodal input and output on a semantic and pragmatic level .", "h": ["SmartKom"], "t": ["computational methods"]}, {"label": "USED-FOR", "tokens": "One of the major scientific goals of SmartKom is to design new [[ computational methods ]] for the seamless << integration and mutual disambiguation of multimodal input and output >> on a semantic and pragmatic level .", "h": ["computational methods"], "t": ["integration and mutual disambiguation of multimodal input and output"]}, {"label": "FEATURE-OF", "tokens": "One of the major scientific goals of SmartKom is to design new computational methods for the seamless << integration and mutual disambiguation of multimodal input and output >> on a [[ semantic and pragmatic level ]] .", "h": ["semantic and pragmatic level"], "t": ["integration and mutual disambiguation of multimodal input and output"]}, {"label": "USED-FOR", "tokens": "<< SmartKom >> is based on the [[ situated delegation-oriented dialog paradigm ]] , in which the user delegates a task to a virtual communication assistant , visualized as a lifelike character on a graphical display .", "h": ["situated delegation-oriented dialog paradigm"], "t": ["SmartKom"]}, {"label": "USED-FOR", "tokens": "We describe the SmartKom architecture , the use of an [[ XML-based markup language ]] for << multimodal content >> , and some of the distinguishing features of the first fully operational SmartKom demonstrator .", "h": ["XML-based markup language"], "t": ["multimodal content"]}, {"label": "USED-FOR", "tokens": "We present a [[ single-image highlight removal method ]] that incorporates illumination-based constraints into << image in-painting >> .", "h": ["single-image highlight removal method"], "t": ["image in-painting"]}, {"label": "PART-OF", "tokens": "We present a single-image highlight removal method that incorporates [[ illumination-based constraints ]] into << image in-painting >> .", "h": ["illumination-based constraints"], "t": ["image in-painting"]}, {"label": "USED-FOR", "tokens": "[[ Constraints ]] provided by observed pixel colors , highlight color analysis and illumination color uniformity are employed in our << method >> to improve estimation of the underlying diffuse color .", "h": ["Constraints"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "Constraints provided by observed [[ pixel colors ]] , << highlight color analysis >> and illumination color uniformity are employed in our method to improve estimation of the underlying diffuse color .", "h": ["pixel colors"], "t": ["highlight color analysis"]}, {"label": "CONJUNCTION", "tokens": "Constraints provided by observed pixel colors , [[ highlight color analysis ]] and << illumination color uniformity >> are employed in our method to improve estimation of the underlying diffuse color .", "h": ["highlight color analysis"], "t": ["illumination color uniformity"]}, {"label": "USED-FOR", "tokens": "Constraints provided by observed pixel colors , highlight color analysis and illumination color uniformity are employed in our [[ method ]] to improve << estimation of the underlying diffuse color >> .", "h": ["method"], "t": ["estimation of the underlying diffuse color"]}, {"label": "USED-FOR", "tokens": "The inclusion of these [[ illumination constraints ]] allows for better << recovery of shading and textures >> by inpainting .", "h": ["illumination constraints"], "t": ["recovery of shading and textures"]}, {"label": "USED-FOR", "tokens": "The inclusion of these illumination constraints allows for better << recovery of shading and textures >> by [[ inpainting ]] .", "h": ["inpainting"], "t": ["recovery of shading and textures"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a novel [[ method ]] , called local non-negative matrix factorization -LRB- LNMF -RRB- , for learning << spatially localized , parts-based subspace representation of visual patterns >> .", "h": ["method"], "t": ["spatially localized , parts-based subspace representation of visual patterns"]}, {"label": "USED-FOR", "tokens": "An [[ objective function ]] is defined to impose << lo-calization constraint >> , in addition to the non-negativity constraint in the standard NMF -LSB- 1 -RSB- .", "h": ["objective function"], "t": ["lo-calization constraint"]}, {"label": "PART-OF", "tokens": "An objective function is defined to impose lo-calization constraint , in addition to the [[ non-negativity constraint ]] in the standard << NMF >> -LSB- 1 -RSB- .", "h": ["non-negativity constraint"], "t": ["NMF"]}, {"label": "USED-FOR", "tokens": "An [[ algorithm ]] is presented for the << learning >> of such basis components .", "h": ["algorithm"], "t": ["learning"]}, {"label": "COMPARE", "tokens": "Experimental results are presented to compare [[ LNMF ]] with the << NMF and PCA methods >> for face representation and recognition , which demonstrates advantages of LNMF .", "h": ["LNMF"], "t": ["NMF and PCA methods"]}, {"label": "USED-FOR", "tokens": "Experimental results are presented to compare [[ LNMF ]] with the NMF and PCA methods for << face representation and recognition >> , which demonstrates advantages of LNMF .", "h": ["LNMF"], "t": ["face representation and recognition"]}, {"label": "USED-FOR", "tokens": "Experimental results are presented to compare LNMF with the [[ NMF and PCA methods ]] for << face representation and recognition >> , which demonstrates advantages of LNMF .", "h": ["NMF and PCA methods"], "t": ["face representation and recognition"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results are presented to compare LNMF with the NMF and PCA methods for [[ face representation and recognition ]] , which demonstrates advantages of << LNMF >> .", "h": ["face representation and recognition"], "t": ["LNMF"]}, {"label": "CONJUNCTION", "tokens": "Many AI researchers have investigated useful ways of verifying and validating knowledge bases for [[ ontologies ]] and << rules >> , but it is not easy to directly apply them to checking process models .", "h": ["ontologies"], "t": ["rules"]}, {"label": "USED-FOR", "tokens": "Other techniques developed for [[ checking and refining planning knowledge ]] tend to focus on << automated plan generation >> rather than helping users author process information .", "h": ["checking and refining planning knowledge"], "t": ["automated plan generation"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a [[ complementary approach ]] which helps users author and check << process models >> .", "h": ["complementary approach"], "t": ["process models"]}, {"label": "USED-FOR", "tokens": "<< It >> builds [[ interdepen-dency models ]] from this analysis and uses them to find errors and propose fixes .", "h": ["interdepen-dency models"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "It builds interdepen-dency models from this analysis and uses [[ them ]] to find << errors >> and propose fixes .", "h": ["them"], "t": ["errors"]}, {"label": "USED-FOR", "tokens": "It builds interdepen-dency models from this analysis and uses [[ them ]] to find errors and propose << fixes >> .", "h": ["them"], "t": ["fixes"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe the research using [[ machine learning techniques ]] to build a << comma checker >> to be integrated in a grammar checker for Basque .", "h": ["machine learning techniques"], "t": ["comma checker"]}, {"label": "PART-OF", "tokens": "In this paper , we describe the research using machine learning techniques to build a [[ comma checker ]] to be integrated in a << grammar checker >> for Basque .", "h": ["comma checker"], "t": ["grammar checker"]}, {"label": "USED-FOR", "tokens": "In this paper , we describe the research using machine learning techniques to build a comma checker to be integrated in a [[ grammar checker ]] for << Basque >> .", "h": ["grammar checker"], "t": ["Basque"]}, {"label": "EVALUATE-FOR", "tokens": "After several experiments , and trained with a little corpus of 100,000 words , the << system >> guesses correctly not placing commas with a [[ precision ]] of 96 % and a recall of 98 % .", "h": ["precision"], "t": ["system"]}, {"label": "EVALUATE-FOR", "tokens": "After several experiments , and trained with a little corpus of 100,000 words , the << system >> guesses correctly not placing commas with a precision of 96 % and a [[ recall ]] of 98 % .", "h": ["recall"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "[[ It ]] also gets a precision of 70 % and a recall of 49 % in the task of << placing commas >> .", "h": ["It"], "t": ["placing commas"]}, {"label": "EVALUATE-FOR", "tokens": "<< It >> also gets a [[ precision ]] of 70 % and a recall of 49 % in the task of placing commas .", "h": ["precision"], "t": ["It"]}, {"label": "EVALUATE-FOR", "tokens": "<< It >> also gets a precision of 70 % and a [[ recall ]] of 49 % in the task of placing commas .", "h": ["recall"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "The present paper reports on a preparatory research for building a [[ language corpus annotation scenario ]] capturing the << discourse relations >> in Czech .", "h": ["language corpus annotation scenario"], "t": ["discourse relations"]}, {"label": "FEATURE-OF", "tokens": "The present paper reports on a preparatory research for building a language corpus annotation scenario capturing the << discourse relations >> in [[ Czech ]] .", "h": ["Czech"], "t": ["discourse relations"]}, {"label": "USED-FOR", "tokens": "We primarily focus on the description of the << syntactically motivated relations in discourse >> , basing our findings on the theoretical background of the [[ Prague Dependency Treebank 2.0 ]] and the Penn Discourse Treebank 2 .", "h": ["Prague Dependency Treebank 2.0"], "t": ["syntactically motivated relations in discourse"]}, {"label": "CONJUNCTION", "tokens": "We primarily focus on the description of the syntactically motivated relations in discourse , basing our findings on the theoretical background of the [[ Prague Dependency Treebank 2.0 ]] and the << Penn Discourse Treebank 2 >> .", "h": ["Prague Dependency Treebank 2.0"], "t": ["Penn Discourse Treebank 2"]}, {"label": "USED-FOR", "tokens": "We primarily focus on the description of the << syntactically motivated relations in discourse >> , basing our findings on the theoretical background of the Prague Dependency Treebank 2.0 and the [[ Penn Discourse Treebank 2 ]] .", "h": ["Penn Discourse Treebank 2"], "t": ["syntactically motivated relations in discourse"]}, {"label": "PART-OF", "tokens": "Our aim is to revisit the present-day [[ syntactico-semantic -LRB- tectogrammatical -RRB- annotation ]] in the << Prague Dependency Treebank >> , extend it for the purposes of a sentence-boundary-crossing representation and eventually to design a new , discourse level of annotation .", "h": ["syntactico-semantic -LRB- tectogrammatical -RRB- annotation"], "t": ["Prague Dependency Treebank"]}, {"label": "USED-FOR", "tokens": "Our aim is to revisit the present-day syntactico-semantic -LRB- tectogrammatical -RRB- annotation in the Prague Dependency Treebank , extend [[ it ]] for the purposes of a << sentence-boundary-crossing representation >> and eventually to design a new , discourse level of annotation .", "h": ["it"], "t": ["sentence-boundary-crossing representation"]}, {"label": "USED-FOR", "tokens": "Our aim is to revisit the present-day syntactico-semantic -LRB- tectogrammatical -RRB- annotation in the Prague Dependency Treebank , extend [[ it ]] for the purposes of a sentence-boundary-crossing representation and eventually to design a new , << discourse level of annotation >> .", "h": ["it"], "t": ["discourse level of annotation"]}, {"label": "COMPARE", "tokens": "In this paper , we propose a feasible process of such a transfer , comparing the possibilities the << Praguian dependency-based approach >> offers with the [[ Penn discourse annotation ]] based primarily on the analysis and classification of discourse connectives .", "h": ["Penn discourse annotation"], "t": ["Praguian dependency-based approach"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper , we propose a feasible process of such a transfer , comparing the possibilities the << Praguian dependency-based approach >> offers with the Penn discourse annotation based primarily on the [[ analysis and classification of discourse connectives ]] .", "h": ["analysis and classification of discourse connectives"], "t": ["Praguian dependency-based approach"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper , we propose a feasible process of such a transfer , comparing the possibilities the Praguian dependency-based approach offers with the << Penn discourse annotation >> based primarily on the [[ analysis and classification of discourse connectives ]] .", "h": ["analysis and classification of discourse connectives"], "t": ["Penn discourse annotation"]}, {"label": "USED-FOR", "tokens": "[[ Regression-based techniques ]] have shown promising results for << people counting in crowded scenes >> .", "h": ["Regression-based techniques"], "t": ["people counting in crowded scenes"]}, {"label": "USED-FOR", "tokens": "However , most existing << techniques >> require expensive and laborious [[ data annotation ]] for model training .", "h": ["data annotation"], "t": ["techniques"]}, {"label": "USED-FOR", "tokens": "However , most existing techniques require expensive and laborious [[ data annotation ]] for << model training >> .", "h": ["data annotation"], "t": ["model training"]}, {"label": "COMPARE", "tokens": "-LRB- 2 -RRB- Rather than learning from only [[ labelled data ]] , the << abundant unlabelled data >> are exploited .", "h": ["labelled data"], "t": ["abundant unlabelled data"]}, {"label": "USED-FOR", "tokens": "All three ideas are implemented in a [[ unified active and semi-supervised regression framework ]] with ability to perform << transfer learning >> , by exploiting the underlying geometric structure of crowd patterns via manifold analysis .", "h": ["unified active and semi-supervised regression framework"], "t": ["transfer learning"]}, {"label": "USED-FOR", "tokens": "All three ideas are implemented in a << unified active and semi-supervised regression framework >> with ability to perform transfer learning , by exploiting the underlying [[ geometric structure of crowd patterns ]] via manifold analysis .", "h": ["geometric structure of crowd patterns"], "t": ["unified active and semi-supervised regression framework"]}, {"label": "USED-FOR", "tokens": "All three ideas are implemented in a unified active and semi-supervised regression framework with ability to perform transfer learning , by exploiting the underlying << geometric structure of crowd patterns >> via [[ manifold analysis ]] .", "h": ["manifold analysis"], "t": ["geometric structure of crowd patterns"]}, {"label": "USED-FOR", "tokens": "[[ Representing images with layers ]] has many important << applications >> , such as video compression , motion analysis , and 3D scene analysis .", "h": ["Representing images with layers"], "t": ["applications"]}, {"label": "HYPONYM-OF", "tokens": "Representing images with layers has many important << applications >> , such as [[ video compression ]] , motion analysis , and 3D scene analysis .", "h": ["video compression"], "t": ["applications"]}, {"label": "CONJUNCTION", "tokens": "Representing images with layers has many important applications , such as [[ video compression ]] , << motion analysis >> , and 3D scene analysis .", "h": ["video compression"], "t": ["motion analysis"]}, {"label": "HYPONYM-OF", "tokens": "Representing images with layers has many important << applications >> , such as video compression , [[ motion analysis ]] , and 3D scene analysis .", "h": ["motion analysis"], "t": ["applications"]}, {"label": "CONJUNCTION", "tokens": "Representing images with layers has many important applications , such as video compression , [[ motion analysis ]] , and << 3D scene analysis >> .", "h": ["motion analysis"], "t": ["3D scene analysis"]}, {"label": "HYPONYM-OF", "tokens": "Representing images with layers has many important << applications >> , such as video compression , motion analysis , and [[ 3D scene analysis ]] .", "h": ["3D scene analysis"], "t": ["applications"]}, {"label": "USED-FOR", "tokens": "This paper presents an [[ approach ]] to reliably extracting << layers >> from images by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace .", "h": ["approach"], "t": ["layers"]}, {"label": "PART-OF", "tokens": "This paper presents an approach to reliably extracting [[ layers ]] from << images >> by taking advantages of the fact that homographies induced by planar patches in the scene form a low dimensional linear subspace .", "h": ["layers"], "t": ["images"]}, {"label": "PART-OF", "tokens": "This paper presents an approach to reliably extracting layers from images by taking advantages of the fact that homographies induced by [[ planar patches ]] in the << scene >> form a low dimensional linear subspace .", "h": ["planar patches"], "t": ["scene"]}, {"label": "PART-OF", "tokens": "[[ Layers ]] in the input << images >> will be mapped in the subspace , where it is proven that they form well-defined clusters and can be reliably identified by a simple mean-shift based clustering algorithm .", "h": ["Layers"], "t": ["images"]}, {"label": "USED-FOR", "tokens": "Layers in the input [[ images ]] will be mapped in the subspace , where it is proven that they form well-defined << clusters >> and can be reliably identified by a simple mean-shift based clustering algorithm .", "h": ["images"], "t": ["clusters"]}, {"label": "USED-FOR", "tokens": "Layers in the input images will be mapped in the subspace , where it is proven that they form well-defined << clusters >> and can be reliably identified by a simple [[ mean-shift based clustering algorithm ]] .", "h": ["mean-shift based clustering algorithm"], "t": ["clusters"]}, {"label": "USED-FOR", "tokens": "Global optimality is achieved since all valid regions are simultaneously taken into account , and << noise >> can be effectively reduced by enforcing the [[ subspace constraint ]] .", "h": ["subspace constraint"], "t": ["noise"]}, {"label": "USED-FOR", "tokens": "The << construction of causal graphs >> from [[ non-experimental data ]] rests on a set of constraints that the graph structure imposes on all probability distributions compatible with the graph .", "h": ["non-experimental data"], "t": ["construction of causal graphs"]}, {"label": "FEATURE-OF", "tokens": "The construction of causal graphs from non-experimental data rests on a set of constraints that the graph structure imposes on all [[ probability distributions ]] compatible with the << graph >> .", "h": ["probability distributions"], "t": ["graph"]}, {"label": "HYPONYM-OF", "tokens": "These << constraints >> are of two types : [[ conditional inde-pendencies ]] and algebraic constraints , first noted by Verma .", "h": ["conditional inde-pendencies"], "t": ["constraints"]}, {"label": "HYPONYM-OF", "tokens": "These << constraints >> are of two types : conditional inde-pendencies and [[ algebraic constraints ]] , first noted by Verma .", "h": ["algebraic constraints"], "t": ["constraints"]}, {"label": "USED-FOR", "tokens": "While [[ conditional independencies ]] are well studied and frequently used in << causal induction algorithms >> , Verma constraints are still poorly understood , and rarely applied .", "h": ["conditional independencies"], "t": ["causal induction algorithms"]}, {"label": "COMPARE", "tokens": "While << conditional independencies >> are well studied and frequently used in causal induction algorithms , [[ Verma constraints ]] are still poorly understood , and rarely applied .", "h": ["Verma constraints"], "t": ["conditional independencies"]}, {"label": "CONJUNCTION", "tokens": "In this paper we examine a special subset of Verma constraints which are easy to understand , easy to identify and easy to apply ; they arise from '' [[ dormant independencies ]] , '' namely , << conditional independencies >> that hold in interventional distributions .", "h": ["dormant independencies"], "t": ["conditional independencies"]}, {"label": "FEATURE-OF", "tokens": "In this paper we examine a special subset of Verma constraints which are easy to understand , easy to identify and easy to apply ; they arise from '' dormant independencies , '' namely , [[ conditional independencies ]] that hold in << interventional distributions >> .", "h": ["conditional independencies"], "t": ["interventional distributions"]}, {"label": "USED-FOR", "tokens": "We give a complete [[ algorithm ]] for determining if a << dormant independence >> between two sets of variables is entailed by the causal graph , such that this independence is identifiable , in other words if it resides in an interventional distribution that can be predicted without resorting to interventions .", "h": ["algorithm"], "t": ["dormant independence"]}, {"label": "FEATURE-OF", "tokens": "We give a complete algorithm for determining if a dormant independence between two sets of variables is entailed by the causal graph , such that this independence is identifiable , in other words if << it >> resides in an [[ interventional distribution ]] that can be predicted without resorting to interventions .", "h": ["interventional distribution"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "We further show the usefulness of [[ dormant independencies ]] in << model testing >> and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph .", "h": ["dormant independencies"], "t": ["model testing"]}, {"label": "USED-FOR", "tokens": "We further show the usefulness of [[ dormant independencies ]] in model testing and << induction >> by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph .", "h": ["dormant independencies"], "t": ["induction"]}, {"label": "CONJUNCTION", "tokens": "We further show the usefulness of dormant independencies in [[ model testing ]] and << induction >> by giving an algorithm that uses constraints entailed by dormant independencies to prune extraneous edges from a given causal graph .", "h": ["model testing"], "t": ["induction"]}, {"label": "USED-FOR", "tokens": "We further show the usefulness of dormant independencies in model testing and induction by giving an [[ algorithm ]] that uses constraints entailed by dormant independencies to prune << extraneous edges >> from a given causal graph .", "h": ["algorithm"], "t": ["extraneous edges"]}, {"label": "USED-FOR", "tokens": "We further show the usefulness of dormant independencies in model testing and induction by giving an << algorithm >> that uses [[ constraints ]] entailed by dormant independencies to prune extraneous edges from a given causal graph .", "h": ["constraints"], "t": ["algorithm"]}, {"label": "PART-OF", "tokens": "We further show the usefulness of dormant independencies in model testing and induction by giving an algorithm that uses constraints entailed by dormant independencies to prune [[ extraneous edges ]] from a given << causal graph >> .", "h": ["extraneous edges"], "t": ["causal graph"]}, {"label": "FEATURE-OF", "tokens": "With the recent popularity of << animated GIFs >> on [[ social media ]] , there is need for ways to index them with rich meta-data .", "h": ["social media"], "t": ["animated GIFs"]}, {"label": "USED-FOR", "tokens": "To advance research on << animated GIF understanding >> , we collected a new [[ dataset ]] , Tumblr GIF -LRB- TGIF -RRB- , with 100K animated GIFs from Tumblr and 120K natural language descriptions obtained via crowdsourcing .", "h": ["dataset"], "t": ["animated GIF understanding"]}, {"label": "CONJUNCTION", "tokens": "To advance research on animated GIF understanding , we collected a new dataset , Tumblr GIF -LRB- TGIF -RRB- , with 100K << animated GIFs >> from Tumblr and 120K [[ natural language descriptions ]] obtained via crowdsourcing .", "h": ["natural language descriptions"], "t": ["animated GIFs"]}, {"label": "USED-FOR", "tokens": "To advance research on animated GIF understanding , we collected a new dataset , Tumblr GIF -LRB- TGIF -RRB- , with 100K animated GIFs from Tumblr and 120K << natural language descriptions >> obtained via [[ crowdsourcing ]] .", "h": ["crowdsourcing"], "t": ["natural language descriptions"]}, {"label": "USED-FOR", "tokens": "The motivation for this work is to develop a testbed for image sequence description systems , where the task is to generate [[ natural language descriptions ]] for << animated GIFs >> or video clips .", "h": ["natural language descriptions"], "t": ["animated GIFs"]}, {"label": "USED-FOR", "tokens": "The motivation for this work is to develop a testbed for image sequence description systems , where the task is to generate [[ natural language descriptions ]] for animated GIFs or << video clips >> .", "h": ["natural language descriptions"], "t": ["video clips"]}, {"label": "CONJUNCTION", "tokens": "The motivation for this work is to develop a testbed for image sequence description systems , where the task is to generate natural language descriptions for [[ animated GIFs ]] or << video clips >> .", "h": ["animated GIFs"], "t": ["video clips"]}, {"label": "USED-FOR", "tokens": "To ensure a high quality dataset , we developed a series of novel [[ quality controls ]] to validate << free-form text input >> from crowd-workers .", "h": ["quality controls"], "t": ["free-form text input"]}, {"label": "CONJUNCTION", "tokens": "We show that there is unambiguous association between [[ visual content ]] and << natural language descriptions >> in our dataset , making it an ideal benchmark for the visual content captioning task .", "h": ["visual content"], "t": ["natural language descriptions"]}, {"label": "PART-OF", "tokens": "We show that there is unambiguous association between [[ visual content ]] and natural language descriptions in our << dataset >> , making it an ideal benchmark for the visual content captioning task .", "h": ["visual content"], "t": ["dataset"]}, {"label": "PART-OF", "tokens": "We show that there is unambiguous association between visual content and [[ natural language descriptions ]] in our << dataset >> , making it an ideal benchmark for the visual content captioning task .", "h": ["natural language descriptions"], "t": ["dataset"]}, {"label": "EVALUATE-FOR", "tokens": "We show that there is unambiguous association between visual content and natural language descriptions in our dataset , making [[ it ]] an ideal benchmark for the << visual content captioning task >> .", "h": ["it"], "t": ["visual content captioning task"]}, {"label": "COMPARE", "tokens": "We perform extensive statistical analyses to compare our [[ dataset ]] to existing << image and video description datasets >> .", "h": ["dataset"], "t": ["image and video description datasets"]}, {"label": "USED-FOR", "tokens": "Next , we provide baseline results on the << animated GIF description task >> , using three [[ representative techniques ]] : nearest neighbor , statistical machine translation , and recurrent neural networks .", "h": ["representative techniques"], "t": ["animated GIF description task"]}, {"label": "HYPONYM-OF", "tokens": "Next , we provide baseline results on the animated GIF description task , using three << representative techniques >> : [[ nearest neighbor ]] , statistical machine translation , and recurrent neural networks .", "h": ["nearest neighbor"], "t": ["representative techniques"]}, {"label": "CONJUNCTION", "tokens": "Next , we provide baseline results on the animated GIF description task , using three representative techniques : [[ nearest neighbor ]] , << statistical machine translation >> , and recurrent neural networks .", "h": ["nearest neighbor"], "t": ["statistical machine translation"]}, {"label": "HYPONYM-OF", "tokens": "Next , we provide baseline results on the animated GIF description task , using three << representative techniques >> : nearest neighbor , [[ statistical machine translation ]] , and recurrent neural networks .", "h": ["statistical machine translation"], "t": ["representative techniques"]}, {"label": "CONJUNCTION", "tokens": "Next , we provide baseline results on the animated GIF description task , using three representative techniques : nearest neighbor , [[ statistical machine translation ]] , and << recurrent neural networks >> .", "h": ["statistical machine translation"], "t": ["recurrent neural networks"]}, {"label": "HYPONYM-OF", "tokens": "Next , we provide baseline results on the animated GIF description task , using three << representative techniques >> : nearest neighbor , statistical machine translation , and [[ recurrent neural networks ]] .", "h": ["recurrent neural networks"], "t": ["representative techniques"]}, {"label": "USED-FOR", "tokens": "Finally , we show that models fine-tuned from our [[ animated GIF description dataset ]] can be helpful for << automatic movie description >> .", "h": ["animated GIF description dataset"], "t": ["automatic movie description"]}, {"label": "USED-FOR", "tokens": "[[ Systemic grammar ]] has been used for << AI text generation >> work in the past , but the implementations have tended be ad hoc or inefficient .", "h": ["Systemic grammar"], "t": ["AI text generation"]}, {"label": "USED-FOR", "tokens": "This paper presents an [[ approach ]] to systemic << text generation >> where AI problem solving techniques are applied directly to an unadulterated systemic grammar .", "h": ["approach"], "t": ["text generation"]}, {"label": "USED-FOR", "tokens": "This paper presents an approach to systemic text generation where [[ AI problem solving techniques ]] are applied directly to an unadulterated << systemic grammar >> .", "h": ["AI problem solving techniques"], "t": ["systemic grammar"]}, {"label": "CONJUNCTION", "tokens": "This approach is made possible by a special relationship between [[ systemic grammar ]] and << problem solving >> : both are organized primarily as choosing from alternatives .", "h": ["systemic grammar"], "t": ["problem solving"]}, {"label": "USED-FOR", "tokens": "The result is simple , efficient << text generation >> firmly based in a [[ linguistic theory ]] .", "h": ["linguistic theory"], "t": ["text generation"]}, {"label": "USED-FOR", "tokens": "In this paper a novel [[ solution ]] to << automatic and unsupervised word sense induction -LRB- WSI -RRB- >> is introduced .", "h": ["solution"], "t": ["automatic and unsupervised word sense induction -LRB- WSI -RRB-"]}, {"label": "HYPONYM-OF", "tokens": "[[ It ]] represents an instantiation of the << one sense per collocation observation >> -LRB- Gale et al. , 1992 -RRB- .", "h": ["It"], "t": ["one sense per collocation observation"]}, {"label": "USED-FOR", "tokens": "Like most existing approaches << it >> utilizes [[ clustering of word co-occurrences ]] .", "h": ["clustering of word co-occurrences"], "t": ["it"]}, {"label": "COMPARE", "tokens": "This [[ approach ]] differs from other << approaches >> to WSI in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs .", "h": ["approach"], "t": ["approaches"]}, {"label": "USED-FOR", "tokens": "This [[ approach ]] differs from other approaches to << WSI >> in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs .", "h": ["approach"], "t": ["WSI"]}, {"label": "USED-FOR", "tokens": "This approach differs from other [[ approaches ]] to << WSI >> in that it enhances the effect of the one sense per collocation observation by using triplets of words instead of pairs .", "h": ["approaches"], "t": ["WSI"]}, {"label": "USED-FOR", "tokens": "This approach differs from other approaches to WSI in that [[ it ]] enhances the effect of the << one sense per collocation observation >> by using triplets of words instead of pairs .", "h": ["it"], "t": ["one sense per collocation observation"]}, {"label": "USED-FOR", "tokens": "This approach differs from other approaches to WSI in that << it >> enhances the effect of the one sense per collocation observation by using [[ triplets of words ]] instead of pairs .", "h": ["triplets of words"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "The combination with a << two-step clustering process >> using [[ sentence co-occurrences ]] as features allows for accurate results .", "h": ["sentence co-occurrences"], "t": ["two-step clustering process"]}, {"label": "EVALUATE-FOR", "tokens": "Additionally , a novel and likewise [[ automatic and unsupervised evaluation method ]] inspired by Schutze 's -LRB- 1992 -RRB- idea of evaluation of << word sense disambiguation algorithms >> is employed .", "h": ["automatic and unsupervised evaluation method"], "t": ["word sense disambiguation algorithms"]}, {"label": "USED-FOR", "tokens": "Offering advantages like reproducability and independency of a given biased gold standard it also enables [[ automatic parameter optimization ]] of the << WSI algorithm >> .", "h": ["automatic parameter optimization"], "t": ["WSI algorithm"]}, {"label": "USED-FOR", "tokens": "This abstract describes a [[ natural language system ]] which deals usefully with << ungrammatical input >> and describes some actual and potential applications of it in computer aided second language learning .", "h": ["natural language system"], "t": ["ungrammatical input"]}, {"label": "USED-FOR", "tokens": "This abstract describes a natural language system which deals usefully with ungrammatical input and describes some actual and potential applications of [[ it ]] in << computer aided second language learning >> .", "h": ["it"], "t": ["computer aided second language learning"]}, {"label": "USED-FOR", "tokens": "However , << this >> is not the only area in which the principles of the [[ system ]] might be used , and the aim in building it was simply to demonstrate the workability of the general mechanism , and provide a framework for assessing developments of it .", "h": ["system"], "t": ["this"]}, {"label": "USED-FOR", "tokens": "In a motorized vehicle a number of easily << measurable signals >> with [[ frequency components ]] related to the rotational speed of the engine can be found , e.g. , vibrations , electrical system voltage level , and ambient sound .", "h": ["frequency components"], "t": ["measurable signals"]}, {"label": "FEATURE-OF", "tokens": "In a motorized vehicle a number of easily measurable signals with [[ frequency components ]] related to the << rotational speed of the engine >> can be found , e.g. , vibrations , electrical system voltage level , and ambient sound .", "h": ["frequency components"], "t": ["rotational speed of the engine"]}, {"label": "HYPONYM-OF", "tokens": "In a motorized vehicle a number of easily << measurable signals >> with frequency components related to the rotational speed of the engine can be found , e.g. , [[ vibrations ]] , electrical system voltage level , and ambient sound .", "h": ["vibrations"], "t": ["measurable signals"]}, {"label": "CONJUNCTION", "tokens": "In a motorized vehicle a number of easily measurable signals with frequency components related to the rotational speed of the engine can be found , e.g. , [[ vibrations ]] , << electrical system voltage level >> , and ambient sound .", "h": ["vibrations"], "t": ["electrical system voltage level"]}, {"label": "HYPONYM-OF", "tokens": "In a motorized vehicle a number of easily << measurable signals >> with frequency components related to the rotational speed of the engine can be found , e.g. , vibrations , [[ electrical system voltage level ]] , and ambient sound .", "h": ["electrical system voltage level"], "t": ["measurable signals"]}, {"label": "CONJUNCTION", "tokens": "In a motorized vehicle a number of easily measurable signals with frequency components related to the rotational speed of the engine can be found , e.g. , vibrations , [[ electrical system voltage level ]] , and << ambient sound >> .", "h": ["electrical system voltage level"], "t": ["ambient sound"]}, {"label": "HYPONYM-OF", "tokens": "In a motorized vehicle a number of easily << measurable signals >> with frequency components related to the rotational speed of the engine can be found , e.g. , vibrations , electrical system voltage level , and [[ ambient sound ]] .", "h": ["ambient sound"], "t": ["measurable signals"]}, {"label": "USED-FOR", "tokens": "These [[ signals ]] could potentially be used to estimate the << speed and related states of the vehicle >> .", "h": ["signals"], "t": ["speed and related states of the vehicle"]}, {"label": "CONJUNCTION", "tokens": "Unfortunately , such estimates would typically require the relations -LRB- scale factors -RRB- between the [[ frequency components ]] and the << speed >> for different gears to be known .", "h": ["frequency components"], "t": ["speed"]}, {"label": "FEATURE-OF", "tokens": "Unfortunately , such estimates would typically require the relations -LRB- scale factors -RRB- between the frequency components and the [[ speed ]] for different << gears >> to be known .", "h": ["speed"], "t": ["gears"]}, {"label": "USED-FOR", "tokens": "Consequently , in this article we look at the problem of estimating these << gear scale factors >> from [[ training data ]] consisting only of speed measurements and measurements of the signal in question .", "h": ["training data"], "t": ["gear scale factors"]}, {"label": "USED-FOR", "tokens": "The << estimation problem >> is formulated as a [[ maximum likelihood estimation problem ]] and heuristics is used to find initial values for a numerical evaluation of the estimator .", "h": ["maximum likelihood estimation problem"], "t": ["estimation problem"]}, {"label": "USED-FOR", "tokens": "The estimation problem is formulated as a maximum likelihood estimation problem and [[ heuristics ]] is used to find initial values for a << numerical evaluation of the estimator >> .", "h": ["heuristics"], "t": ["numerical evaluation of the estimator"]}, {"label": "EVALUATE-FOR", "tokens": "Finally , a measurement campaign is conducted and the functionality of the << estimation method >> is verified on [[ real data ]] .", "h": ["real data"], "t": ["estimation method"]}, {"label": "FEATURE-OF", "tokens": "<< LPC based speech coders >> operating at [[ bit rates ]] below 3.0 kbits/sec are usually associated with buzzy or metallic artefacts in the synthetic speech .", "h": ["bit rates"], "t": ["LPC based speech coders"]}, {"label": "FEATURE-OF", "tokens": "LPC based speech coders operating at bit rates below 3.0 kbits/sec are usually associated with [[ buzzy or metallic artefacts ]] in the << synthetic speech >> .", "h": ["buzzy or metallic artefacts"], "t": ["synthetic speech"]}, {"label": "USED-FOR", "tokens": "In this paper a new LPC vocoder is presented which splits the << LPC excitation >> into two frequency bands using a [[ variable cutoff frequency ]] .", "h": ["variable cutoff frequency"], "t": ["LPC excitation"]}, {"label": "USED-FOR", "tokens": "In this paper a new LPC vocoder is presented which splits the LPC excitation into two << frequency bands >> using a [[ variable cutoff frequency ]] .", "h": ["variable cutoff frequency"], "t": ["frequency bands"]}, {"label": "USED-FOR", "tokens": "In doing so the [[ coder ]] 's performance during both mixed voicing speech and speech containing acoustic noise is greatly improved , producing << soft natural sounding speech >> .", "h": ["coder"], "t": ["soft natural sounding speech"]}, {"label": "USED-FOR", "tokens": "In doing so the << coder >> 's performance during both [[ mixed voicing speech ]] and speech containing acoustic noise is greatly improved , producing soft natural sounding speech .", "h": ["mixed voicing speech"], "t": ["coder"]}, {"label": "USED-FOR", "tokens": "In doing so the << coder >> 's performance during both mixed voicing speech and [[ speech containing acoustic noise ]] is greatly improved , producing soft natural sounding speech .", "h": ["speech containing acoustic noise"], "t": ["coder"]}, {"label": "CONJUNCTION", "tokens": "The paper also describes new [[ parameter determination ]] and << quantisation techniques >> vital to the operation of this coder at such low bit rates .", "h": ["parameter determination"], "t": ["quantisation techniques"]}, {"label": "USED-FOR", "tokens": "The paper also describes new [[ parameter determination ]] and quantisation techniques vital to the operation of this << coder >> at such low bit rates .", "h": ["parameter determination"], "t": ["coder"]}, {"label": "USED-FOR", "tokens": "The paper also describes new parameter determination and [[ quantisation techniques ]] vital to the operation of this << coder >> at such low bit rates .", "h": ["quantisation techniques"], "t": ["coder"]}, {"label": "FEATURE-OF", "tokens": "The paper also describes new parameter determination and quantisation techniques vital to the operation of this << coder >> at such [[ low bit rates ]] .", "h": ["low bit rates"], "t": ["coder"]}, {"label": "USED-FOR", "tokens": "We consider a problem of << blind source separation >> from a set of [[ instantaneous linear mixtures ]] , where the mixing matrix is unknown .", "h": ["instantaneous linear mixtures"], "t": ["blind source separation"]}, {"label": "USED-FOR", "tokens": "It was discovered recently , that exploiting the << sparsity of sources >> in an appropriate representation according to some [[ signal dictionary ]] , dramatically improves the quality of separation .", "h": ["signal dictionary"], "t": ["sparsity of sources"]}, {"label": "EVALUATE-FOR", "tokens": "It was discovered recently , that exploiting the << sparsity of sources >> in an appropriate representation according to some signal dictionary , dramatically improves the [[ quality of separation ]] .", "h": ["quality of separation"], "t": ["sparsity of sources"]}, {"label": "HYPONYM-OF", "tokens": "In this work we use the property of << multi scale transforms >> , such as [[ wavelet or wavelet packets ]] , to decompose signals into sets of local features with various degrees of sparsity .", "h": ["wavelet or wavelet packets"], "t": ["multi scale transforms"]}, {"label": "EVALUATE-FOR", "tokens": "The performance of the << algorithm >> is verified on [[ noise-free and noisy data ]] .", "h": ["noise-free and noisy data"], "t": ["algorithm"]}, {"label": "CONJUNCTION", "tokens": "Experiments with [[ simulated signals ]] , << musical sounds >> and images demonstrate significant improvement of separation quality over previously reported results .", "h": ["simulated signals"], "t": ["musical sounds"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments with [[ simulated signals ]] , musical sounds and images demonstrate significant improvement of << separation quality >> over previously reported results .", "h": ["simulated signals"], "t": ["separation quality"]}, {"label": "CONJUNCTION", "tokens": "Experiments with simulated signals , [[ musical sounds ]] and << images >> demonstrate significant improvement of separation quality over previously reported results .", "h": ["musical sounds"], "t": ["images"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments with simulated signals , [[ musical sounds ]] and images demonstrate significant improvement of << separation quality >> over previously reported results .", "h": ["musical sounds"], "t": ["separation quality"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments with simulated signals , musical sounds and [[ images ]] demonstrate significant improvement of << separation quality >> over previously reported results .", "h": ["images"], "t": ["separation quality"]}, {"label": "USED-FOR", "tokens": "In this paper , we explore << multilingual feature-level data sharing >> via [[ Deep Neural Network -LRB- DNN -RRB- stacked bottleneck features ]] .", "h": ["Deep Neural Network -LRB- DNN -RRB- stacked bottleneck features"], "t": ["multilingual feature-level data sharing"]}, {"label": "USED-FOR", "tokens": "Given a set of available source languages , we apply [[ language identification ]] to pick the language most similar to the target language , for more efficient use of << multilingual resources >> .", "h": ["language identification"], "t": ["multilingual resources"]}, {"label": "COMPARE", "tokens": "Our experiments with IARPA-Babel languages show that << bottleneck features >> trained on the most similar source language perform better than [[ those ]] trained on all available source languages .", "h": ["those"], "t": ["bottleneck features"]}, {"label": "USED-FOR", "tokens": "Further analysis suggests that only [[ data ]] similar to the target language is useful for << multilingual training >> .", "h": ["data"], "t": ["multilingual training"]}, {"label": "USED-FOR", "tokens": "This article introduces a [[ bidirectional grammar generation system ]] called feature structure-directed generation , developed for a << dialogue translation system >> .", "h": ["bidirectional grammar generation system"], "t": ["dialogue translation system"]}, {"label": "HYPONYM-OF", "tokens": "This article introduces a << bidirectional grammar generation system >> called [[ feature structure-directed generation ]] , developed for a dialogue translation system .", "h": ["feature structure-directed generation"], "t": ["bidirectional grammar generation system"]}, {"label": "USED-FOR", "tokens": "This article introduces a bidirectional grammar generation system called [[ feature structure-directed generation ]] , developed for a << dialogue translation system >> .", "h": ["feature structure-directed generation"], "t": ["dialogue translation system"]}, {"label": "USED-FOR", "tokens": "The << system >> utilizes [[ typed feature structures ]] to control the top-down derivation in a declarative way .", "h": ["typed feature structures"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "The system utilizes [[ typed feature structures ]] to control the << top-down derivation >> in a declarative way .", "h": ["typed feature structures"], "t": ["top-down derivation"]}, {"label": "USED-FOR", "tokens": "This << generation system >> also uses [[ disjunctive feature structures ]] to reduce the number of copies of the derivation tree .", "h": ["disjunctive feature structures"], "t": ["generation system"]}, {"label": "USED-FOR", "tokens": "This generation system also uses [[ disjunctive feature structures ]] to reduce the number of copies of the << derivation tree >> .", "h": ["disjunctive feature structures"], "t": ["derivation tree"]}, {"label": "USED-FOR", "tokens": "The [[ grammar ]] for this << generator >> is designed to properly generate the speaker 's intention in a telephone dialogue .", "h": ["grammar"], "t": ["generator"]}, {"label": "USED-FOR", "tokens": "The [[ grammar ]] for this generator is designed to properly generate the << speaker 's intention >> in a telephone dialogue .", "h": ["grammar"], "t": ["speaker 's intention"]}, {"label": "FEATURE-OF", "tokens": "The grammar for this generator is designed to properly generate the << speaker 's intention >> in a [[ telephone dialogue ]] .", "h": ["telephone dialogue"], "t": ["speaker 's intention"]}, {"label": "USED-FOR", "tokens": "[[ Automatic image annotation ]] is a newly developed and promising technique to provide << semantic image retrieval >> via text descriptions .", "h": ["Automatic image annotation"], "t": ["semantic image retrieval"]}, {"label": "USED-FOR", "tokens": "Automatic image annotation is a newly developed and promising technique to provide << semantic image retrieval >> via [[ text descriptions ]] .", "h": ["text descriptions"], "t": ["semantic image retrieval"]}, {"label": "USED-FOR", "tokens": "It concerns a process of << automatically labeling the image contents >> with a pre-defined set of [[ keywords ]] which are exploited to represent the image semantics .", "h": ["keywords"], "t": ["automatically labeling the image contents"]}, {"label": "USED-FOR", "tokens": "It concerns a process of automatically labeling the image contents with a pre-defined set of [[ keywords ]] which are exploited to represent the << image semantics >> .", "h": ["keywords"], "t": ["image semantics"]}, {"label": "USED-FOR", "tokens": "A [[ Maximum Entropy Model-based approach ]] to the task of << automatic image annotation >> is proposed in this paper .", "h": ["Maximum Entropy Model-based approach"], "t": ["automatic image annotation"]}, {"label": "USED-FOR", "tokens": "In the phase of training , a basic [[ visual vocabulary ]] consisting of blob-tokens to describe the << image content >> is generated at first ; then the statistical relationship is modeled between the blob-tokens and keywords by a Maximum Entropy Model constructed from the training set of labeled images .", "h": ["visual vocabulary"], "t": ["image content"]}, {"label": "PART-OF", "tokens": "In the phase of training , a basic << visual vocabulary >> consisting of [[ blob-tokens ]] to describe the image content is generated at first ; then the statistical relationship is modeled between the blob-tokens and keywords by a Maximum Entropy Model constructed from the training set of labeled images .", "h": ["blob-tokens"], "t": ["visual vocabulary"]}, {"label": "USED-FOR", "tokens": "In the phase of training , a basic visual vocabulary consisting of blob-tokens to describe the image content is generated at first ; then the << statistical relationship >> is modeled between the blob-tokens and keywords by a [[ Maximum Entropy Model ]] constructed from the training set of labeled images .", "h": ["Maximum Entropy Model"], "t": ["statistical relationship"]}, {"label": "USED-FOR", "tokens": "In the phase of annotation , for an unlabeled image , the most likely associated << keywords >> are predicted in terms of the [[ blob-token set ]] extracted from the given image .", "h": ["blob-token set"], "t": ["keywords"]}, {"label": "USED-FOR", "tokens": "We carried out experiments on a << medium-sized image collection >> with about 5000 images from [[ Corel Photo CDs ]] .", "h": ["Corel Photo CDs"], "t": ["medium-sized image collection"]}, {"label": "USED-FOR", "tokens": "The experimental results demonstrated that the << annotation >> performance of this [[ method ]] outperforms some traditional annotation methods by about 8 % in mean precision , showing a potential of the Maximum Entropy Model in the task of automatic image annotation .", "h": ["method"], "t": ["annotation"]}, {"label": "COMPARE", "tokens": "The experimental results demonstrated that the annotation performance of this [[ method ]] outperforms some traditional << annotation methods >> by about 8 % in mean precision , showing a potential of the Maximum Entropy Model in the task of automatic image annotation .", "h": ["method"], "t": ["annotation methods"]}, {"label": "USED-FOR", "tokens": "The experimental results demonstrated that the << annotation >> performance of this method outperforms some traditional [[ annotation methods ]] by about 8 % in mean precision , showing a potential of the Maximum Entropy Model in the task of automatic image annotation .", "h": ["annotation methods"], "t": ["annotation"]}, {"label": "EVALUATE-FOR", "tokens": "The experimental results demonstrated that the annotation performance of this method outperforms some traditional << annotation methods >> by about 8 % in [[ mean precision ]] , showing a potential of the Maximum Entropy Model in the task of automatic image annotation .", "h": ["mean precision"], "t": ["annotation methods"]}, {"label": "USED-FOR", "tokens": "The experimental results demonstrated that the annotation performance of this method outperforms some traditional annotation methods by about 8 % in mean precision , showing a potential of the [[ Maximum Entropy Model ]] in the task of << automatic image annotation >> .", "h": ["Maximum Entropy Model"], "t": ["automatic image annotation"]}, {"label": "FEATURE-OF", "tokens": "However most of the works found in the literature have focused on identifying and understanding << temporal expressions >> in [[ newswire texts ]] .", "h": ["newswire texts"], "t": ["temporal expressions"]}, {"label": "FEATURE-OF", "tokens": "In this paper we report our work on anchoring << temporal expressions >> in a novel genre , [[ emails ]] .", "h": ["emails"], "t": ["temporal expressions"]}, {"label": "HYPONYM-OF", "tokens": "The highly under-specified nature of these expressions fits well with our << constraint-based representation of time >> , [[ Time Calculus for Natural Language -LRB- TCNL -RRB- ]] .", "h": ["Time Calculus for Natural Language -LRB- TCNL -RRB-"], "t": ["constraint-based representation of time"]}, {"label": "COMPARE", "tokens": "We have developed and evaluated a Temporal Expression Anchoror -LRB- TEA -RRB- , and the result shows that [[ it ]] performs significantly better than the << baseline >> , and compares favorably with some of the closely related work .", "h": ["it"], "t": ["baseline"]}, {"label": "USED-FOR", "tokens": "We address the problem of populating [[ object category detection datasets ]] with dense , << per-object 3D reconstructions >> , bootstrapped from class labels , ground truth figure-ground segmentations and a small set of keypoint annotations .", "h": ["object category detection datasets"], "t": ["per-object 3D reconstructions"]}, {"label": "USED-FOR", "tokens": "We address the problem of populating object category detection datasets with dense , << per-object 3D reconstructions >> , bootstrapped from class labels , [[ ground truth figure-ground segmentations ]] and a small set of keypoint annotations .", "h": ["ground truth figure-ground segmentations"], "t": ["per-object 3D reconstructions"]}, {"label": "CONJUNCTION", "tokens": "We address the problem of populating object category detection datasets with dense , per-object 3D reconstructions , bootstrapped from class labels , [[ ground truth figure-ground segmentations ]] and a small set of << keypoint annotations >> .", "h": ["ground truth figure-ground segmentations"], "t": ["keypoint annotations"]}, {"label": "USED-FOR", "tokens": "We address the problem of populating object category detection datasets with dense , << per-object 3D reconstructions >> , bootstrapped from class labels , ground truth figure-ground segmentations and a small set of [[ keypoint annotations ]] .", "h": ["keypoint annotations"], "t": ["per-object 3D reconstructions"]}, {"label": "USED-FOR", "tokens": "Our proposed [[ algorithm ]] first estimates << camera viewpoint >> using rigid structure-from-motion , then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions .", "h": ["algorithm"], "t": ["camera viewpoint"]}, {"label": "USED-FOR", "tokens": "Our proposed [[ algorithm ]] first estimates camera viewpoint using rigid structure-from-motion , then reconstructs << object shapes >> by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions .", "h": ["algorithm"], "t": ["object shapes"]}, {"label": "USED-FOR", "tokens": "Our proposed << algorithm >> first estimates camera viewpoint using [[ rigid structure-from-motion ]] , then reconstructs object shapes by optimizing over visual hull proposals guided by loose within-class shape similarity assumptions .", "h": ["rigid structure-from-motion"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "Our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion , then reconstructs << object shapes >> by optimizing over [[ visual hull proposals ]] guided by loose within-class shape similarity assumptions .", "h": ["visual hull proposals"], "t": ["object shapes"]}, {"label": "USED-FOR", "tokens": "Our proposed algorithm first estimates camera viewpoint using rigid structure-from-motion , then reconstructs object shapes by optimizing over << visual hull proposals >> guided by [[ loose within-class shape similarity assumptions ]] .", "h": ["loose within-class shape similarity assumptions"], "t": ["visual hull proposals"]}, {"label": "USED-FOR", "tokens": "We show that our [[ method ]] is able to produce convincing << per-object 3D reconstructions >> on one of the most challenging existing object-category detection datasets , PASCAL VOC .", "h": ["method"], "t": ["per-object 3D reconstructions"]}, {"label": "USED-FOR", "tokens": "We show that our << method >> is able to produce convincing per-object 3D reconstructions on one of the most challenging existing [[ object-category detection datasets ]] , PASCAL VOC .", "h": ["object-category detection datasets"], "t": ["method"]}, {"label": "HYPONYM-OF", "tokens": "We show that our method is able to produce convincing per-object 3D reconstructions on one of the most challenging existing << object-category detection datasets >> , [[ PASCAL VOC ]] .", "h": ["PASCAL VOC"], "t": ["object-category detection datasets"]}, {"label": "USED-FOR", "tokens": "[[ Probabilistic models ]] have been previously shown to be efficient and effective for << modeling and recognition of human motion >> .", "h": ["Probabilistic models"], "t": ["modeling and recognition of human motion"]}, {"label": "USED-FOR", "tokens": "In particular we focus on methods which represent the << human motion model >> as a [[ triangulated graph ]] .", "h": ["triangulated graph"], "t": ["human motion model"]}, {"label": "USED-FOR", "tokens": "Previous approaches learned << models >> based just on [[ positions ]] and velocities of the body parts while ignoring their appearance .", "h": ["positions"], "t": ["models"]}, {"label": "CONJUNCTION", "tokens": "Previous approaches learned models based just on [[ positions ]] and << velocities >> of the body parts while ignoring their appearance .", "h": ["positions"], "t": ["velocities"]}, {"label": "USED-FOR", "tokens": "Previous approaches learned << models >> based just on positions and [[ velocities ]] of the body parts while ignoring their appearance .", "h": ["velocities"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "Moreover , a [[ heuristic approach ]] was commonly used to obtain << translation invariance >> .", "h": ["heuristic approach"], "t": ["translation invariance"]}, {"label": "USED-FOR", "tokens": "In this paper we suggest an improved [[ approach ]] for learning such << models >> and using them for human motion recognition .", "h": ["approach"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "In this paper we suggest an improved approach for learning such models and using [[ them ]] for << human motion recognition >> .", "h": ["them"], "t": ["human motion recognition"]}, {"label": "USED-FOR", "tokens": "The suggested [[ approach ]] combines multiple cues , i.e. , positions , velocities and appearance into both the << learning and detection phases >> .", "h": ["approach"], "t": ["learning and detection phases"]}, {"label": "HYPONYM-OF", "tokens": "The suggested approach combines multiple << cues >> , i.e. , [[ positions ]] , velocities and appearance into both the learning and detection phases .", "h": ["positions"], "t": ["cues"]}, {"label": "CONJUNCTION", "tokens": "The suggested approach combines multiple cues , i.e. , [[ positions ]] , << velocities >> and appearance into both the learning and detection phases .", "h": ["positions"], "t": ["velocities"]}, {"label": "HYPONYM-OF", "tokens": "The suggested approach combines multiple << cues >> , i.e. , positions , [[ velocities ]] and appearance into both the learning and detection phases .", "h": ["velocities"], "t": ["cues"]}, {"label": "CONJUNCTION", "tokens": "The suggested approach combines multiple cues , i.e. , positions , [[ velocities ]] and << appearance >> into both the learning and detection phases .", "h": ["velocities"], "t": ["appearance"]}, {"label": "HYPONYM-OF", "tokens": "The suggested approach combines multiple << cues >> , i.e. , positions , velocities and [[ appearance ]] into both the learning and detection phases .", "h": ["appearance"], "t": ["cues"]}, {"label": "USED-FOR", "tokens": "Furthermore , we introduce [[ global variables ]] in the << model >> , which can represent global properties such as translation , scale or viewpoint .", "h": ["global variables"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "Furthermore , we introduce [[ global variables ]] in the model , which can represent << global properties >> such as translation , scale or viewpoint .", "h": ["global variables"], "t": ["global properties"]}, {"label": "HYPONYM-OF", "tokens": "Furthermore , we introduce global variables in the model , which can represent << global properties >> such as [[ translation ]] , scale or viewpoint .", "h": ["translation"], "t": ["global properties"]}, {"label": "CONJUNCTION", "tokens": "Furthermore , we introduce global variables in the model , which can represent global properties such as [[ translation ]] , << scale >> or viewpoint .", "h": ["translation"], "t": ["scale"]}, {"label": "HYPONYM-OF", "tokens": "Furthermore , we introduce global variables in the model , which can represent << global properties >> such as translation , [[ scale ]] or viewpoint .", "h": ["scale"], "t": ["global properties"]}, {"label": "CONJUNCTION", "tokens": "Furthermore , we introduce global variables in the model , which can represent global properties such as translation , [[ scale ]] or << viewpoint >> .", "h": ["scale"], "t": ["viewpoint"]}, {"label": "HYPONYM-OF", "tokens": "Furthermore , we introduce global variables in the model , which can represent << global properties >> such as translation , scale or [[ viewpoint ]] .", "h": ["viewpoint"], "t": ["global properties"]}, {"label": "USED-FOR", "tokens": "The << model >> is learned in an [[ unsupervised manner ]] from un-labelled data .", "h": ["unsupervised manner"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "The model is learned in an << unsupervised manner >> from [[ un-labelled data ]] .", "h": ["un-labelled data"], "t": ["unsupervised manner"]}, {"label": "USED-FOR", "tokens": "We show that the suggested << hybrid proba-bilistic model >> -LRB- which combines [[ global variables ]] , like translation , with local variables , like relative positions and appearances of body parts -RRB- , leads to : -LRB- i -RRB- faster convergence of learning phase , -LRB- ii -RRB- robustness to occlusions , and , -LRB- iii -RRB- higher recognition rate .", "h": ["global variables"], "t": ["hybrid proba-bilistic model"]}, {"label": "HYPONYM-OF", "tokens": "We show that the suggested hybrid proba-bilistic model -LRB- which combines << global variables >> , like [[ translation ]] , with local variables , like relative positions and appearances of body parts -RRB- , leads to : -LRB- i -RRB- faster convergence of learning phase , -LRB- ii -RRB- robustness to occlusions , and , -LRB- iii -RRB- higher recognition rate .", "h": ["translation"], "t": ["global variables"]}, {"label": "HYPONYM-OF", "tokens": "We show that the suggested hybrid proba-bilistic model -LRB- which combines global variables , like translation , with << local variables >> , like [[ relative positions ]] and appearances of body parts -RRB- , leads to : -LRB- i -RRB- faster convergence of learning phase , -LRB- ii -RRB- robustness to occlusions , and , -LRB- iii -RRB- higher recognition rate .", "h": ["relative positions"], "t": ["local variables"]}, {"label": "CONJUNCTION", "tokens": "We show that the suggested hybrid proba-bilistic model -LRB- which combines global variables , like translation , with local variables , like [[ relative positions ]] and << appearances of body parts >> -RRB- , leads to : -LRB- i -RRB- faster convergence of learning phase , -LRB- ii -RRB- robustness to occlusions , and , -LRB- iii -RRB- higher recognition rate .", "h": ["relative positions"], "t": ["appearances of body parts"]}, {"label": "HYPONYM-OF", "tokens": "We show that the suggested hybrid proba-bilistic model -LRB- which combines global variables , like translation , with << local variables >> , like relative positions and [[ appearances of body parts ]] -RRB- , leads to : -LRB- i -RRB- faster convergence of learning phase , -LRB- ii -RRB- robustness to occlusions , and , -LRB- iii -RRB- higher recognition rate .", "h": ["appearances of body parts"], "t": ["local variables"]}, {"label": "FEATURE-OF", "tokens": "We show that the suggested hybrid proba-bilistic model -LRB- which combines global variables , like translation , with local variables , like relative positions and appearances of body parts -RRB- , leads to : -LRB- i -RRB- [[ faster convergence ]] of << learning phase >> , -LRB- ii -RRB- robustness to occlusions , and , -LRB- iii -RRB- higher recognition rate .", "h": ["faster convergence"], "t": ["learning phase"]}, {"label": "CONJUNCTION", "tokens": "We show that the suggested hybrid proba-bilistic model -LRB- which combines global variables , like translation , with local variables , like relative positions and appearances of body parts -RRB- , leads to : -LRB- i -RRB- [[ faster convergence ]] of learning phase , -LRB- ii -RRB- << robustness >> to occlusions , and , -LRB- iii -RRB- higher recognition rate .", "h": ["faster convergence"], "t": ["robustness"]}, {"label": "CONJUNCTION", "tokens": "We show that the suggested hybrid proba-bilistic model -LRB- which combines global variables , like translation , with local variables , like relative positions and appearances of body parts -RRB- , leads to : -LRB- i -RRB- faster convergence of learning phase , -LRB- ii -RRB- [[ robustness ]] to occlusions , and , -LRB- iii -RRB- higher << recognition rate >> .", "h": ["robustness"], "t": ["recognition rate"]}, {"label": "CONJUNCTION", "tokens": "[[ Factor analysis ]] and << principal components analysis >> can be used to model linear relationships between observed variables and linearly map high-dimensional data to a lower-dimensional hidden space .", "h": ["Factor analysis"], "t": ["principal components analysis"]}, {"label": "USED-FOR", "tokens": "[[ Factor analysis ]] and principal components analysis can be used to model << linear relationships between observed variables >> and linearly map high-dimensional data to a lower-dimensional hidden space .", "h": ["Factor analysis"], "t": ["linear relationships between observed variables"]}, {"label": "USED-FOR", "tokens": "Factor analysis and [[ principal components analysis ]] can be used to model << linear relationships between observed variables >> and linearly map high-dimensional data to a lower-dimensional hidden space .", "h": ["principal components analysis"], "t": ["linear relationships between observed variables"]}, {"label": "USED-FOR", "tokens": "We describe a [[ nonlinear generalization of factor analysis ]] , called `` product analy-sis '' , that models the << observed variables >> as a linear combination of products of normally distributed hidden variables .", "h": ["nonlinear generalization of factor analysis"], "t": ["observed variables"]}, {"label": "HYPONYM-OF", "tokens": "We describe a << nonlinear generalization of factor analysis >> , called [[ `` product analy-sis '' ]] , that models the observed variables as a linear combination of products of normally distributed hidden variables .", "h": ["`` product analy-sis ''"], "t": ["nonlinear generalization of factor analysis"]}, {"label": "USED-FOR", "tokens": "We describe a << nonlinear generalization of factor analysis >> , called `` product analy-sis '' , that models the observed variables as a [[ linear combination of products of normally distributed hidden variables ]] .", "h": ["linear combination of products of normally distributed hidden variables"], "t": ["nonlinear generalization of factor analysis"]}, {"label": "USED-FOR", "tokens": "Just as << factor analysis >> can be viewed as [[ unsupervised linear regression ]] on unobserved , normally distributed hidden variables , product analysis can be viewed as unsupervised linear regression on products of unobserved , normally distributed hidden variables .", "h": ["unsupervised linear regression"], "t": ["factor analysis"]}, {"label": "USED-FOR", "tokens": "Just as factor analysis can be viewed as unsupervised linear regression on unobserved , normally distributed hidden variables , << product analysis >> can be viewed as [[ unsupervised linear regression ]] on products of unobserved , normally distributed hidden variables .", "h": ["unsupervised linear regression"], "t": ["product analysis"]}, {"label": "USED-FOR", "tokens": "The mapping between the data and the hidden space is nonlinear , so we use an [[ approximate variational technique ]] for << inference >> and learning .", "h": ["approximate variational technique"], "t": ["inference"]}, {"label": "USED-FOR", "tokens": "The mapping between the data and the hidden space is nonlinear , so we use an [[ approximate variational technique ]] for inference and << learning >> .", "h": ["approximate variational technique"], "t": ["learning"]}, {"label": "CONJUNCTION", "tokens": "The mapping between the data and the hidden space is nonlinear , so we use an approximate variational technique for [[ inference ]] and << learning >> .", "h": ["inference"], "t": ["learning"]}, {"label": "HYPONYM-OF", "tokens": "Since [[ product analysis ]] is a << generalization of factor analysis >> , product analysis always finds a higher data likelihood than factor analysis .", "h": ["product analysis"], "t": ["generalization of factor analysis"]}, {"label": "COMPARE", "tokens": "Since product analysis is a generalization of factor analysis , [[ product analysis ]] always finds a higher data likelihood than << factor analysis >> .", "h": ["product analysis"], "t": ["factor analysis"]}, {"label": "CONJUNCTION", "tokens": "We give results on [[ pattern recognition ]] and << illumination-invariant image clustering >> .", "h": ["pattern recognition"], "t": ["illumination-invariant image clustering"]}, {"label": "USED-FOR", "tokens": "This paper describes a [[ domain independent strategy ]] for the << multimedia articulation of answers >> elicited by a natural language interface to database query applications .", "h": ["domain independent strategy"], "t": ["multimedia articulation of answers"]}, {"label": "USED-FOR", "tokens": "This paper describes a domain independent strategy for the [[ multimedia articulation of answers ]] elicited by a << natural language interface >> to database query applications .", "h": ["multimedia articulation of answers"], "t": ["natural language interface"]}, {"label": "USED-FOR", "tokens": "This paper describes a domain independent strategy for the multimedia articulation of answers elicited by a [[ natural language interface ]] to << database query applications >> .", "h": ["natural language interface"], "t": ["database query applications"]}, {"label": "PART-OF", "tokens": "<< Multimedia answers >> include [[ videodisc images ]] and heuristically-produced complete sentences in text or text-to-speech form .", "h": ["videodisc images"], "t": ["Multimedia answers"]}, {"label": "CONJUNCTION", "tokens": "[[ Deictic reference ]] and << feedback >> about the discourse are enabled .", "h": ["Deictic reference"], "t": ["feedback"]}, {"label": "FEATURE-OF", "tokens": "[[ Deictic reference ]] and feedback about the << discourse >> are enabled .", "h": ["Deictic reference"], "t": ["discourse"]}, {"label": "FEATURE-OF", "tokens": "Deictic reference and [[ feedback ]] about the << discourse >> are enabled .", "h": ["feedback"], "t": ["discourse"]}, {"label": "USED-FOR", "tokens": "The [[ LOGON MT demonstrator ]] assembles independently valuable << general-purpose NLP components >> into a machine translation pipeline that capitalizes on output quality .", "h": ["LOGON MT demonstrator"], "t": ["general-purpose NLP components"]}, {"label": "PART-OF", "tokens": "The LOGON MT demonstrator assembles independently valuable [[ general-purpose NLP components ]] into a << machine translation pipeline >> that capitalizes on output quality .", "h": ["general-purpose NLP components"], "t": ["machine translation pipeline"]}, {"label": "PART-OF", "tokens": "The << demonstrator >> embodies an interesting combination of [[ hand-built , symbolic resources ]] and stochastic processes .", "h": ["hand-built , symbolic resources"], "t": ["demonstrator"]}, {"label": "CONJUNCTION", "tokens": "The demonstrator embodies an interesting combination of [[ hand-built , symbolic resources ]] and << stochastic processes >> .", "h": ["hand-built , symbolic resources"], "t": ["stochastic processes"]}, {"label": "PART-OF", "tokens": "The << demonstrator >> embodies an interesting combination of hand-built , symbolic resources and [[ stochastic processes ]] .", "h": ["stochastic processes"], "t": ["demonstrator"]}, {"label": "CONJUNCTION", "tokens": "We describe both the [[ syntax ]] and << semantics >> of a general propositional language of context , and give a Hilbert style proof system for this language .", "h": ["syntax"], "t": ["semantics"]}, {"label": "FEATURE-OF", "tokens": "We describe both the [[ syntax ]] and semantics of a general << propositional language of context >> , and give a Hilbert style proof system for this language .", "h": ["syntax"], "t": ["propositional language of context"]}, {"label": "FEATURE-OF", "tokens": "We describe both the syntax and [[ semantics ]] of a general << propositional language of context >> , and give a Hilbert style proof system for this language .", "h": ["semantics"], "t": ["propositional language of context"]}, {"label": "USED-FOR", "tokens": "We describe both the syntax and semantics of a general propositional language of context , and give a [[ Hilbert style proof system ]] for this << language >> .", "h": ["Hilbert style proof system"], "t": ["language"]}, {"label": "USED-FOR", "tokens": "A << propositional logic of context >> extends [[ classical propositional logic ]] in two ways .", "h": ["classical propositional logic"], "t": ["propositional logic of context"]}, {"label": "HYPONYM-OF", "tokens": "[[ Image matching ]] is a fundamental problem in << Computer Vision >> .", "h": ["Image matching"], "t": ["Computer Vision"]}, {"label": "USED-FOR", "tokens": "In the context of << feature-based matching >> , [[ SIFT ]] and its variants have long excelled in a wide array of applications .", "h": ["SIFT"], "t": ["feature-based matching"]}, {"label": "FEATURE-OF", "tokens": "However , for ultra-wide baselines , as in the case of << aerial images >> captured under [[ large camera rotations ]] , the appearance variation goes beyond the reach of SIFT and RANSAC .", "h": ["large camera rotations"], "t": ["aerial images"]}, {"label": "CONJUNCTION", "tokens": "However , for ultra-wide baselines , as in the case of aerial images captured under large camera rotations , the appearance variation goes beyond the reach of [[ SIFT ]] and << RANSAC >> .", "h": ["SIFT"], "t": ["RANSAC"]}, {"label": "USED-FOR", "tokens": "In this paper we propose a data-driven , deep learning-based approach that sidesteps local correspondence by framing the << problem >> as a [[ classification task ]] .", "h": ["classification task"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "We train our << models >> on a [[ dataset of urban aerial imagery ]] consisting of ` same ' and ` different ' pairs , collected for this purpose , and characterize the problem via a human study with annotations from Amazon Mechanical Turk .", "h": ["dataset of urban aerial imagery"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "We train our models on a dataset of urban aerial imagery consisting of ` same ' and ` different ' pairs , collected for this purpose , and characterize the << problem >> via a [[ human study ]] with annotations from Amazon Mechanical Turk .", "h": ["human study"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "We train our models on a dataset of urban aerial imagery consisting of ` same ' and ` different ' pairs , collected for this purpose , and characterize the problem via a << human study >> with [[ annotations from Amazon Mechanical Turk ]] .", "h": ["annotations from Amazon Mechanical Turk"], "t": ["human study"]}, {"label": "COMPARE", "tokens": "We demonstrate that our [[ models ]] outperform the << state-of-the-art >> on ultra-wide baseline matching and approach human accuracy .", "h": ["models"], "t": ["state-of-the-art"]}, {"label": "COMPARE", "tokens": "We demonstrate that our [[ models ]] outperform the state-of-the-art on ultra-wide baseline matching and approach << human accuracy >> .", "h": ["models"], "t": ["human accuracy"]}, {"label": "EVALUATE-FOR", "tokens": "We demonstrate that our << models >> outperform the state-of-the-art on [[ ultra-wide baseline matching ]] and approach human accuracy .", "h": ["ultra-wide baseline matching"], "t": ["models"]}, {"label": "EVALUATE-FOR", "tokens": "We demonstrate that our models outperform the << state-of-the-art >> on [[ ultra-wide baseline matching ]] and approach human accuracy .", "h": ["ultra-wide baseline matching"], "t": ["state-of-the-art"]}, {"label": "USED-FOR", "tokens": "We argue that a more sophisticated and [[ fine-grained annotation ]] in the tree-bank would have very positve effects on << stochastic parsers >> trained on the tree-bank and on grammars induced from the treebank , and it would make the treebank more valuable as a source of data for theoretical linguistic investigations .", "h": ["fine-grained annotation"], "t": ["stochastic parsers"]}, {"label": "USED-FOR", "tokens": "We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on << stochastic parsers >> trained on the [[ tree-bank ]] and on grammars induced from the treebank , and it would make the treebank more valuable as a source of data for theoretical linguistic investigations .", "h": ["tree-bank"], "t": ["stochastic parsers"]}, {"label": "USED-FOR", "tokens": "We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on << grammars >> induced from the [[ treebank ]] , and it would make the treebank more valuable as a source of data for theoretical linguistic investigations .", "h": ["treebank"], "t": ["grammars"]}, {"label": "USED-FOR", "tokens": "We argue that a more sophisticated and fine-grained annotation in the tree-bank would have very positve effects on stochastic parsers trained on the tree-bank and on grammars induced from the treebank , and it would make the [[ treebank ]] more valuable as a source of data for << theoretical linguistic investigations >> .", "h": ["treebank"], "t": ["theoretical linguistic investigations"]}, {"label": "HYPONYM-OF", "tokens": "The information gained from corpus research and the analyses that are proposed are realized in the framework of [[ SILVA ]] , a << parsing and extraction tool >> for German text corpora .", "h": ["SILVA"], "t": ["parsing and extraction tool"]}, {"label": "USED-FOR", "tokens": "The information gained from corpus research and the analyses that are proposed are realized in the framework of << SILVA >> , a parsing and extraction tool for [[ German text corpora ]] .", "h": ["German text corpora"], "t": ["SILVA"]}, {"label": "USED-FOR", "tokens": "While [[ paraphrasing ]] is critical both for << interpretation and generation of natural language >> , current systems use manual or semi-automatic methods to collect paraphrases .", "h": ["paraphrasing"], "t": ["interpretation and generation of natural language"]}, {"label": "USED-FOR", "tokens": "While paraphrasing is critical both for interpretation and generation of natural language , current [[ systems ]] use manual or semi-automatic methods to collect << paraphrases >> .", "h": ["systems"], "t": ["paraphrases"]}, {"label": "USED-FOR", "tokens": "While paraphrasing is critical both for interpretation and generation of natural language , current << systems >> use [[ manual or semi-automatic methods ]] to collect paraphrases .", "h": ["manual or semi-automatic methods"], "t": ["systems"]}, {"label": "USED-FOR", "tokens": "We present an [[ unsupervised learning algorithm ]] for << identification of paraphrases >> from a corpus of multiple English translations of the same source text .", "h": ["unsupervised learning algorithm"], "t": ["identification of paraphrases"]}, {"label": "USED-FOR", "tokens": "We present an unsupervised learning algorithm for << identification of paraphrases >> from a [[ corpus of multiple English translations ]] of the same source text .", "h": ["corpus of multiple English translations"], "t": ["identification of paraphrases"]}, {"label": "USED-FOR", "tokens": "Our [[ approach ]] yields << phrasal and single word lexical paraphrases >> as well as syntactic paraphrases .", "h": ["approach"], "t": ["phrasal and single word lexical paraphrases"]}, {"label": "USED-FOR", "tokens": "Our [[ approach ]] yields phrasal and single word lexical paraphrases as well as << syntactic paraphrases >> .", "h": ["approach"], "t": ["syntactic paraphrases"]}, {"label": "CONJUNCTION", "tokens": "Our approach yields [[ phrasal and single word lexical paraphrases ]] as well as << syntactic paraphrases >> .", "h": ["phrasal and single word lexical paraphrases"], "t": ["syntactic paraphrases"]}, {"label": "USED-FOR", "tokens": "An efficient [[ bit-vector-based CKY-style parser ]] for << context-free parsing >> is presented .", "h": ["bit-vector-based CKY-style parser"], "t": ["context-free parsing"]}, {"label": "USED-FOR", "tokens": "The [[ parser ]] computes a compact << parse forest representation >> of the complete set of possible analyses for large treebank grammars and long input sentences .", "h": ["parser"], "t": ["parse forest representation"]}, {"label": "USED-FOR", "tokens": "The parser computes a compact [[ parse forest representation ]] of the complete set of possible analyses for << large treebank grammars >> and long input sentences .", "h": ["parse forest representation"], "t": ["large treebank grammars"]}, {"label": "USED-FOR", "tokens": "The << parser >> uses [[ bit-vector operations ]] to parallelise the basic parsing operations .", "h": ["bit-vector operations"], "t": ["parser"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a [[ partially-blurred-image classification and analysis framework ]] for << automatically detecting images >> containing blurred regions and recognizing the blur types for those regions without needing to perform blur kernel estimation and image deblurring .", "h": ["partially-blurred-image classification and analysis framework"], "t": ["automatically detecting images"]}, {"label": "PART-OF", "tokens": "In this paper , we propose a partially-blurred-image classification and analysis framework for automatically detecting << images >> containing [[ blurred regions ]] and recognizing the blur types for those regions without needing to perform blur kernel estimation and image deblurring .", "h": ["blurred regions"], "t": ["images"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we propose a partially-blurred-image classification and analysis framework for automatically detecting images containing blurred regions and recognizing the blur types for those regions without needing to perform [[ blur kernel estimation ]] and << image deblurring >> .", "h": ["blur kernel estimation"], "t": ["image deblurring"]}, {"label": "USED-FOR", "tokens": "We develop several << blur features >> modeled by [[ image color ]] , gradient , and spectrum information , and use feature parameter training to robustly classify blurred images .", "h": ["image color"], "t": ["blur features"]}, {"label": "CONJUNCTION", "tokens": "We develop several blur features modeled by [[ image color ]] , << gradient >> , and spectrum information , and use feature parameter training to robustly classify blurred images .", "h": ["image color"], "t": ["gradient"]}, {"label": "USED-FOR", "tokens": "We develop several << blur features >> modeled by image color , [[ gradient ]] , and spectrum information , and use feature parameter training to robustly classify blurred images .", "h": ["gradient"], "t": ["blur features"]}, {"label": "CONJUNCTION", "tokens": "We develop several blur features modeled by image color , [[ gradient ]] , and << spectrum information >> , and use feature parameter training to robustly classify blurred images .", "h": ["gradient"], "t": ["spectrum information"]}, {"label": "USED-FOR", "tokens": "We develop several << blur features >> modeled by image color , gradient , and [[ spectrum information ]] , and use feature parameter training to robustly classify blurred images .", "h": ["spectrum information"], "t": ["blur features"]}, {"label": "USED-FOR", "tokens": "We develop several blur features modeled by image color , gradient , and spectrum information , and use [[ feature parameter training ]] to robustly classify << blurred images >> .", "h": ["feature parameter training"], "t": ["blurred images"]}, {"label": "USED-FOR", "tokens": "Our << blur detection >> is based on [[ image patches ]] , making region-wise training and classification in one image efficient .", "h": ["image patches"], "t": ["blur detection"]}, {"label": "USED-FOR", "tokens": "Our << blur detection >> is based on image patches , making [[ region-wise training and classification ]] in one image efficient .", "h": ["region-wise training and classification"], "t": ["blur detection"]}, {"label": "USED-FOR", "tokens": "Extensive experiments show that our [[ method ]] works satisfactorily on challenging image data , which establishes a technical foundation for solving several << computer vision problems >> , such as motion analysis and image restoration , using the blur information .", "h": ["method"], "t": ["computer vision problems"]}, {"label": "EVALUATE-FOR", "tokens": "Extensive experiments show that our << method >> works satisfactorily on challenging [[ image data ]] , which establishes a technical foundation for solving several computer vision problems , such as motion analysis and image restoration , using the blur information .", "h": ["image data"], "t": ["method"]}, {"label": "HYPONYM-OF", "tokens": "Extensive experiments show that our method works satisfactorily on challenging image data , which establishes a technical foundation for solving several << computer vision problems >> , such as [[ motion analysis ]] and image restoration , using the blur information .", "h": ["motion analysis"], "t": ["computer vision problems"]}, {"label": "CONJUNCTION", "tokens": "Extensive experiments show that our method works satisfactorily on challenging image data , which establishes a technical foundation for solving several computer vision problems , such as [[ motion analysis ]] and << image restoration >> , using the blur information .", "h": ["motion analysis"], "t": ["image restoration"]}, {"label": "HYPONYM-OF", "tokens": "Extensive experiments show that our method works satisfactorily on challenging image data , which establishes a technical foundation for solving several << computer vision problems >> , such as motion analysis and [[ image restoration ]] , using the blur information .", "h": ["image restoration"], "t": ["computer vision problems"]}, {"label": "USED-FOR", "tokens": "Extensive experiments show that our << method >> works satisfactorily on challenging image data , which establishes a technical foundation for solving several computer vision problems , such as motion analysis and image restoration , using the [[ blur information ]] .", "h": ["blur information"], "t": ["method"]}, {"label": "HYPONYM-OF", "tokens": "We have recently reported on two new << word-sense disambiguation systems >> , [[ one ]] trained on bilingual material -LRB- the Canadian Hansards -RRB- and the other trained on monolingual material -LRB- Roget 's Thesaurus and Grolier 's Encyclopedia -RRB- .", "h": ["one"], "t": ["word-sense disambiguation systems"]}, {"label": "CONJUNCTION", "tokens": "We have recently reported on two new word-sense disambiguation systems , [[ one ]] trained on bilingual material -LRB- the Canadian Hansards -RRB- and the << other >> trained on monolingual material -LRB- Roget 's Thesaurus and Grolier 's Encyclopedia -RRB- .", "h": ["one"], "t": ["other"]}, {"label": "EVALUATE-FOR", "tokens": "We have recently reported on two new word-sense disambiguation systems , << one >> trained on [[ bilingual material ]] -LRB- the Canadian Hansards -RRB- and the other trained on monolingual material -LRB- Roget 's Thesaurus and Grolier 's Encyclopedia -RRB- .", "h": ["bilingual material"], "t": ["one"]}, {"label": "HYPONYM-OF", "tokens": "We have recently reported on two new << word-sense disambiguation systems >> , one trained on bilingual material -LRB- the Canadian Hansards -RRB- and the [[ other ]] trained on monolingual material -LRB- Roget 's Thesaurus and Grolier 's Encyclopedia -RRB- .", "h": ["other"], "t": ["word-sense disambiguation systems"]}, {"label": "USED-FOR", "tokens": "We have recently reported on two new word-sense disambiguation systems , one trained on bilingual material -LRB- the Canadian Hansards -RRB- and the << other >> trained on [[ monolingual material ]] -LRB- Roget 's Thesaurus and Grolier 's Encyclopedia -RRB- .", "h": ["monolingual material"], "t": ["other"]}, {"label": "HYPONYM-OF", "tokens": "We have recently reported on two new word-sense disambiguation systems , one trained on bilingual material -LRB- the Canadian Hansards -RRB- and the other trained on << monolingual material >> -LRB- [[ Roget 's Thesaurus ]] and Grolier 's Encyclopedia -RRB- .", "h": ["Roget 's Thesaurus"], "t": ["monolingual material"]}, {"label": "CONJUNCTION", "tokens": "We have recently reported on two new word-sense disambiguation systems , one trained on bilingual material -LRB- the Canadian Hansards -RRB- and the other trained on monolingual material -LRB- [[ Roget 's Thesaurus ]] and << Grolier 's Encyclopedia >> -RRB- .", "h": ["Roget 's Thesaurus"], "t": ["Grolier 's Encyclopedia"]}, {"label": "HYPONYM-OF", "tokens": "We have recently reported on two new word-sense disambiguation systems , one trained on bilingual material -LRB- the Canadian Hansards -RRB- and the other trained on << monolingual material >> -LRB- Roget 's Thesaurus and [[ Grolier 's Encyclopedia ]] -RRB- .", "h": ["Grolier 's Encyclopedia"], "t": ["monolingual material"]}, {"label": "EVALUATE-FOR", "tokens": "In addition , [[ it ]] could also be used to help evaluate << disambiguation algorithms >> that did not make use of the discourse constraint .", "h": ["it"], "t": ["disambiguation algorithms"]}, {"label": "USED-FOR", "tokens": "We study and compare two novel [[ embedding methods ]] for << segmenting feature points of piece-wise planar structures >> from two -LRB- uncalibrated -RRB- perspective images .", "h": ["embedding methods"], "t": ["segmenting feature points of piece-wise planar structures"]}, {"label": "FEATURE-OF", "tokens": "We show that a set of different << homographies >> can be embedded in different ways to a [[ higher-dimensional real or complex space ]] , so that each homography corresponds to either a complex bilinear form or a real quadratic form .", "h": ["higher-dimensional real or complex space"], "t": ["homographies"]}, {"label": "FEATURE-OF", "tokens": "We show that a set of different homographies can be embedded in different ways to a higher-dimensional real or complex space , so that each << homography >> corresponds to either a [[ complex bilinear form ]] or a real quadratic form .", "h": ["complex bilinear form"], "t": ["homography"]}, {"label": "CONJUNCTION", "tokens": "We show that a set of different homographies can be embedded in different ways to a higher-dimensional real or complex space , so that each homography corresponds to either a [[ complex bilinear form ]] or a << real quadratic form >> .", "h": ["complex bilinear form"], "t": ["real quadratic form"]}, {"label": "FEATURE-OF", "tokens": "We show that a set of different homographies can be embedded in different ways to a higher-dimensional real or complex space , so that each << homography >> corresponds to either a complex bilinear form or a [[ real quadratic form ]] .", "h": ["real quadratic form"], "t": ["homography"]}, {"label": "USED-FOR", "tokens": "We give a << closed-form segmentation solution >> for each case by utilizing these properties based on [[ subspace-segmentation methods ]] .", "h": ["subspace-segmentation methods"], "t": ["closed-form segmentation solution"]}, {"label": "FEATURE-OF", "tokens": "These theoretical results show that one can intrinsically segment a << piece-wise planar scene >> from [[ 2-D images ]] without explicitly performing any 3-D reconstruction .", "h": ["2-D images"], "t": ["piece-wise planar scene"]}, {"label": "PART-OF", "tokens": "[[ Background maintenance ]] is a frequent element of << video surveillance systems >> .", "h": ["Background maintenance"], "t": ["video surveillance systems"]}, {"label": "USED-FOR", "tokens": "We develop Wallflower , a [[ three-component system ]] for << background maintenance >> : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .", "h": ["three-component system"], "t": ["background maintenance"]}, {"label": "PART-OF", "tokens": "We develop Wallflower , a << three-component system >> for background maintenance : the [[ pixel-level component ]] performs Wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .", "h": ["pixel-level component"], "t": ["three-component system"]}, {"label": "CONJUNCTION", "tokens": "We develop Wallflower , a three-component system for background maintenance : the [[ pixel-level component ]] performs Wiener filtering to make probabilistic predictions of the expected background ; the << region-level component >> fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .", "h": ["pixel-level component"], "t": ["region-level component"]}, {"label": "USED-FOR", "tokens": "We develop Wallflower , a three-component system for background maintenance : the << pixel-level component >> performs [[ Wiener filtering ]] to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .", "h": ["Wiener filtering"], "t": ["pixel-level component"]}, {"label": "USED-FOR", "tokens": "We develop Wallflower , a three-component system for background maintenance : the pixel-level component performs [[ Wiener filtering ]] to make << probabilistic predictions of the expected background >> ; the region-level component fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .", "h": ["Wiener filtering"], "t": ["probabilistic predictions of the expected background"]}, {"label": "PART-OF", "tokens": "We develop Wallflower , a << three-component system >> for background maintenance : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the [[ region-level component ]] fills in homogeneous regions of foreground objects ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .", "h": ["region-level component"], "t": ["three-component system"]}, {"label": "USED-FOR", "tokens": "We develop Wallflower , a three-component system for background maintenance : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the [[ region-level component ]] fills in << homogeneous regions of foreground objects >> ; and the frame-level component detects sudden , global changes in the image and swaps in better approximations of the background .", "h": ["region-level component"], "t": ["homogeneous regions of foreground objects"]}, {"label": "CONJUNCTION", "tokens": "We develop Wallflower , a three-component system for background maintenance : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the [[ region-level component ]] fills in homogeneous regions of foreground objects ; and the << frame-level component >> detects sudden , global changes in the image and swaps in better approximations of the background .", "h": ["region-level component"], "t": ["frame-level component"]}, {"label": "PART-OF", "tokens": "We develop Wallflower , a << three-component system >> for background maintenance : the pixel-level component performs Wiener filtering to make probabilistic predictions of the expected background ; the region-level component fills in homogeneous regions of foreground objects ; and the [[ frame-level component ]] detects sudden , global changes in the image and swaps in better approximations of the background .", "h": ["frame-level component"], "t": ["three-component system"]}, {"label": "COMPARE", "tokens": "We compare our [[ system ]] with 8 other << background subtraction algorithms >> .", "h": ["system"], "t": ["background subtraction algorithms"]}, {"label": "COMPARE", "tokens": "[[ Wallflower ]] is shown to outperform previous << algorithms >> by handling a greater set of the difficult situations that can occur .", "h": ["Wallflower"], "t": ["algorithms"]}, {"label": "USED-FOR", "tokens": "Finally , we analyze the experimental results and propose [[ normative principles ]] for << background maintenance >> .", "h": ["normative principles"], "t": ["background maintenance"]}, {"label": "USED-FOR", "tokens": "Is it possible to use [[ out-of-domain acoustic training data ]] to improve a << speech recognizer >> 's performance on a speciic , independent application ?", "h": ["out-of-domain acoustic training data"], "t": ["speech recognizer"]}, {"label": "USED-FOR", "tokens": "In our experiments , we use [[ Wallstreet Journal -LRB- WSJ -RRB- data ]] to train a << recognizer >> , which is adapted and evaluated in the Phonebook domain .", "h": ["Wallstreet Journal -LRB- WSJ -RRB- data"], "t": ["recognizer"]}, {"label": "EVALUATE-FOR", "tokens": "In our experiments , we use Wallstreet Journal -LRB- WSJ -RRB- data to train a << recognizer >> , which is adapted and evaluated in the [[ Phonebook domain ]] .", "h": ["Phonebook domain"], "t": ["recognizer"]}, {"label": "USED-FOR", "tokens": "First , starting from the [[ WSJ-trained recognizer ]] , how much adaptation data -LRB- taken from the Phonebook training corpus -RRB- is necessary to achieve a reasonable << recognition >> performance in spite of the high degree of mismatch ?", "h": ["WSJ-trained recognizer"], "t": ["recognition"]}, {"label": "USED-FOR", "tokens": "First , starting from the << WSJ-trained recognizer >> , how much [[ adaptation data ]] -LRB- taken from the Phonebook training corpus -RRB- is necessary to achieve a reasonable recognition performance in spite of the high degree of mismatch ?", "h": ["adaptation data"], "t": ["WSJ-trained recognizer"]}, {"label": "PART-OF", "tokens": "First , starting from the WSJ-trained recognizer , how much [[ adaptation data ]] -LRB- taken from the << Phonebook training corpus >> -RRB- is necessary to achieve a reasonable recognition performance in spite of the high degree of mismatch ?", "h": ["adaptation data"], "t": ["Phonebook training corpus"]}, {"label": "USED-FOR", "tokens": "Second , is it possible to improve the << recognition >> performance of a [[ Phonebook-trained baseline acoustic model ]] by using additional out-of-domain training data ?", "h": ["Phonebook-trained baseline acoustic model"], "t": ["recognition"]}, {"label": "USED-FOR", "tokens": "Second , is it possible to improve the recognition performance of a << Phonebook-trained baseline acoustic model >> by using additional [[ out-of-domain training data ]] ?", "h": ["out-of-domain training data"], "t": ["Phonebook-trained baseline acoustic model"]}, {"label": "USED-FOR", "tokens": "This paper proposes an [[ approach ]] to << full parsing >> suitable for Information Extraction from texts .", "h": ["approach"], "t": ["full parsing"]}, {"label": "USED-FOR", "tokens": "This paper proposes an approach to [[ full parsing ]] suitable for << Information Extraction >> from texts .", "h": ["full parsing"], "t": ["Information Extraction"]}, {"label": "USED-FOR", "tokens": "[[ It ]] was implemented in the << IE module >> of FACILE , a EU project for multilingual text classification and IE .", "h": ["It"], "t": ["IE module"]}, {"label": "PART-OF", "tokens": "It was implemented in the [[ IE module ]] of << FACILE , a EU project for multilingual text classification and IE >> .", "h": ["IE module"], "t": ["FACILE , a EU project for multilingual text classification and IE"]}, {"label": "USED-FOR", "tokens": "It then presents an implemented << graphic interpretation system >> that takes into account a variety of [[ communicative signals ]] , and an evaluation study showing that evidence obtained from shallow processing of the graphic 's caption has a significant impact on the system 's success .", "h": ["communicative signals"], "t": ["graphic interpretation system"]}, {"label": "USED-FOR", "tokens": "It then presents an implemented graphic interpretation system that takes into account a variety of communicative signals , and an evaluation study showing that evidence obtained from [[ shallow processing ]] of the graphic 's caption has a significant impact on the << system >> 's success .", "h": ["shallow processing"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "It then presents an implemented graphic interpretation system that takes into account a variety of communicative signals , and an evaluation study showing that evidence obtained from << shallow processing >> of the [[ graphic 's caption ]] has a significant impact on the system 's success .", "h": ["graphic 's caption"], "t": ["shallow processing"]}, {"label": "USED-FOR", "tokens": "[[ Graphical models ]] such as Bayesian Networks -LRB- BNs -RRB- are being increasingly applied to various << computer vision problems >> .", "h": ["Graphical models"], "t": ["computer vision problems"]}, {"label": "HYPONYM-OF", "tokens": "<< Graphical models >> such as [[ Bayesian Networks -LRB- BNs -RRB- ]] are being increasingly applied to various computer vision problems .", "h": ["Bayesian Networks -LRB- BNs -RRB-"], "t": ["Graphical models"]}, {"label": "USED-FOR", "tokens": "One bottleneck in using BN is that learning the << BN model parameters >> often requires a large amount of reliable and [[ representative training data ]] , which proves to be difficult to acquire for many computer vision tasks .", "h": ["representative training data"], "t": ["BN model parameters"]}, {"label": "USED-FOR", "tokens": "One bottleneck in using BN is that learning the BN model parameters often requires a large amount of reliable and [[ representative training data ]] , which proves to be difficult to acquire for many << computer vision tasks >> .", "h": ["representative training data"], "t": ["computer vision tasks"]}, {"label": "FEATURE-OF", "tokens": "On the other hand , there is often available [[ qualitative prior knowledge ]] about the << model >> .", "h": ["qualitative prior knowledge"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "Such << knowledge >> comes either from [[ domain experts ]] based on their experience or from various physical or geometric constraints that govern the objects we try to model .", "h": ["domain experts"], "t": ["knowledge"]}, {"label": "CONJUNCTION", "tokens": "Such knowledge comes either from [[ domain experts ]] based on their experience or from various << physical or geometric constraints >> that govern the objects we try to model .", "h": ["domain experts"], "t": ["physical or geometric constraints"]}, {"label": "USED-FOR", "tokens": "Such << knowledge >> comes either from domain experts based on their experience or from various [[ physical or geometric constraints ]] that govern the objects we try to model .", "h": ["physical or geometric constraints"], "t": ["knowledge"]}, {"label": "COMPARE", "tokens": "Unlike the [[ quantitative prior ]] , the << qualitative prior >> is often ignored due to the difficulty of incorporating them into the model learning process .", "h": ["quantitative prior"], "t": ["qualitative prior"]}, {"label": "PART-OF", "tokens": "Unlike the quantitative prior , the qualitative prior is often ignored due to the difficulty of incorporating [[ them ]] into the << model learning process >> .", "h": ["them"], "t": ["model learning process"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we introduce a closed-form solution to systematically combine the [[ limited training data ]] with some generic << qualitative knowledge >> for BN parameter learning .", "h": ["limited training data"], "t": ["qualitative knowledge"]}, {"label": "USED-FOR", "tokens": "In this paper , we introduce a closed-form solution to systematically combine the [[ limited training data ]] with some generic qualitative knowledge for << BN parameter learning >> .", "h": ["limited training data"], "t": ["BN parameter learning"]}, {"label": "USED-FOR", "tokens": "In this paper , we introduce a closed-form solution to systematically combine the limited training data with some generic [[ qualitative knowledge ]] for << BN parameter learning >> .", "h": ["qualitative knowledge"], "t": ["BN parameter learning"]}, {"label": "USED-FOR", "tokens": "In this paper , we introduce a << closed-form solution >> to systematically combine the limited training data with some generic qualitative knowledge for [[ BN parameter learning ]] .", "h": ["BN parameter learning"], "t": ["closed-form solution"]}, {"label": "COMPARE", "tokens": "To validate our method , we compare [[ it ]] with the << Maximum Likelihood -LRB- ML -RRB- estimation method >> under sparse data and with the Expectation Maximization -LRB- EM -RRB- algorithm under incomplete data respectively .", "h": ["it"], "t": ["Maximum Likelihood -LRB- ML -RRB- estimation method"]}, {"label": "COMPARE", "tokens": "To validate our method , we compare [[ it ]] with the Maximum Likelihood -LRB- ML -RRB- estimation method under sparse data and with the << Expectation Maximization -LRB- EM -RRB- algorithm >> under incomplete data respectively .", "h": ["it"], "t": ["Expectation Maximization -LRB- EM -RRB- algorithm"]}, {"label": "USED-FOR", "tokens": "To validate our method , we compare << it >> with the Maximum Likelihood -LRB- ML -RRB- estimation method under [[ sparse data ]] and with the Expectation Maximization -LRB- EM -RRB- algorithm under incomplete data respectively .", "h": ["sparse data"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "To validate our method , we compare it with the << Maximum Likelihood -LRB- ML -RRB- estimation method >> under [[ sparse data ]] and with the Expectation Maximization -LRB- EM -RRB- algorithm under incomplete data respectively .", "h": ["sparse data"], "t": ["Maximum Likelihood -LRB- ML -RRB- estimation method"]}, {"label": "USED-FOR", "tokens": "To validate our method , we compare << it >> with the Maximum Likelihood -LRB- ML -RRB- estimation method under sparse data and with the Expectation Maximization -LRB- EM -RRB- algorithm under [[ incomplete data ]] respectively .", "h": ["incomplete data"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "To validate our method , we compare it with the Maximum Likelihood -LRB- ML -RRB- estimation method under sparse data and with the << Expectation Maximization -LRB- EM -RRB- algorithm >> under [[ incomplete data ]] respectively .", "h": ["incomplete data"], "t": ["Expectation Maximization -LRB- EM -RRB- algorithm"]}, {"label": "USED-FOR", "tokens": "To further demonstrate its applications for << computer vision >> , we apply [[ it ]] to learn a BN model for facial Action Unit -LRB- AU -RRB- recognition from real image data .", "h": ["it"], "t": ["computer vision"]}, {"label": "USED-FOR", "tokens": "To further demonstrate its applications for computer vision , we apply [[ it ]] to learn a << BN model >> for facial Action Unit -LRB- AU -RRB- recognition from real image data .", "h": ["it"], "t": ["BN model"]}, {"label": "USED-FOR", "tokens": "To further demonstrate its applications for computer vision , we apply it to learn a [[ BN model ]] for << facial Action Unit -LRB- AU -RRB- recognition >> from real image data .", "h": ["BN model"], "t": ["facial Action Unit -LRB- AU -RRB- recognition"]}, {"label": "USED-FOR", "tokens": "To further demonstrate its applications for computer vision , we apply it to learn a BN model for << facial Action Unit -LRB- AU -RRB- recognition >> from [[ real image data ]] .", "h": ["real image data"], "t": ["facial Action Unit -LRB- AU -RRB- recognition"]}, {"label": "CONJUNCTION", "tokens": "The experimental results show that with simple and [[ generic qualitative constraints ]] and using only a small amount of << training data >> , our method can robustly and accurately estimate the BN model parameters .", "h": ["generic qualitative constraints"], "t": ["training data"]}, {"label": "USED-FOR", "tokens": "The experimental results show that with simple and [[ generic qualitative constraints ]] and using only a small amount of training data , our << method >> can robustly and accurately estimate the BN model parameters .", "h": ["generic qualitative constraints"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The experimental results show that with simple and generic qualitative constraints and using only a small amount of [[ training data ]] , our << method >> can robustly and accurately estimate the BN model parameters .", "h": ["training data"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The experimental results show that with simple and generic qualitative constraints and using only a small amount of training data , our [[ method ]] can robustly and accurately estimate the << BN model parameters >> .", "h": ["method"], "t": ["BN model parameters"]}, {"label": "USED-FOR", "tokens": "In this paper we introduce a [[ modal language LT ]] for imposing << constraints on trees >> , and an extension LT -LRB- LF -RRB- for imposing constraints on trees decorated with feature structures .", "h": ["modal language LT"], "t": ["constraints on trees"]}, {"label": "USED-FOR", "tokens": "In this paper we introduce a modal language LT for imposing constraints on trees , and an [[ extension LT -LRB- LF -RRB- ]] for imposing << constraints on trees decorated with feature structures >> .", "h": ["extension LT -LRB- LF -RRB-"], "t": ["constraints on trees decorated with feature structures"]}, {"label": "USED-FOR", "tokens": "The motivation for introducing these [[ languages ]] is to provide tools for formalising << grammatical frameworks >> perspicuously , and the paper illustrates this by showing how the leading ideas of GPSG can be captured in LT -LRB- LF -RRB- .", "h": ["languages"], "t": ["grammatical frameworks"]}, {"label": "USED-FOR", "tokens": "The motivation for introducing these languages is to provide tools for formalising grammatical frameworks perspicuously , and the paper illustrates this by showing how the leading ideas of [[ GPSG ]] can be captured in << LT -LRB- LF -RRB- >> .", "h": ["GPSG"], "t": ["LT -LRB- LF -RRB-"]}, {"label": "USED-FOR", "tokens": "Previous research has demonstrated the utility of [[ clustering ]] in << inducing semantic verb classes >> from undisambiguated corpus data .", "h": ["clustering"], "t": ["inducing semantic verb classes"]}, {"label": "USED-FOR", "tokens": "Previous research has demonstrated the utility of << clustering >> in inducing semantic verb classes from [[ undisambiguated corpus data ]] .", "h": ["undisambiguated corpus data"], "t": ["clustering"]}, {"label": "PART-OF", "tokens": "We describe a new << approach >> which involves [[ clustering subcategorization frame -LRB- SCF -RRB- distributions ]] using the Information Bottleneck and nearest neighbour methods .", "h": ["clustering subcategorization frame -LRB- SCF -RRB- distributions"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "We describe a new approach which involves << clustering subcategorization frame -LRB- SCF -RRB- distributions >> using the [[ Information Bottleneck and nearest neighbour methods ]] .", "h": ["Information Bottleneck and nearest neighbour methods"], "t": ["clustering subcategorization frame -LRB- SCF -RRB- distributions"]}, {"label": "USED-FOR", "tokens": "A novel [[ evaluation scheme ]] is proposed which accounts for the effect of << polysemy >> on the clusters , offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data .", "h": ["evaluation scheme"], "t": ["polysemy"]}, {"label": "EVALUATE-FOR", "tokens": "A novel [[ evaluation scheme ]] is proposed which accounts for the effect of polysemy on the clusters , offering us a good insight into the potential and limitations of << semantically classifying undisambiguated SCF data >> .", "h": ["evaluation scheme"], "t": ["semantically classifying undisambiguated SCF data"]}, {"label": "FEATURE-OF", "tokens": "A novel evaluation scheme is proposed which accounts for the effect of [[ polysemy ]] on the << clusters >> , offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data .", "h": ["polysemy"], "t": ["clusters"]}, {"label": "USED-FOR", "tokens": "Due to the capacity of [[ pan-tilt-zoom -LRB- PTZ -RRB- cameras ]] to simultaneously cover a << panoramic area >> and maintain high resolution imagery , researches in automated surveillance systems with multiple PTZ cameras have become increasingly important .", "h": ["pan-tilt-zoom -LRB- PTZ -RRB- cameras"], "t": ["panoramic area"]}, {"label": "USED-FOR", "tokens": "Due to the capacity of [[ pan-tilt-zoom -LRB- PTZ -RRB- cameras ]] to simultaneously cover a panoramic area and maintain << high resolution imagery >> , researches in automated surveillance systems with multiple PTZ cameras have become increasingly important .", "h": ["pan-tilt-zoom -LRB- PTZ -RRB- cameras"], "t": ["high resolution imagery"]}, {"label": "FEATURE-OF", "tokens": "Due to the capacity of pan-tilt-zoom -LRB- PTZ -RRB- cameras to simultaneously cover a panoramic area and maintain high resolution imagery , researches in << automated surveillance systems >> with multiple [[ PTZ cameras ]] have become increasingly important .", "h": ["PTZ cameras"], "t": ["automated surveillance systems"]}, {"label": "USED-FOR", "tokens": "Most existing [[ algorithms ]] require the prior knowledge of intrinsic parameters of the PTZ camera to infer the << relative positioning >> and orientation among multiple PTZ cameras .", "h": ["algorithms"], "t": ["relative positioning"]}, {"label": "USED-FOR", "tokens": "Most existing [[ algorithms ]] require the prior knowledge of intrinsic parameters of the PTZ camera to infer the relative positioning and << orientation >> among multiple PTZ cameras .", "h": ["algorithms"], "t": ["orientation"]}, {"label": "USED-FOR", "tokens": "Most existing << algorithms >> require the [[ prior knowledge of intrinsic parameters of the PTZ camera ]] to infer the relative positioning and orientation among multiple PTZ cameras .", "h": ["prior knowledge of intrinsic parameters of the PTZ camera"], "t": ["algorithms"]}, {"label": "CONJUNCTION", "tokens": "Most existing algorithms require the prior knowledge of intrinsic parameters of the PTZ camera to infer the [[ relative positioning ]] and << orientation >> among multiple PTZ cameras .", "h": ["relative positioning"], "t": ["orientation"]}, {"label": "FEATURE-OF", "tokens": "Most existing algorithms require the prior knowledge of intrinsic parameters of the PTZ camera to infer the [[ relative positioning ]] and orientation among multiple << PTZ cameras >> .", "h": ["relative positioning"], "t": ["PTZ cameras"]}, {"label": "FEATURE-OF", "tokens": "Most existing algorithms require the prior knowledge of intrinsic parameters of the PTZ camera to infer the relative positioning and [[ orientation ]] among multiple << PTZ cameras >> .", "h": ["orientation"], "t": ["PTZ cameras"]}, {"label": "USED-FOR", "tokens": "To overcome this limitation , we propose a novel [[ mapping algorithm ]] that derives the << relative positioning >> and orientation between two PTZ cameras based on a unified polynomial model .", "h": ["mapping algorithm"], "t": ["relative positioning"]}, {"label": "USED-FOR", "tokens": "To overcome this limitation , we propose a novel [[ mapping algorithm ]] that derives the relative positioning and << orientation >> between two PTZ cameras based on a unified polynomial model .", "h": ["mapping algorithm"], "t": ["orientation"]}, {"label": "CONJUNCTION", "tokens": "To overcome this limitation , we propose a novel mapping algorithm that derives the [[ relative positioning ]] and << orientation >> between two PTZ cameras based on a unified polynomial model .", "h": ["relative positioning"], "t": ["orientation"]}, {"label": "FEATURE-OF", "tokens": "To overcome this limitation , we propose a novel mapping algorithm that derives the [[ relative positioning ]] and orientation between two << PTZ cameras >> based on a unified polynomial model .", "h": ["relative positioning"], "t": ["PTZ cameras"]}, {"label": "FEATURE-OF", "tokens": "To overcome this limitation , we propose a novel mapping algorithm that derives the relative positioning and [[ orientation ]] between two << PTZ cameras >> based on a unified polynomial model .", "h": ["orientation"], "t": ["PTZ cameras"]}, {"label": "USED-FOR", "tokens": "To overcome this limitation , we propose a novel << mapping algorithm >> that derives the relative positioning and orientation between two PTZ cameras based on a [[ unified polynomial model ]] .", "h": ["unified polynomial model"], "t": ["mapping algorithm"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results demonstrate that our proposed << algorithm >> presents substantially reduced [[ computational complexity ]] and improved flexibility at the cost of slightly decreased pixel accuracy , as compared with the work of Chen and Wang .", "h": ["computational complexity"], "t": ["algorithm"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results demonstrate that our proposed << algorithm >> presents substantially reduced computational complexity and improved [[ flexibility ]] at the cost of slightly decreased pixel accuracy , as compared with the work of Chen and Wang .", "h": ["flexibility"], "t": ["algorithm"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results demonstrate that our proposed << algorithm >> presents substantially reduced computational complexity and improved flexibility at the cost of slightly decreased [[ pixel accuracy ]] , as compared with the work of Chen and Wang .", "h": ["pixel accuracy"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "This slightly decreased << pixel accuracy >> can be compensated by [[ consistent labeling approaches ]] without added cost for the application of automated surveillance systems along with changing configurations and a larger number of PTZ cameras .", "h": ["consistent labeling approaches"], "t": ["pixel accuracy"]}, {"label": "USED-FOR", "tokens": "This paper presents a new [[ two-pass algorithm ]] for << Extra Large -LRB- more than 1M words -RRB- Vocabulary COntinuous Speech recognition >> based on the Information Retrieval -LRB- ELVIRCOS -RRB- .", "h": ["two-pass algorithm"], "t": ["Extra Large -LRB- more than 1M words -RRB- Vocabulary COntinuous Speech recognition"]}, {"label": "USED-FOR", "tokens": "This paper presents a new << two-pass algorithm >> for Extra Large -LRB- more than 1M words -RRB- Vocabulary COntinuous Speech recognition based on the [[ Information Retrieval -LRB- ELVIRCOS -RRB- ]] .", "h": ["Information Retrieval -LRB- ELVIRCOS -RRB-"], "t": ["two-pass algorithm"]}, {"label": "HYPONYM-OF", "tokens": "The principle of this approach is to decompose a recognition process into two << passes >> where the [[ first pass ]] builds the words subset for the second pass recognition by using information retrieval procedure .", "h": ["first pass"], "t": ["passes"]}, {"label": "HYPONYM-OF", "tokens": "The principle of this approach is to decompose a recognition process into two << passes >> where the first pass builds the words subset for the [[ second pass recognition ]] by using information retrieval procedure .", "h": ["second pass recognition"], "t": ["passes"]}, {"label": "USED-FOR", "tokens": "The principle of this approach is to decompose a recognition process into two passes where the first pass builds the words subset for the << second pass recognition >> by using [[ information retrieval procedure ]] .", "h": ["information retrieval procedure"], "t": ["second pass recognition"]}, {"label": "USED-FOR", "tokens": "[[ Word graph composition ]] for << continuous speech >> is presented .", "h": ["Word graph composition"], "t": ["continuous speech"]}, {"label": "USED-FOR", "tokens": "With this [[ approach ]] a high performances for << large vocabulary speech recognition >> can be obtained .", "h": ["approach"], "t": ["large vocabulary speech recognition"]}, {"label": "CONJUNCTION", "tokens": "First , images are partitioned into regions using << one-class classification >> and [[ patch-based clustering algorithms ]] where one-class classifiers model the regions with relatively uniform color and texture properties , and clustering of patches aims to detect structures in the remaining regions .", "h": ["patch-based clustering algorithms"], "t": ["one-class classification"]}, {"label": "USED-FOR", "tokens": "First , images are partitioned into regions using one-class classification and patch-based clustering algorithms where << one-class classifiers >> model the regions with relatively [[ uniform color and texture properties ]] , and clustering of patches aims to detect structures in the remaining regions .", "h": ["uniform color and texture properties"], "t": ["one-class classifiers"]}, {"label": "USED-FOR", "tokens": "Next , the resulting regions are clustered to obtain a codebook of region types , and two [[ models ]] are constructed for << scene representation >> : a '' bag of individual regions '' representation where each region is regarded separately , and a '' bag of region pairs '' representation where regions with particular spatial relationships are considered together .", "h": ["models"], "t": ["scene representation"]}, {"label": "USED-FOR", "tokens": "Given these representations , << scene classification >> is done using [[ Bayesian classifiers ]] .", "h": ["Bayesian classifiers"], "t": ["scene classification"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on the [[ LabelMe data set ]] showed that the proposed << models >> significantly out-perform a baseline global feature-based approach .", "h": ["LabelMe data set"], "t": ["models"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments on the [[ LabelMe data set ]] showed that the proposed models significantly out-perform a << baseline global feature-based approach >> .", "h": ["LabelMe data set"], "t": ["baseline global feature-based approach"]}, {"label": "COMPARE", "tokens": "Experiments on the LabelMe data set showed that the proposed [[ models ]] significantly out-perform a << baseline global feature-based approach >> .", "h": ["models"], "t": ["baseline global feature-based approach"]}, {"label": "USED-FOR", "tokens": "The [[ model ]] is designed for use in << error correction >> , with a focus on post-processing the output of black-box OCR systems in order to make it more useful for NLP tasks .", "h": ["model"], "t": ["error correction"]}, {"label": "USED-FOR", "tokens": "The [[ model ]] is designed for use in error correction , with a focus on << post-processing >> the output of black-box OCR systems in order to make it more useful for NLP tasks .", "h": ["model"], "t": ["post-processing"]}, {"label": "PART-OF", "tokens": "The model is designed for use in << error correction >> , with a focus on [[ post-processing ]] the output of black-box OCR systems in order to make it more useful for NLP tasks .", "h": ["post-processing"], "t": ["error correction"]}, {"label": "USED-FOR", "tokens": "The model is designed for use in error correction , with a focus on post-processing the output of black-box OCR systems in order to make [[ it ]] more useful for << NLP tasks >> .", "h": ["it"], "t": ["NLP tasks"]}, {"label": "USED-FOR", "tokens": "We present an implementation of the << model >> based on [[ finite-state models ]] , demonstrate the model 's ability to significantly reduce character and word error rate , and provide evaluation results involving automatic extraction of translation lexicons from printed text .", "h": ["finite-state models"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "We present an implementation of the model based on finite-state models , demonstrate the << model >> 's ability to significantly reduce [[ character and word error rate ]] , and provide evaluation results involving automatic extraction of translation lexicons from printed text .", "h": ["character and word error rate"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "We present an implementation of the model based on finite-state models , demonstrate the << model >> 's ability to significantly reduce character and word error rate , and provide evaluation results involving [[ automatic extraction of translation lexicons ]] from printed text .", "h": ["automatic extraction of translation lexicons"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "We present an implementation of the model based on finite-state models , demonstrate the model 's ability to significantly reduce character and word error rate , and provide evaluation results involving << automatic extraction of translation lexicons >> from [[ printed text ]] .", "h": ["printed text"], "t": ["automatic extraction of translation lexicons"]}, {"label": "USED-FOR", "tokens": "We present a [[ framework ]] for << word alignment >> based on log-linear models .", "h": ["framework"], "t": ["word alignment"]}, {"label": "USED-FOR", "tokens": "We present a << framework >> for word alignment based on [[ log-linear models ]] .", "h": ["log-linear models"], "t": ["framework"]}, {"label": "USED-FOR", "tokens": "All [[ knowledge sources ]] are treated as << feature functions >> , which depend on the source langauge sentence , the target language sentence and possible additional variables .", "h": ["knowledge sources"], "t": ["feature functions"]}, {"label": "USED-FOR", "tokens": "[[ Log-linear models ]] allow << statistical alignment models >> to be easily extended by incorporating syntactic information .", "h": ["Log-linear models"], "t": ["statistical alignment models"]}, {"label": "USED-FOR", "tokens": "<< Log-linear models >> allow statistical alignment models to be easily extended by incorporating [[ syntactic information ]] .", "h": ["syntactic information"], "t": ["Log-linear models"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we use [[ IBM Model 3 alignment probabilities ]] , << POS correspondence >> , and bilingual dictionary coverage as features .", "h": ["IBM Model 3 alignment probabilities"], "t": ["POS correspondence"]}, {"label": "USED-FOR", "tokens": "In this paper , we use [[ IBM Model 3 alignment probabilities ]] , POS correspondence , and bilingual dictionary coverage as << features >> .", "h": ["IBM Model 3 alignment probabilities"], "t": ["features"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we use IBM Model 3 alignment probabilities , [[ POS correspondence ]] , and << bilingual dictionary coverage >> as features .", "h": ["POS correspondence"], "t": ["bilingual dictionary coverage"]}, {"label": "USED-FOR", "tokens": "In this paper , we use IBM Model 3 alignment probabilities , [[ POS correspondence ]] , and bilingual dictionary coverage as << features >> .", "h": ["POS correspondence"], "t": ["features"]}, {"label": "USED-FOR", "tokens": "In this paper , we use IBM Model 3 alignment probabilities , POS correspondence , and [[ bilingual dictionary coverage ]] as << features >> .", "h": ["bilingual dictionary coverage"], "t": ["features"]}, {"label": "COMPARE", "tokens": "Our experiments show that [[ log-linear models ]] significantly outperform << IBM translation models >> .", "h": ["log-linear models"], "t": ["IBM translation models"]}, {"label": "USED-FOR", "tokens": "[[ Hough voting ]] in a geometric transformation space allows us to realize << spatial verification >> , but remains sensitive to feature detection errors because of the inflexible quan-tization of single feature correspondences .", "h": ["Hough voting"], "t": ["spatial verification"]}, {"label": "FEATURE-OF", "tokens": "<< Hough voting >> in a [[ geometric transformation space ]] allows us to realize spatial verification , but remains sensitive to feature detection errors because of the inflexible quan-tization of single feature correspondences .", "h": ["geometric transformation space"], "t": ["Hough voting"]}, {"label": "USED-FOR", "tokens": "To handle this problem , we propose a new [[ method ]] , called adaptive dither voting , for << robust spatial verification >> .", "h": ["method"], "t": ["robust spatial verification"]}, {"label": "USED-FOR", "tokens": "For each correspondence , instead of hard-mapping it to a single transformation , the << method >> augments its description by using [[ multiple dithered transformations ]] that are deterministically generated by the other correspondences .", "h": ["multiple dithered transformations"], "t": ["method"]}, {"label": "FEATURE-OF", "tokens": "We also propose exploiting the [[ non-uniformity ]] of a << Hough histogram >> as the spatial similarity to handle multiple matching surfaces .", "h": ["non-uniformity"], "t": ["Hough histogram"]}, {"label": "USED-FOR", "tokens": "We also propose exploiting the [[ non-uniformity ]] of a Hough histogram as the spatial similarity to handle << multiple matching surfaces >> .", "h": ["non-uniformity"], "t": ["multiple matching surfaces"]}, {"label": "USED-FOR", "tokens": "The [[ method ]] outperforms its state-of-the-art counterparts in both accuracy and scalability , especially when it comes to the << retrieval of small , rotated objects >> .", "h": ["method"], "t": ["retrieval of small , rotated objects"]}, {"label": "COMPARE", "tokens": "The << method >> outperforms its state-of-the-art [[ counterparts ]] in both accuracy and scalability , especially when it comes to the retrieval of small , rotated objects .", "h": ["counterparts"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "The << method >> outperforms its state-of-the-art counterparts in both [[ accuracy ]] and scalability , especially when it comes to the retrieval of small , rotated objects .", "h": ["accuracy"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "The method outperforms its state-of-the-art << counterparts >> in both [[ accuracy ]] and scalability , especially when it comes to the retrieval of small , rotated objects .", "h": ["accuracy"], "t": ["counterparts"]}, {"label": "EVALUATE-FOR", "tokens": "The << method >> outperforms its state-of-the-art counterparts in both accuracy and [[ scalability ]] , especially when it comes to the retrieval of small , rotated objects .", "h": ["scalability"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "The method outperforms its state-of-the-art << counterparts >> in both accuracy and [[ scalability ]] , especially when it comes to the retrieval of small , rotated objects .", "h": ["scalability"], "t": ["counterparts"]}, {"label": "USED-FOR", "tokens": "We propose a novel technique called [[ bispectral photo-metric stereo ]] that makes effective use of fluorescence for << shape reconstruction >> .", "h": ["bispectral photo-metric stereo"], "t": ["shape reconstruction"]}, {"label": "USED-FOR", "tokens": "We propose a novel technique called << bispectral photo-metric stereo >> that makes effective use of [[ fluorescence ]] for shape reconstruction .", "h": ["fluorescence"], "t": ["bispectral photo-metric stereo"]}, {"label": "EVALUATE-FOR", "tokens": "Due to the [[ complexity ]] of its << emission process >> , fluo-rescence tends to be excluded from most algorithms in computer vision and image processing .", "h": ["complexity"], "t": ["emission process"]}, {"label": "USED-FOR", "tokens": "Due to the complexity of its emission process , fluo-rescence tends to be excluded from most [[ algorithms ]] in << computer vision >> and image processing .", "h": ["algorithms"], "t": ["computer vision"]}, {"label": "USED-FOR", "tokens": "Due to the complexity of its emission process , fluo-rescence tends to be excluded from most [[ algorithms ]] in computer vision and << image processing >> .", "h": ["algorithms"], "t": ["image processing"]}, {"label": "CONJUNCTION", "tokens": "Due to the complexity of its emission process , fluo-rescence tends to be excluded from most algorithms in [[ computer vision ]] and << image processing >> .", "h": ["computer vision"], "t": ["image processing"]}, {"label": "USED-FOR", "tokens": "Moreover , [[ fluorescence 's wavelength-shifting property ]] enables us to estimate the << shape >> of an object by applying photomet-ric stereo to emission-only images without suffering from specular reflection .", "h": ["fluorescence 's wavelength-shifting property"], "t": ["shape"]}, {"label": "USED-FOR", "tokens": "Moreover , fluorescence 's wavelength-shifting property enables us to estimate the << shape >> of an object by applying [[ photomet-ric stereo ]] to emission-only images without suffering from specular reflection .", "h": ["photomet-ric stereo"], "t": ["shape"]}, {"label": "USED-FOR", "tokens": "Moreover , fluorescence 's wavelength-shifting property enables us to estimate the shape of an object by applying << photomet-ric stereo >> to [[ emission-only images ]] without suffering from specular reflection .", "h": ["emission-only images"], "t": ["photomet-ric stereo"]}, {"label": "COMPARE", "tokens": "This is the significant advantage of the << fluorescence-based method >> over previous [[ methods ]] based on reflection .", "h": ["methods"], "t": ["fluorescence-based method"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an [[ approach ]] for learning a << visual representation >> from the raw spatiotemporal signals in videos .", "h": ["approach"], "t": ["visual representation"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an approach for learning a << visual representation >> from the [[ raw spatiotemporal signals in videos ]] .", "h": ["raw spatiotemporal signals in videos"], "t": ["visual representation"]}, {"label": "USED-FOR", "tokens": "We formulate our << method >> as an [[ unsupervised sequential verification task ]] , i.e. , we determine whether a sequence of frames from a video is in the correct temporal order .", "h": ["unsupervised sequential verification task"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "With this simple [[ task ]] and no semantic labels , we learn a powerful << visual representation >> using a Convolutional Neural Network -LRB- CNN -RRB- .", "h": ["task"], "t": ["visual representation"]}, {"label": "USED-FOR", "tokens": "With this simple task and no semantic labels , we learn a powerful << visual representation >> using a [[ Convolutional Neural Network -LRB- CNN -RRB- ]] .", "h": ["Convolutional Neural Network -LRB- CNN -RRB-"], "t": ["visual representation"]}, {"label": "PART-OF", "tokens": "The << representation >> contains [[ complementary information ]] to that learned from supervised image datasets like ImageNet .", "h": ["complementary information"], "t": ["representation"]}, {"label": "USED-FOR", "tokens": "The representation contains << complementary information >> to that learned from [[ supervised image datasets ]] like ImageNet .", "h": ["supervised image datasets"], "t": ["complementary information"]}, {"label": "HYPONYM-OF", "tokens": "The representation contains complementary information to that learned from << supervised image datasets >> like [[ ImageNet ]] .", "h": ["ImageNet"], "t": ["supervised image datasets"]}, {"label": "USED-FOR", "tokens": "Qualitative results show that our [[ method ]] captures information that is temporally varying , such as << human pose >> .", "h": ["method"], "t": ["human pose"]}, {"label": "USED-FOR", "tokens": "When used as [[ pre-training ]] for << action recognition >> , our method gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51 .", "h": ["pre-training"], "t": ["action recognition"]}, {"label": "USED-FOR", "tokens": "When used as << pre-training >> for action recognition , [[ our method ]] gives significant gains over learning without external data on benchmark datasets like UCF101 and HMDB51 .", "h": ["our method"], "t": ["pre-training"]}, {"label": "COMPARE", "tokens": "When used as pre-training for action recognition , [[ our method ]] gives significant gains over << learning without external data >> on benchmark datasets like UCF101 and HMDB51 .", "h": ["our method"], "t": ["learning without external data"]}, {"label": "EVALUATE-FOR", "tokens": "When used as pre-training for action recognition , << our method >> gives significant gains over learning without external data on [[ benchmark datasets ]] like UCF101 and HMDB51 .", "h": ["benchmark datasets"], "t": ["our method"]}, {"label": "EVALUATE-FOR", "tokens": "When used as pre-training for action recognition , our method gives significant gains over << learning without external data >> on [[ benchmark datasets ]] like UCF101 and HMDB51 .", "h": ["benchmark datasets"], "t": ["learning without external data"]}, {"label": "HYPONYM-OF", "tokens": "When used as pre-training for action recognition , our method gives significant gains over learning without external data on << benchmark datasets >> like [[ UCF101 ]] and HMDB51 .", "h": ["UCF101"], "t": ["benchmark datasets"]}, {"label": "CONJUNCTION", "tokens": "When used as pre-training for action recognition , our method gives significant gains over learning without external data on benchmark datasets like [[ UCF101 ]] and << HMDB51 >> .", "h": ["UCF101"], "t": ["HMDB51"]}, {"label": "HYPONYM-OF", "tokens": "When used as pre-training for action recognition , our method gives significant gains over learning without external data on << benchmark datasets >> like UCF101 and [[ HMDB51 ]] .", "h": ["HMDB51"], "t": ["benchmark datasets"]}, {"label": "EVALUATE-FOR", "tokens": "To demonstrate its sensitivity to human pose , we show results for << pose estimation >> on the [[ FLIC and MPII datasets ]] that are competitive , or better than approaches using significantly more supervision .", "h": ["FLIC and MPII datasets"], "t": ["pose estimation"]}, {"label": "USED-FOR", "tokens": "To demonstrate its sensitivity to human pose , we show results for pose estimation on the FLIC and MPII datasets that are competitive , or better than << approaches >> using significantly more [[ supervision ]] .", "h": ["supervision"], "t": ["approaches"]}, {"label": "CONJUNCTION", "tokens": "<< Our method >> can be combined with [[ supervised representations ]] to provide an additional boost in accuracy .", "h": ["supervised representations"], "t": ["Our method"]}, {"label": "EVALUATE-FOR", "tokens": "<< Our method >> can be combined with supervised representations to provide an additional boost in [[ accuracy ]] .", "h": ["accuracy"], "t": ["Our method"]}, {"label": "USED-FOR", "tokens": "`` To explain complex phenomena , an [[ explanation system ]] must be able to select information from a formal representation of domain knowledge , organize the selected information into multisentential discourse plans , and realize the << discourse plans >> in text .", "h": ["explanation system"], "t": ["discourse plans"]}, {"label": "USED-FOR", "tokens": "This paper reports on a seven-year effort to empirically study << explanation generation >> from [[ semantically rich , large-scale knowledge bases ]] .", "h": ["semantically rich , large-scale knowledge bases"], "t": ["explanation generation"]}, {"label": "USED-FOR", "tokens": "In particular , it describes a [[ robust explanation system ]] that constructs << multisentential and multi-paragraph explanations >> from the a large-scale knowledge base in the domain of botanical anatomy , physiology , and development .", "h": ["robust explanation system"], "t": ["multisentential and multi-paragraph explanations"]}, {"label": "USED-FOR", "tokens": "In particular , it describes a << robust explanation system >> that constructs multisentential and multi-paragraph explanations from the a [[ large-scale knowledge base ]] in the domain of botanical anatomy , physiology , and development .", "h": ["large-scale knowledge base"], "t": ["robust explanation system"]}, {"label": "FEATURE-OF", "tokens": "In particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a << large-scale knowledge base >> in the domain of [[ botanical anatomy ]] , physiology , and development .", "h": ["botanical anatomy"], "t": ["large-scale knowledge base"]}, {"label": "CONJUNCTION", "tokens": "In particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a large-scale knowledge base in the domain of [[ botanical anatomy ]] , << physiology >> , and development .", "h": ["botanical anatomy"], "t": ["physiology"]}, {"label": "FEATURE-OF", "tokens": "In particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a << large-scale knowledge base >> in the domain of botanical anatomy , [[ physiology ]] , and development .", "h": ["physiology"], "t": ["large-scale knowledge base"]}, {"label": "CONJUNCTION", "tokens": "In particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a large-scale knowledge base in the domain of botanical anatomy , [[ physiology ]] , and << development >> .", "h": ["physiology"], "t": ["development"]}, {"label": "FEATURE-OF", "tokens": "In particular , it describes a robust explanation system that constructs multisentential and multi-paragraph explanations from the a << large-scale knowledge base >> in the domain of botanical anatomy , physiology , and [[ development ]] .", "h": ["development"], "t": ["large-scale knowledge base"]}, {"label": "EVALUATE-FOR", "tokens": "We introduce the evaluation methodology and describe how performance was assessed with this [[ methodology ]] in the most extensive empirical evaluation conducted on an << explanation system >> .", "h": ["methodology"], "t": ["explanation system"]}, {"label": "FEATURE-OF", "tokens": "We present an << operable definition >> of focus which is argued to be of a [[ cognito-pragmatic nature ]] and explore how it is determined in discourse in a formalized manner .", "h": ["cognito-pragmatic nature"], "t": ["operable definition"]}, {"label": "HYPONYM-OF", "tokens": "For this purpose , a file card model of discourse model and knowledge store is introduced enabling the decomposition and formal representation of its determination process as a << programmable algorithm >> -LRB- [[ FDA ]] -RRB- .", "h": ["FDA"], "t": ["programmable algorithm"]}, {"label": "USED-FOR", "tokens": "Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via [[ FDA ]] as a << discourse-level construct >> into speech synthesis systems , in particular , concept-to-speech systems , is also briefly discussed .", "h": ["FDA"], "t": ["discourse-level construct"]}, {"label": "PART-OF", "tokens": "Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via FDA as a [[ discourse-level construct ]] into << speech synthesis systems >> , in particular , concept-to-speech systems , is also briefly discussed .", "h": ["discourse-level construct"], "t": ["speech synthesis systems"]}, {"label": "HYPONYM-OF", "tokens": "Interdisciplinary evidence from social and cognitive psychology is cited and the prospect of the integration of focus via FDA as a discourse-level construct into << speech synthesis systems >> , in particular , [[ concept-to-speech systems ]] , is also briefly discussed .", "h": ["concept-to-speech systems"], "t": ["speech synthesis systems"]}, {"label": "USED-FOR", "tokens": "[[ Inference ]] in these << models >> involves solving a combinatorial optimization problem , with methods such as graph cuts , belief propagation .", "h": ["Inference"], "t": ["models"]}, {"label": "PART-OF", "tokens": "<< Inference >> in these models involves solving a [[ combinatorial optimization problem ]] , with methods such as graph cuts , belief propagation .", "h": ["combinatorial optimization problem"], "t": ["Inference"]}, {"label": "USED-FOR", "tokens": "Inference in these models involves solving a << combinatorial optimization problem >> , with [[ methods ]] such as graph cuts , belief propagation .", "h": ["methods"], "t": ["combinatorial optimization problem"]}, {"label": "USED-FOR", "tokens": "Inference in these models involves solving a combinatorial optimization problem , with << methods >> such as [[ graph cuts ]] , belief propagation .", "h": ["graph cuts"], "t": ["methods"]}, {"label": "CONJUNCTION", "tokens": "Inference in these models involves solving a combinatorial optimization problem , with methods such as [[ graph cuts ]] , << belief propagation >> .", "h": ["graph cuts"], "t": ["belief propagation"]}, {"label": "USED-FOR", "tokens": "Inference in these models involves solving a combinatorial optimization problem , with << methods >> such as graph cuts , [[ belief propagation ]] .", "h": ["belief propagation"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "To overcome this , state-of-the-art [[ structured learning methods ]] frame the << problem >> as one of large margin estimation .", "h": ["structured learning methods"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "To overcome this , state-of-the-art structured learning methods frame the << problem >> as one of [[ large margin estimation ]] .", "h": ["large margin estimation"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "[[ Iterative solutions ]] have been proposed to solve the resulting << convex optimization problem >> .", "h": ["Iterative solutions"], "t": ["convex optimization problem"]}, {"label": "USED-FOR", "tokens": "We show how the resulting << optimization problem >> can be reduced to an equivalent [[ convex problem ]] with a small number of constraints , and solve it using an efficient scheme .", "h": ["convex problem"], "t": ["optimization problem"]}, {"label": "HYPONYM-OF", "tokens": "[[ Interpreting metaphors ]] is an integral and inescapable process in << human understanding of natural language >> .", "h": ["Interpreting metaphors"], "t": ["human understanding of natural language"]}, {"label": "USED-FOR", "tokens": "This paper discusses a [[ method ]] of << analyzing metaphors >> based on the existence of a small number of generalized metaphor mappings .", "h": ["method"], "t": ["analyzing metaphors"]}, {"label": "USED-FOR", "tokens": "This paper discusses a method of << analyzing metaphors >> based on the existence of a small number of [[ generalized metaphor mappings ]] .", "h": ["generalized metaphor mappings"], "t": ["analyzing metaphors"]}, {"label": "PART-OF", "tokens": "Each << generalized metaphor >> contains a [[ recognition network ]] , a basic mapping , additional transfer mappings , and an implicit intention component .", "h": ["recognition network"], "t": ["generalized metaphor"]}, {"label": "CONJUNCTION", "tokens": "Each generalized metaphor contains a [[ recognition network ]] , a << basic mapping >> , additional transfer mappings , and an implicit intention component .", "h": ["recognition network"], "t": ["basic mapping"]}, {"label": "PART-OF", "tokens": "Each << generalized metaphor >> contains a recognition network , a [[ basic mapping ]] , additional transfer mappings , and an implicit intention component .", "h": ["basic mapping"], "t": ["generalized metaphor"]}, {"label": "PART-OF", "tokens": "Each << generalized metaphor >> contains a recognition network , a basic mapping , additional [[ transfer mappings ]] , and an implicit intention component .", "h": ["transfer mappings"], "t": ["generalized metaphor"]}, {"label": "CONJUNCTION", "tokens": "Each generalized metaphor contains a recognition network , a << basic mapping >> , additional [[ transfer mappings ]] , and an implicit intention component .", "h": ["transfer mappings"], "t": ["basic mapping"]}, {"label": "CONJUNCTION", "tokens": "Each generalized metaphor contains a recognition network , a basic mapping , additional [[ transfer mappings ]] , and an << implicit intention component >> .", "h": ["transfer mappings"], "t": ["implicit intention component"]}, {"label": "PART-OF", "tokens": "Each << generalized metaphor >> contains a recognition network , a basic mapping , additional transfer mappings , and an [[ implicit intention component ]] .", "h": ["implicit intention component"], "t": ["generalized metaphor"]}, {"label": "USED-FOR", "tokens": "It is argued that the [[ method ]] reduces << metaphor interpretation >> from a reconstruction to a recognition task .", "h": ["method"], "t": ["metaphor interpretation"]}, {"label": "USED-FOR", "tokens": "It is argued that the method reduces << metaphor interpretation >> from a reconstruction to a [[ recognition task ]] .", "h": ["recognition task"], "t": ["metaphor interpretation"]}, {"label": "USED-FOR", "tokens": "This study presents a << method to automatically acquire paraphrases >> using [[ bilingual corpora ]] , which utilizes the bilingual dependency relations obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques .", "h": ["bilingual corpora"], "t": ["method to automatically acquire paraphrases"]}, {"label": "USED-FOR", "tokens": "This study presents a << method to automatically acquire paraphrases >> using bilingual corpora , which utilizes the [[ bilingual dependency relations ]] obtained by projecting a monolingual dependency parse onto the other language sentence based on statistical alignment techniques .", "h": ["bilingual dependency relations"], "t": ["method to automatically acquire paraphrases"]}, {"label": "USED-FOR", "tokens": "This study presents a method to automatically acquire paraphrases using bilingual corpora , which utilizes the << bilingual dependency relations >> obtained by projecting a [[ monolingual dependency parse ]] onto the other language sentence based on statistical alignment techniques .", "h": ["monolingual dependency parse"], "t": ["bilingual dependency relations"]}, {"label": "USED-FOR", "tokens": "This study presents a method to automatically acquire paraphrases using bilingual corpora , which utilizes the << bilingual dependency relations >> obtained by projecting a monolingual dependency parse onto the other language sentence based on [[ statistical alignment techniques ]] .", "h": ["statistical alignment techniques"], "t": ["bilingual dependency relations"]}, {"label": "USED-FOR", "tokens": "Since the << paraphrasing method >> is capable of clearly disambiguating the sense of an original phrase using the [[ bilingual context of dependency relation ]] , it would be possible to obtain interchangeable paraphrases under a given context .", "h": ["bilingual context of dependency relation"], "t": ["paraphrasing method"]}, {"label": "USED-FOR", "tokens": "Also , we provide an advanced [[ method ]] to acquire << generalized translation knowledge >> using the extracted paraphrases .", "h": ["method"], "t": ["generalized translation knowledge"]}, {"label": "USED-FOR", "tokens": "Also , we provide an advanced << method >> to acquire generalized translation knowledge using the extracted [[ paraphrases ]] .", "h": ["paraphrases"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "We applied the [[ method ]] to acquire the << generalized translation knowledge >> for Korean-English translation .", "h": ["method"], "t": ["generalized translation knowledge"]}, {"label": "USED-FOR", "tokens": "We applied the method to acquire the [[ generalized translation knowledge ]] for << Korean-English translation >> .", "h": ["generalized translation knowledge"], "t": ["Korean-English translation"]}, {"label": "USED-FOR", "tokens": "Through experiments with parallel corpora of a Korean and English language pairs , we show that our [[ paraphrasing method ]] effectively extracts << paraphrases >> with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .", "h": ["paraphrasing method"], "t": ["paraphrases"]}, {"label": "EVALUATE-FOR", "tokens": "Through experiments with parallel corpora of a Korean and English language pairs , we show that our << paraphrasing method >> effectively extracts paraphrases with high [[ precision ]] , 94.3 % and 84.6 % respectively for Korean and English , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .", "h": ["precision"], "t": ["paraphrasing method"]}, {"label": "CONJUNCTION", "tokens": "Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for [[ Korean ]] and << English >> , and the translation knowledge extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % compression ratio .", "h": ["Korean"], "t": ["English"]}, {"label": "USED-FOR", "tokens": "Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the << translation knowledge >> extracted from the [[ bilingual corpora ]] could be generalized successfully using the paraphrases with the 12.5 % compression ratio .", "h": ["bilingual corpora"], "t": ["translation knowledge"]}, {"label": "USED-FOR", "tokens": "Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the << translation knowledge >> extracted from the bilingual corpora could be generalized successfully using the [[ paraphrases ]] with the 12.5 % compression ratio .", "h": ["paraphrases"], "t": ["translation knowledge"]}, {"label": "EVALUATE-FOR", "tokens": "Through experiments with parallel corpora of a Korean and English language pairs , we show that our paraphrasing method effectively extracts paraphrases with high precision , 94.3 % and 84.6 % respectively for Korean and English , and the << translation knowledge >> extracted from the bilingual corpora could be generalized successfully using the paraphrases with the 12.5 % [[ compression ratio ]] .", "h": ["compression ratio"], "t": ["translation knowledge"]}, {"label": "HYPONYM-OF", "tokens": "We provide a << logical definition of Minimalist grammars >> , that are [[ Stabler 's formalization of Chomsky 's minimalist program ]] .", "h": ["Stabler 's formalization of Chomsky 's minimalist program"], "t": ["logical definition of Minimalist grammars"]}, {"label": "USED-FOR", "tokens": "Our [[ logical definition ]] leads to a neat relation to categorial grammar , -LRB- yielding a treatment of << Montague semantics >> -RRB- , a parsing-as-deduction in a resource sensitive logic , and a learning algorithm from structured data -LRB- based on a typing-algorithm and type-unification -RRB- .", "h": ["logical definition"], "t": ["Montague semantics"]}, {"label": "USED-FOR", "tokens": "Our logical definition leads to a neat relation to categorial grammar , -LRB- yielding a treatment of Montague semantics -RRB- , a [[ parsing-as-deduction ]] in a << resource sensitive logic >> , and a learning algorithm from structured data -LRB- based on a typing-algorithm and type-unification -RRB- .", "h": ["parsing-as-deduction"], "t": ["resource sensitive logic"]}, {"label": "USED-FOR", "tokens": "Our logical definition leads to a neat relation to categorial grammar , -LRB- yielding a treatment of Montague semantics -RRB- , a parsing-as-deduction in a resource sensitive logic , and a << learning algorithm >> from [[ structured data ]] -LRB- based on a typing-algorithm and type-unification -RRB- .", "h": ["structured data"], "t": ["learning algorithm"]}, {"label": "USED-FOR", "tokens": "Our logical definition leads to a neat relation to categorial grammar , -LRB- yielding a treatment of Montague semantics -RRB- , a parsing-as-deduction in a resource sensitive logic , and a << learning algorithm >> from structured data -LRB- based on a [[ typing-algorithm ]] and type-unification -RRB- .", "h": ["typing-algorithm"], "t": ["learning algorithm"]}, {"label": "CONJUNCTION", "tokens": "Our logical definition leads to a neat relation to categorial grammar , -LRB- yielding a treatment of Montague semantics -RRB- , a parsing-as-deduction in a resource sensitive logic , and a learning algorithm from structured data -LRB- based on a [[ typing-algorithm ]] and << type-unification >> -RRB- .", "h": ["typing-algorithm"], "t": ["type-unification"]}, {"label": "USED-FOR", "tokens": "Our logical definition leads to a neat relation to categorial grammar , -LRB- yielding a treatment of Montague semantics -RRB- , a parsing-as-deduction in a resource sensitive logic , and a << learning algorithm >> from structured data -LRB- based on a typing-algorithm and [[ type-unification ]] -RRB- .", "h": ["type-unification"], "t": ["learning algorithm"]}, {"label": "USED-FOR", "tokens": "There are several [[ approaches ]] that model << information extraction >> as a token classification task , using various tagging strategies to combine multiple tokens .", "h": ["approaches"], "t": ["information extraction"]}, {"label": "HYPONYM-OF", "tokens": "There are several approaches that model [[ information extraction ]] as a << token classification task >> , using various tagging strategies to combine multiple tokens .", "h": ["information extraction"], "t": ["token classification task"]}, {"label": "USED-FOR", "tokens": "There are several approaches that model information extraction as a << token classification task >> , using various [[ tagging strategies ]] to combine multiple tokens .", "h": ["tagging strategies"], "t": ["token classification task"]}, {"label": "COMPARE", "tokens": "We also introduce a new strategy , called Begin/After tagging or BIA , and show that [[ it ]] is competitive to the best other << strategies >> .", "h": ["it"], "t": ["strategies"]}, {"label": "USED-FOR", "tokens": "The objective is a generic [[ system ]] of tools , including a core English lexicon , grammar , and concept representations , for building << natural language processing -LRB- NLP -RRB- systems >> for text understanding .", "h": ["system"], "t": ["natural language processing -LRB- NLP -RRB- systems"]}, {"label": "PART-OF", "tokens": "The objective is a generic << system >> of tools , including a [[ core English lexicon ]] , grammar , and concept representations , for building natural language processing -LRB- NLP -RRB- systems for text understanding .", "h": ["core English lexicon"], "t": ["system"]}, {"label": "PART-OF", "tokens": "The objective is a generic << system >> of tools , including a core English lexicon , [[ grammar ]] , and concept representations , for building natural language processing -LRB- NLP -RRB- systems for text understanding .", "h": ["grammar"], "t": ["system"]}, {"label": "PART-OF", "tokens": "The objective is a generic << system >> of tools , including a core English lexicon , grammar , and [[ concept representations ]] , for building natural language processing -LRB- NLP -RRB- systems for text understanding .", "h": ["concept representations"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "The objective is a generic system of tools , including a core English lexicon , grammar , and concept representations , for building [[ natural language processing -LRB- NLP -RRB- systems ]] for << text understanding >> .", "h": ["natural language processing -LRB- NLP -RRB- systems"], "t": ["text understanding"]}, {"label": "USED-FOR", "tokens": "Systems built with [[ PAKTUS ]] are intended to generate input to << knowledge based systems >> ordata base systems .", "h": ["PAKTUS"], "t": ["knowledge based systems"]}, {"label": "USED-FOR", "tokens": "Input to the << NLP system >> is typically derived from an existing [[ electronic message stream ]] , such as a news wire .", "h": ["electronic message stream"], "t": ["NLP system"]}, {"label": "HYPONYM-OF", "tokens": "Input to the NLP system is typically derived from an existing << electronic message stream >> , such as a [[ news wire ]] .", "h": ["news wire"], "t": ["electronic message stream"]}, {"label": "USED-FOR", "tokens": "[[ PAKTUS ]] supports the adaptation of the generic core to a variety of domains : << JINTACCS messages >> , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "h": ["PAKTUS"], "t": ["JINTACCS messages"]}, {"label": "USED-FOR", "tokens": "[[ PAKTUS ]] supports the adaptation of the generic core to a variety of domains : JINTACCS messages , << RAINFORM messages >> , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "h": ["PAKTUS"], "t": ["RAINFORM messages"]}, {"label": "USED-FOR", "tokens": "[[ PAKTUS ]] supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , << news reports >> about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "h": ["PAKTUS"], "t": ["news reports"]}, {"label": "CONJUNCTION", "tokens": "PAKTUS supports the adaptation of the generic core to a variety of domains : [[ JINTACCS messages ]] , << RAINFORM messages >> , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "h": ["JINTACCS messages"], "t": ["RAINFORM messages"]}, {"label": "CONJUNCTION", "tokens": "PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , [[ RAINFORM messages ]] , << news reports >> about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "h": ["RAINFORM messages"], "t": ["news reports"]}, {"label": "FEATURE-OF", "tokens": "PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , << news reports >> about a specific type of [[ event ]] , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "h": ["event"], "t": ["news reports"]}, {"label": "HYPONYM-OF", "tokens": "PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of << event >> , such as [[ financial transfers ]] or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "h": ["financial transfers"], "t": ["event"]}, {"label": "CONJUNCTION", "tokens": "PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as [[ financial transfers ]] or << terrorist acts >> , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "h": ["financial transfers"], "t": ["terrorist acts"]}, {"label": "HYPONYM-OF", "tokens": "PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of << event >> , such as financial transfers or [[ terrorist acts ]] , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and discourse patterns .", "h": ["terrorist acts"], "t": ["event"]}, {"label": "USED-FOR", "tokens": "<< PAKTUS >> supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring [[ sublanguage and domain-specific grammar ]] , words , conceptual mappings , and discourse patterns .", "h": ["sublanguage and domain-specific grammar"], "t": ["PAKTUS"]}, {"label": "CONJUNCTION", "tokens": "PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring [[ sublanguage and domain-specific grammar ]] , << words >> , conceptual mappings , and discourse patterns .", "h": ["sublanguage and domain-specific grammar"], "t": ["words"]}, {"label": "USED-FOR", "tokens": "<< PAKTUS >> supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , [[ words ]] , conceptual mappings , and discourse patterns .", "h": ["words"], "t": ["PAKTUS"]}, {"label": "CONJUNCTION", "tokens": "PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , [[ words ]] , << conceptual mappings >> , and discourse patterns .", "h": ["words"], "t": ["conceptual mappings"]}, {"label": "USED-FOR", "tokens": "<< PAKTUS >> supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , [[ conceptual mappings ]] , and discourse patterns .", "h": ["conceptual mappings"], "t": ["PAKTUS"]}, {"label": "CONJUNCTION", "tokens": "PAKTUS supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , [[ conceptual mappings ]] , and << discourse patterns >> .", "h": ["conceptual mappings"], "t": ["discourse patterns"]}, {"label": "USED-FOR", "tokens": "<< PAKTUS >> supports the adaptation of the generic core to a variety of domains : JINTACCS messages , RAINFORM messages , news reports about a specific type of event , such as financial transfers or terrorist acts , etc. , by acquiring sublanguage and domain-specific grammar , words , conceptual mappings , and [[ discourse patterns ]] .", "h": ["discourse patterns"], "t": ["PAKTUS"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper the << LIMSI recognizer >> which was evaluated in the [[ ARPA NOV93 CSR test ]] is described , and experimental results on the WSJ and BREF corpora under closely matched conditions are reported .", "h": ["ARPA NOV93 CSR test"], "t": ["LIMSI recognizer"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper the << LIMSI recognizer >> which was evaluated in the ARPA NOV93 CSR test is described , and experimental results on the [[ WSJ and BREF corpora ]] under closely matched conditions are reported .", "h": ["WSJ and BREF corpora"], "t": ["LIMSI recognizer"]}, {"label": "EVALUATE-FOR", "tokens": "For both [[ corpora ]] << word recognition >> experiments were carried out with vocabularies containing up to 20k words .", "h": ["corpora"], "t": ["word recognition"]}, {"label": "USED-FOR", "tokens": "The << recognizer >> makes use of [[ continuous density HMM ]] with Gaussian mixture for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling .", "h": ["continuous density HMM"], "t": ["recognizer"]}, {"label": "CONJUNCTION", "tokens": "The recognizer makes use of [[ continuous density HMM ]] with << Gaussian mixture >> for acoustic modeling and n-gram statistics estimated on the newspaper texts for language modeling .", "h": ["continuous density HMM"], "t": ["Gaussian mixture"]}, {"label": "USED-FOR", "tokens": "The recognizer makes use of [[ continuous density HMM ]] with Gaussian mixture for << acoustic modeling >> and n-gram statistics estimated on the newspaper texts for language modeling .", "h": ["continuous density HMM"], "t": ["acoustic modeling"]}, {"label": "CONJUNCTION", "tokens": "The recognizer makes use of [[ continuous density HMM ]] with Gaussian mixture for acoustic modeling and << n-gram statistics >> estimated on the newspaper texts for language modeling .", "h": ["continuous density HMM"], "t": ["n-gram statistics"]}, {"label": "USED-FOR", "tokens": "The recognizer makes use of continuous density HMM with [[ Gaussian mixture ]] for << acoustic modeling >> and n-gram statistics estimated on the newspaper texts for language modeling .", "h": ["Gaussian mixture"], "t": ["acoustic modeling"]}, {"label": "USED-FOR", "tokens": "The << recognizer >> makes use of continuous density HMM with Gaussian mixture for acoustic modeling and [[ n-gram statistics ]] estimated on the newspaper texts for language modeling .", "h": ["n-gram statistics"], "t": ["recognizer"]}, {"label": "USED-FOR", "tokens": "The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and [[ n-gram statistics ]] estimated on the newspaper texts for << language modeling >> .", "h": ["n-gram statistics"], "t": ["language modeling"]}, {"label": "EVALUATE-FOR", "tokens": "The recognizer makes use of continuous density HMM with Gaussian mixture for acoustic modeling and << n-gram statistics >> estimated on the [[ newspaper texts ]] for language modeling .", "h": ["newspaper texts"], "t": ["n-gram statistics"]}, {"label": "USED-FOR", "tokens": "The << recognizer >> uses a [[ time-synchronous graph-search strategy ]] which is shown to still be viable with a 20k-word vocabulary when used with bigram back-off language models .", "h": ["time-synchronous graph-search strategy"], "t": ["recognizer"]}, {"label": "USED-FOR", "tokens": "The << recognizer >> uses a time-synchronous graph-search strategy which is shown to still be viable with a 20k-word vocabulary when used with [[ bigram back-off language models ]] .", "h": ["bigram back-off language models"], "t": ["recognizer"]}, {"label": "CONJUNCTION", "tokens": "The recognizer uses a << time-synchronous graph-search strategy >> which is shown to still be viable with a 20k-word vocabulary when used with [[ bigram back-off language models ]] .", "h": ["bigram back-off language models"], "t": ["time-synchronous graph-search strategy"]}, {"label": "USED-FOR", "tokens": "A second forward pass , which makes use of a << word graph >> generated with the [[ bigram ]] , incorporates a trigram language model .", "h": ["bigram"], "t": ["word graph"]}, {"label": "CONJUNCTION", "tokens": "A second forward pass , which makes use of a << word graph >> generated with the bigram , incorporates a [[ trigram language model ]] .", "h": ["trigram language model"], "t": ["word graph"]}, {"label": "USED-FOR", "tokens": "<< Acoustic modeling >> uses [[ cepstrum-based features ]] , context-dependent phone models -LRB- intra and interword -RRB- , phone duration models , and sex-dependent models .", "h": ["cepstrum-based features"], "t": ["Acoustic modeling"]}, {"label": "CONJUNCTION", "tokens": "Acoustic modeling uses [[ cepstrum-based features ]] , << context-dependent phone models -LRB- intra and interword -RRB- >> , phone duration models , and sex-dependent models .", "h": ["cepstrum-based features"], "t": ["context-dependent phone models -LRB- intra and interword -RRB-"]}, {"label": "USED-FOR", "tokens": "<< Acoustic modeling >> uses cepstrum-based features , [[ context-dependent phone models -LRB- intra and interword -RRB- ]] , phone duration models , and sex-dependent models .", "h": ["context-dependent phone models -LRB- intra and interword -RRB-"], "t": ["Acoustic modeling"]}, {"label": "CONJUNCTION", "tokens": "Acoustic modeling uses cepstrum-based features , [[ context-dependent phone models -LRB- intra and interword -RRB- ]] , << phone duration models >> , and sex-dependent models .", "h": ["context-dependent phone models -LRB- intra and interword -RRB-"], "t": ["phone duration models"]}, {"label": "USED-FOR", "tokens": "<< Acoustic modeling >> uses cepstrum-based features , context-dependent phone models -LRB- intra and interword -RRB- , [[ phone duration models ]] , and sex-dependent models .", "h": ["phone duration models"], "t": ["Acoustic modeling"]}, {"label": "CONJUNCTION", "tokens": "Acoustic modeling uses cepstrum-based features , context-dependent phone models -LRB- intra and interword -RRB- , [[ phone duration models ]] , and << sex-dependent models >> .", "h": ["phone duration models"], "t": ["sex-dependent models"]}, {"label": "USED-FOR", "tokens": "<< Acoustic modeling >> uses cepstrum-based features , context-dependent phone models -LRB- intra and interword -RRB- , phone duration models , and [[ sex-dependent models ]] .", "h": ["sex-dependent models"], "t": ["Acoustic modeling"]}, {"label": "USED-FOR", "tokens": "The [[ co-occurrence pattern ]] , a combination of binary or local features , is more discriminative than individual features and has shown its advantages in << object , scene , and action recognition >> .", "h": ["co-occurrence pattern"], "t": ["object , scene , and action recognition"]}, {"label": "PART-OF", "tokens": "The << co-occurrence pattern >> , a combination of [[ binary or local features ]] , is more discriminative than individual features and has shown its advantages in object , scene , and action recognition .", "h": ["binary or local features"], "t": ["co-occurrence pattern"]}, {"label": "USED-FOR", "tokens": "Then we propose a novel [[ data mining method ]] to efficiently discover the << optimal co-occurrence pattern >> with minimum empirical error , despite the noisy training dataset .", "h": ["data mining method"], "t": ["optimal co-occurrence pattern"]}, {"label": "FEATURE-OF", "tokens": "Then we propose a novel data mining method to efficiently discover the << optimal co-occurrence pattern >> with [[ minimum empirical error ]] , despite the noisy training dataset .", "h": ["minimum empirical error"], "t": ["optimal co-occurrence pattern"]}, {"label": "USED-FOR", "tokens": "Then we propose a novel << data mining method >> to efficiently discover the optimal co-occurrence pattern with minimum empirical error , despite the [[ noisy training dataset ]] .", "h": ["noisy training dataset"], "t": ["data mining method"]}, {"label": "USED-FOR", "tokens": "This [[ mining procedure ]] of << AND and OR patterns >> is readily integrated to boosting , which improves the generalization ability over the conventional boosting decision trees and boosting decision stumps .", "h": ["mining procedure"], "t": ["AND and OR patterns"]}, {"label": "PART-OF", "tokens": "This mining procedure of [[ AND and OR patterns ]] is readily integrated to << boosting >> , which improves the generalization ability over the conventional boosting decision trees and boosting decision stumps .", "h": ["AND and OR patterns"], "t": ["boosting"]}, {"label": "COMPARE", "tokens": "This mining procedure of AND and OR patterns is readily integrated to [[ boosting ]] , which improves the generalization ability over the conventional << boosting decision trees >> and boosting decision stumps .", "h": ["boosting"], "t": ["boosting decision trees"]}, {"label": "COMPARE", "tokens": "This mining procedure of AND and OR patterns is readily integrated to [[ boosting ]] , which improves the generalization ability over the conventional boosting decision trees and << boosting decision stumps >> .", "h": ["boosting"], "t": ["boosting decision stumps"]}, {"label": "EVALUATE-FOR", "tokens": "This mining procedure of AND and OR patterns is readily integrated to << boosting >> , which improves the [[ generalization ability ]] over the conventional boosting decision trees and boosting decision stumps .", "h": ["generalization ability"], "t": ["boosting"]}, {"label": "EVALUATE-FOR", "tokens": "This mining procedure of AND and OR patterns is readily integrated to boosting , which improves the [[ generalization ability ]] over the conventional << boosting decision trees >> and boosting decision stumps .", "h": ["generalization ability"], "t": ["boosting decision trees"]}, {"label": "EVALUATE-FOR", "tokens": "This mining procedure of AND and OR patterns is readily integrated to boosting , which improves the [[ generalization ability ]] over the conventional boosting decision trees and << boosting decision stumps >> .", "h": ["generalization ability"], "t": ["boosting decision stumps"]}, {"label": "CONJUNCTION", "tokens": "This mining procedure of AND and OR patterns is readily integrated to boosting , which improves the generalization ability over the conventional [[ boosting decision trees ]] and << boosting decision stumps >> .", "h": ["boosting decision trees"], "t": ["boosting decision stumps"]}, {"label": "EVALUATE-FOR", "tokens": "Our versatile experiments on [[ object , scene , and action cat-egorization ]] validate the advantages of the discovered << dis-criminative co-occurrence patterns >> .", "h": ["object , scene , and action cat-egorization"], "t": ["dis-criminative co-occurrence patterns"]}, {"label": "USED-FOR", "tokens": "Empirical experience and observations have shown us when powerful and highly tunable [[ classifiers ]] such as maximum entropy classifiers , boosting and SVMs are applied to << language processing tasks >> , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .", "h": ["classifiers"], "t": ["language processing tasks"]}, {"label": "HYPONYM-OF", "tokens": "Empirical experience and observations have shown us when powerful and highly tunable << classifiers >> such as [[ maximum entropy classifiers ]] , boosting and SVMs are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .", "h": ["maximum entropy classifiers"], "t": ["classifiers"]}, {"label": "CONJUNCTION", "tokens": "Empirical experience and observations have shown us when powerful and highly tunable classifiers such as [[ maximum entropy classifiers ]] , << boosting >> and SVMs are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .", "h": ["maximum entropy classifiers"], "t": ["boosting"]}, {"label": "HYPONYM-OF", "tokens": "Empirical experience and observations have shown us when powerful and highly tunable << classifiers >> such as maximum entropy classifiers , [[ boosting ]] and SVMs are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .", "h": ["boosting"], "t": ["classifiers"]}, {"label": "CONJUNCTION", "tokens": "Empirical experience and observations have shown us when powerful and highly tunable classifiers such as maximum entropy classifiers , [[ boosting ]] and << SVMs >> are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .", "h": ["boosting"], "t": ["SVMs"]}, {"label": "HYPONYM-OF", "tokens": "Empirical experience and observations have shown us when powerful and highly tunable << classifiers >> such as maximum entropy classifiers , boosting and [[ SVMs ]] are applied to language processing tasks , it is possible to achieve high accuracies , but eventually their performances all tend to plateau out at around the same point .", "h": ["SVMs"], "t": ["classifiers"]}, {"label": "HYPONYM-OF", "tokens": "In recent work , we introduced [[ N-fold Templated Piped Correction , or NTPC -LRB- `` nitpick '' -RRB- ]] , an intriguing << error corrector >> that is designed to work in these extreme operating conditions .", "h": ["N-fold Templated Piped Correction , or NTPC -LRB- `` nitpick '' -RRB-"], "t": ["error corrector"]}, {"label": "COMPARE", "tokens": "Despite its simplicity , [[ it ]] consistently and robustly improves the accuracy of existing highly accurate << base models >> .", "h": ["it"], "t": ["base models"]}, {"label": "USED-FOR", "tokens": "Focused interaction of this kind is facilitated by a [[ construction-specific approach ]] to << flexible parsing >> , with specialized parsing techniques for each type of construction , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .", "h": ["construction-specific approach"], "t": ["flexible parsing"]}, {"label": "CONJUNCTION", "tokens": "Focused interaction of this kind is facilitated by a [[ construction-specific approach ]] to flexible parsing , with << specialized parsing techniques >> for each type of construction , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .", "h": ["construction-specific approach"], "t": ["specialized parsing techniques"]}, {"label": "USED-FOR", "tokens": "Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing , with [[ specialized parsing techniques ]] for each type of << construction >> , and specialized ambiguity representations for each type of ambiguity that a particular construction can give rise to .", "h": ["specialized parsing techniques"], "t": ["construction"]}, {"label": "CONJUNCTION", "tokens": "Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing , with [[ specialized parsing techniques ]] for each type of construction , and specialized << ambiguity representations >> for each type of ambiguity that a particular construction can give rise to .", "h": ["specialized parsing techniques"], "t": ["ambiguity representations"]}, {"label": "USED-FOR", "tokens": "Focused interaction of this kind is facilitated by a construction-specific approach to flexible parsing , with specialized parsing techniques for each type of construction , and specialized [[ ambiguity representations ]] for each type of << ambiguity >> that a particular construction can give rise to .", "h": ["ambiguity representations"], "t": ["ambiguity"]}, {"label": "USED-FOR", "tokens": "A [[ construction-specific approach ]] also aids in << task-specific language development >> by allowing a language definition that is natural in terms of the task domain to be interpreted directly without compilation into a uniform grammar formalism , thus greatly speeding the testing of changes to the language definition .", "h": ["construction-specific approach"], "t": ["task-specific language development"]}, {"label": "USED-FOR", "tokens": "A proposal to deal with << French tenses >> in the framework of [[ Discourse Representation Theory ]] is presented , as it has been implemented for a fragment at the IMS .", "h": ["Discourse Representation Theory"], "t": ["French tenses"]}, {"label": "USED-FOR", "tokens": "A proposal to deal with French tenses in the framework of Discourse Representation Theory is presented , as [[ it ]] has been implemented for a fragment at the << IMS >> .", "h": ["it"], "t": ["IMS"]}, {"label": "USED-FOR", "tokens": "<< It >> is based on the [[ theory of tenses ]] of H. Kamp and Ch .", "h": ["theory of tenses"], "t": ["It"]}, {"label": "USED-FOR", "tokens": "Instead of using [[ operators ]] to express the << meaning of the tenses >> the Reichenbachian point of view is adopted and refined such that the impact of the tenses with respect to the meaning of the text is understood as contribution to the integration of the events of a sentence in the event structure of the preceeding text .", "h": ["operators"], "t": ["meaning of the tenses"]}, {"label": "USED-FOR", "tokens": "Thereby a << system of relevant times >> provided by the [[ preceeding text ]] and by the temporal adverbials of the sentence being processed is used .", "h": ["preceeding text"], "t": ["system of relevant times"]}, {"label": "CONJUNCTION", "tokens": "Thereby a system of relevant times provided by the [[ preceeding text ]] and by the << temporal adverbials >> of the sentence being processed is used .", "h": ["preceeding text"], "t": ["temporal adverbials"]}, {"label": "USED-FOR", "tokens": "Thereby a << system of relevant times >> provided by the preceeding text and by the [[ temporal adverbials ]] of the sentence being processed is used .", "h": ["temporal adverbials"], "t": ["system of relevant times"]}, {"label": "PART-OF", "tokens": "This << system >> consists of one or more [[ reference times ]] and temporal perspective times , the speech time and the location time .", "h": ["reference times"], "t": ["system"]}, {"label": "CONJUNCTION", "tokens": "This system consists of one or more [[ reference times ]] and << temporal perspective times >> , the speech time and the location time .", "h": ["reference times"], "t": ["temporal perspective times"]}, {"label": "PART-OF", "tokens": "This << system >> consists of one or more reference times and [[ temporal perspective times ]] , the speech time and the location time .", "h": ["temporal perspective times"], "t": ["system"]}, {"label": "CONJUNCTION", "tokens": "This system consists of one or more reference times and [[ temporal perspective times ]] , the << speech time >> and the location time .", "h": ["temporal perspective times"], "t": ["speech time"]}, {"label": "PART-OF", "tokens": "This << system >> consists of one or more reference times and temporal perspective times , the [[ speech time ]] and the location time .", "h": ["speech time"], "t": ["system"]}, {"label": "CONJUNCTION", "tokens": "This system consists of one or more reference times and temporal perspective times , the [[ speech time ]] and the << location time >> .", "h": ["speech time"], "t": ["location time"]}, {"label": "PART-OF", "tokens": "This << system >> consists of one or more reference times and temporal perspective times , the speech time and the [[ location time ]] .", "h": ["location time"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "In opposition to the approach of Kamp and Rohrer the exact << meaning of the tenses >> is fixed by the [[ resolution component ]] and not in the process of syntactic analysis .", "h": ["resolution component"], "t": ["meaning of the tenses"]}, {"label": "COMPARE", "tokens": "In opposition to the approach of Kamp and Rohrer the exact meaning of the tenses is fixed by the [[ resolution component ]] and not in the process of << syntactic analysis >> .", "h": ["resolution component"], "t": ["syntactic analysis"]}, {"label": "PART-OF", "tokens": "The work presented in this paper is the first step in a project which aims to cluster and summarise [[ electronic discussions ]] in the context of << help-desk applications >> .", "h": ["electronic discussions"], "t": ["help-desk applications"]}, {"label": "FEATURE-OF", "tokens": "In this paper , we identify [[ features ]] of << electronic discussions >> that influence the clustering process , and offer a filtering mechanism that removes undesirable influences .", "h": ["features"], "t": ["electronic discussions"]}, {"label": "EVALUATE-FOR", "tokens": "We tested the << clustering and filtering processes >> on [[ electronic newsgroup discussions ]] , and evaluated their performance by means of two experiments : coarse-level clustering simple information retrieval .", "h": ["electronic newsgroup discussions"], "t": ["clustering and filtering processes"]}, {"label": "EVALUATE-FOR", "tokens": "We tested the << clustering and filtering processes >> on electronic newsgroup discussions , and evaluated their performance by means of two [[ experiments ]] : coarse-level clustering simple information retrieval .", "h": ["experiments"], "t": ["clustering and filtering processes"]}, {"label": "EVALUATE-FOR", "tokens": "We tested the << clustering and filtering processes >> on electronic newsgroup discussions , and evaluated their performance by means of two [[ experiments ]] : coarse-level clustering simple information retrieval .", "h": ["experiments"], "t": ["clustering and filtering processes"]}, {"label": "HYPONYM-OF", "tokens": "We tested the clustering and filtering processes on electronic newsgroup discussions , and evaluated their performance by means of two << experiments >> : [[ coarse-level clustering ]] simple information retrieval .", "h": ["coarse-level clustering"], "t": ["experiments"]}, {"label": "HYPONYM-OF", "tokens": "We tested the clustering and filtering processes on electronic newsgroup discussions , and evaluated their performance by means of two << experiments >> : coarse-level clustering simple [[ information retrieval ]] .", "h": ["information retrieval"], "t": ["experiments"]}, {"label": "USED-FOR", "tokens": "The paper presents a [[ method ]] for << word sense disambiguation >> based on parallel corpora .", "h": ["method"], "t": ["word sense disambiguation"]}, {"label": "USED-FOR", "tokens": "The paper presents a << method >> for word sense disambiguation based on [[ parallel corpora ]] .", "h": ["parallel corpora"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The [[ method ]] exploits recent advances in << word alignment >> and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus .", "h": ["method"], "t": ["word alignment"]}, {"label": "USED-FOR", "tokens": "The [[ method ]] exploits recent advances in word alignment and << word clustering >> based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus .", "h": ["method"], "t": ["word clustering"]}, {"label": "CONJUNCTION", "tokens": "The method exploits recent advances in [[ word alignment ]] and << word clustering >> based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus .", "h": ["word alignment"], "t": ["word clustering"]}, {"label": "USED-FOR", "tokens": "The << method >> exploits recent advances in word alignment and word clustering based on [[ automatic extraction of translation equivalents ]] and being supported by available aligned wordnets for the languages in the corpus .", "h": ["automatic extraction of translation equivalents"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The << method >> exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available [[ aligned wordnets ]] for the languages in the corpus .", "h": ["aligned wordnets"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The same [[ system ]] used in a validation mode , can be used to check and spot << alignment errors in multilingually aligned wordnets >> as BalkaNet and EuroWordNet .", "h": ["system"], "t": ["alignment errors in multilingually aligned wordnets"]}, {"label": "HYPONYM-OF", "tokens": "The same system used in a validation mode , can be used to check and spot alignment errors in << multilingually aligned wordnets >> as [[ BalkaNet ]] and EuroWordNet .", "h": ["BalkaNet"], "t": ["multilingually aligned wordnets"]}, {"label": "CONJUNCTION", "tokens": "The same system used in a validation mode , can be used to check and spot alignment errors in multilingually aligned wordnets as [[ BalkaNet ]] and << EuroWordNet >> .", "h": ["BalkaNet"], "t": ["EuroWordNet"]}, {"label": "HYPONYM-OF", "tokens": "The same system used in a validation mode , can be used to check and spot alignment errors in << multilingually aligned wordnets >> as BalkaNet and [[ EuroWordNet ]] .", "h": ["EuroWordNet"], "t": ["multilingually aligned wordnets"]}, {"label": "USED-FOR", "tokens": "This paper investigates critical configurations for << projective reconstruction >> from multiple [[ images ]] taken by a camera moving in a straight line .", "h": ["images"], "t": ["projective reconstruction"]}, {"label": "FEATURE-OF", "tokens": "Projective reconstruction refers to a determination of the [[ 3D geometrical configuration ]] of a set of << 3D points and cameras >> , given only correspondences between points in the images .", "h": ["3D geometrical configuration"], "t": ["3D points and cameras"]}, {"label": "USED-FOR", "tokens": "Porting a [[ Natural Language Processing -LRB- NLP -RRB- system ]] to a << new domain >> remains one of the bottlenecks in syntactic parsing , because of the amount of effort required to fix gaps in the lexicon , and to attune the existing grammar to the idiosyncracies of the new sublanguage .", "h": ["Natural Language Processing -LRB- NLP -RRB- system"], "t": ["new domain"]}, {"label": "USED-FOR", "tokens": "Porting a Natural Language Processing -LRB- NLP -RRB- system to a new domain remains one of the bottlenecks in syntactic parsing , because of the amount of effort required to fix gaps in the lexicon , and to attune the existing [[ grammar ]] to the << idiosyncracies of the new sublanguage >> .", "h": ["grammar"], "t": ["idiosyncracies of the new sublanguage"]}, {"label": "PART-OF", "tokens": "This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a << hybrid system >> that combines traditional [[ knowledge-based techniques ]] with a corpus-based approach .", "h": ["knowledge-based techniques"], "t": ["hybrid system"]}, {"label": "CONJUNCTION", "tokens": "This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a hybrid system that combines traditional [[ knowledge-based techniques ]] with a << corpus-based approach >> .", "h": ["knowledge-based techniques"], "t": ["corpus-based approach"]}, {"label": "PART-OF", "tokens": "This paper shows how the process of fitting a lexicalized grammar to a domain can be automated to a great extent by using a << hybrid system >> that combines traditional knowledge-based techniques with a [[ corpus-based approach ]] .", "h": ["corpus-based approach"], "t": ["hybrid system"]}, {"label": "USED-FOR", "tokens": "Unification is often the appropriate [[ method ]] for expressing << relations between representations >> in the form of feature structures ; however , there are circumstances in which a different approach is desirable .", "h": ["method"], "t": ["relations between representations"]}, {"label": "USED-FOR", "tokens": "Unification is often the appropriate method for expressing << relations between representations >> in the form of [[ feature structures ]] ; however , there are circumstances in which a different approach is desirable .", "h": ["feature structures"], "t": ["relations between representations"]}, {"label": "COMPARE", "tokens": "Unification is often the appropriate << method >> for expressing relations between representations in the form of feature structures ; however , there are circumstances in which a different [[ approach ]] is desirable .", "h": ["approach"], "t": ["method"]}, {"label": "FEATURE-OF", "tokens": "A << declarative formalism >> is presented which permits [[ direct mappings of one feature structure into another ]] , and illustrative examples are given of its application to areas of current interest .", "h": ["direct mappings of one feature structure into another"], "t": ["declarative formalism"]}, {"label": "USED-FOR", "tokens": "To support engaging human users in robust , mixed-initiative speech dialogue interactions which reach beyond current capabilities in dialogue systems , the DARPA Communicator program -LSB- 1 -RSB- is funding the development of a [[ distributed message-passing infrastructure ]] for << dialogue systems >> which all Communicator participants are using .", "h": ["distributed message-passing infrastructure"], "t": ["dialogue systems"]}, {"label": "USED-FOR", "tokens": "We propose a novel [[ limited-memory stochastic block BFGS update ]] for << incorporating enriched curvature information in stochastic approximation methods >> .", "h": ["limited-memory stochastic block BFGS update"], "t": ["incorporating enriched curvature information in stochastic approximation methods"]}, {"label": "USED-FOR", "tokens": "In our method , the estimate of the << inverse Hessian matrix >> that is maintained by [[ it ]] , is updated at each iteration using a sketch of the Hessian , i.e. , a randomly generated compressed form of the Hessian .", "h": ["it"], "t": ["inverse Hessian matrix"]}, {"label": "USED-FOR", "tokens": "In our method , the estimate of the inverse Hessian matrix that is maintained by << it >> , is updated at each iteration using a sketch of the [[ Hessian ]] , i.e. , a randomly generated compressed form of the Hessian .", "h": ["Hessian"], "t": ["it"]}, {"label": "HYPONYM-OF", "tokens": "In our method , the estimate of the inverse Hessian matrix that is maintained by it , is updated at each iteration using a sketch of the << Hessian >> , i.e. , a [[ randomly generated compressed form of the Hessian ]] .", "h": ["randomly generated compressed form of the Hessian"], "t": ["Hessian"]}, {"label": "USED-FOR", "tokens": "We propose several sketching strategies , present a new [[ quasi-Newton method ]] that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute << batch stochastic gradients >> , and prove linear convergence of the resulting method .", "h": ["quasi-Newton method"], "t": ["batch stochastic gradients"]}, {"label": "USED-FOR", "tokens": "We propose several sketching strategies , present a new << quasi-Newton method >> that uses [[ stochastic block BFGS updates ]] combined with the variance reduction approach SVRG to compute batch stochastic gradients , and prove linear convergence of the resulting method .", "h": ["stochastic block BFGS updates"], "t": ["quasi-Newton method"]}, {"label": "CONJUNCTION", "tokens": "We propose several sketching strategies , present a new quasi-Newton method that uses [[ stochastic block BFGS updates ]] combined with the << variance reduction approach SVRG >> to compute batch stochastic gradients , and prove linear convergence of the resulting method .", "h": ["stochastic block BFGS updates"], "t": ["variance reduction approach SVRG"]}, {"label": "USED-FOR", "tokens": "We propose several sketching strategies , present a new << quasi-Newton method >> that uses stochastic block BFGS updates combined with the [[ variance reduction approach SVRG ]] to compute batch stochastic gradients , and prove linear convergence of the resulting method .", "h": ["variance reduction approach SVRG"], "t": ["quasi-Newton method"]}, {"label": "FEATURE-OF", "tokens": "We propose several sketching strategies , present a new quasi-Newton method that uses stochastic block BFGS updates combined with the variance reduction approach SVRG to compute batch stochastic gradients , and prove [[ linear convergence ]] of the resulting << method >> .", "h": ["linear convergence"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "Numerical tests on [[ large-scale logistic regression problems ]] reveal that our << method >> is more robust and substantially outperforms current state-of-the-art methods .", "h": ["large-scale logistic regression problems"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "Numerical tests on [[ large-scale logistic regression problems ]] reveal that our method is more robust and substantially outperforms current << state-of-the-art methods >> .", "h": ["large-scale logistic regression problems"], "t": ["state-of-the-art methods"]}, {"label": "COMPARE", "tokens": "Numerical tests on large-scale logistic regression problems reveal that our [[ method ]] is more robust and substantially outperforms current << state-of-the-art methods >> .", "h": ["method"], "t": ["state-of-the-art methods"]}, {"label": "USED-FOR", "tokens": "The goal of this research is to develop a [[ spoken language system ]] that will demonstrate the usefulness of voice input for << interactive problem solving >> .", "h": ["spoken language system"], "t": ["interactive problem solving"]}, {"label": "USED-FOR", "tokens": "The goal of this research is to develop a spoken language system that will demonstrate the usefulness of [[ voice input ]] for << interactive problem solving >> .", "h": ["voice input"], "t": ["interactive problem solving"]}, {"label": "CONJUNCTION", "tokens": "Combining [[ speech recognition ]] and << natural language processing >> to achieve speech understanding , the system will be demonstrated in an application domain relevant to the DoD .", "h": ["speech recognition"], "t": ["natural language processing"]}, {"label": "USED-FOR", "tokens": "Combining [[ speech recognition ]] and natural language processing to achieve << speech understanding >> , the system will be demonstrated in an application domain relevant to the DoD .", "h": ["speech recognition"], "t": ["speech understanding"]}, {"label": "USED-FOR", "tokens": "Combining speech recognition and [[ natural language processing ]] to achieve << speech understanding >> , the system will be demonstrated in an application domain relevant to the DoD .", "h": ["natural language processing"], "t": ["speech understanding"]}, {"label": "USED-FOR", "tokens": "The objective of this project is to develop a << robust and high-performance speech recognition system >> using a [[ segment-based approach ]] to phonetic recognition .", "h": ["segment-based approach"], "t": ["robust and high-performance speech recognition system"]}, {"label": "USED-FOR", "tokens": "The objective of this project is to develop a robust and high-performance speech recognition system using a [[ segment-based approach ]] to << phonetic recognition >> .", "h": ["segment-based approach"], "t": ["phonetic recognition"]}, {"label": "USED-FOR", "tokens": "The objective of this project is to develop a << robust and high-performance speech recognition system >> using a segment-based approach to [[ phonetic recognition ]] .", "h": ["phonetic recognition"], "t": ["robust and high-performance speech recognition system"]}, {"label": "USED-FOR", "tokens": "The [[ recognition system ]] will eventually be integrated with natural language processing to achieve << spoken language understanding >> .", "h": ["recognition system"], "t": ["spoken language understanding"]}, {"label": "CONJUNCTION", "tokens": "The << recognition system >> will eventually be integrated with [[ natural language processing ]] to achieve spoken language understanding .", "h": ["natural language processing"], "t": ["recognition system"]}, {"label": "USED-FOR", "tokens": "The recognition system will eventually be integrated with [[ natural language processing ]] to achieve << spoken language understanding >> .", "h": ["natural language processing"], "t": ["spoken language understanding"]}, {"label": "PART-OF", "tokens": "[[ Spelling-checkers ]] have become an integral part of most << text processing software >> .", "h": ["Spelling-checkers"], "t": ["text processing software"]}, {"label": "USED-FOR", "tokens": "From different reasons among which the speed of processing prevails << they >> are usually based on [[ dictionaries of word forms ]] instead of words .", "h": ["dictionaries of word forms"], "t": ["they"]}, {"label": "USED-FOR", "tokens": "This << approach >> is sufficient for [[ languages ]] with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .", "h": ["languages"], "t": ["approach"]}, {"label": "FEATURE-OF", "tokens": "This approach is sufficient for << languages >> with little [[ inflection ]] such as English , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .", "h": ["inflection"], "t": ["languages"]}, {"label": "HYPONYM-OF", "tokens": "This approach is sufficient for << languages >> with little inflection such as [[ English ]] , but fails for highly inflective languages such as Czech , Russian , Slovak or other Slavonic languages .", "h": ["English"], "t": ["languages"]}, {"label": "HYPONYM-OF", "tokens": "This approach is sufficient for languages with little inflection such as English , but fails for << highly inflective languages >> such as [[ Czech ]] , Russian , Slovak or other Slavonic languages .", "h": ["Czech"], "t": ["highly inflective languages"]}, {"label": "CONJUNCTION", "tokens": "This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as [[ Czech ]] , << Russian >> , Slovak or other Slavonic languages .", "h": ["Czech"], "t": ["Russian"]}, {"label": "HYPONYM-OF", "tokens": "This approach is sufficient for languages with little inflection such as English , but fails for << highly inflective languages >> such as Czech , [[ Russian ]] , Slovak or other Slavonic languages .", "h": ["Russian"], "t": ["highly inflective languages"]}, {"label": "CONJUNCTION", "tokens": "This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , [[ Russian ]] , << Slovak >> or other Slavonic languages .", "h": ["Russian"], "t": ["Slovak"]}, {"label": "HYPONYM-OF", "tokens": "This approach is sufficient for languages with little inflection such as English , but fails for << highly inflective languages >> such as Czech , Russian , [[ Slovak ]] or other Slavonic languages .", "h": ["Slovak"], "t": ["highly inflective languages"]}, {"label": "CONJUNCTION", "tokens": "This approach is sufficient for languages with little inflection such as English , but fails for highly inflective languages such as Czech , Russian , [[ Slovak ]] or other << Slavonic languages >> .", "h": ["Slovak"], "t": ["Slavonic languages"]}, {"label": "HYPONYM-OF", "tokens": "This approach is sufficient for languages with little inflection such as English , but fails for << highly inflective languages >> such as Czech , Russian , Slovak or other [[ Slavonic languages ]] .", "h": ["Slavonic languages"], "t": ["highly inflective languages"]}, {"label": "USED-FOR", "tokens": "We have developed a special [[ method ]] for describing << inflection >> for the purpose of building spelling-checkers for such languages .", "h": ["method"], "t": ["inflection"]}, {"label": "USED-FOR", "tokens": "We have developed a special [[ method ]] for describing inflection for the purpose of building << spelling-checkers >> for such languages .", "h": ["method"], "t": ["spelling-checkers"]}, {"label": "USED-FOR", "tokens": "We have developed a special method for describing inflection for the purpose of building [[ spelling-checkers ]] for such << languages >> .", "h": ["spelling-checkers"], "t": ["languages"]}, {"label": "USED-FOR", "tokens": "The speed of the resulting program lies somewhere in the middle of the scale of existing << spelling-checkers >> for [[ English ]] and the main dictionary fits into the standard 360K floppy , whereas the number of recognized word forms exceeds 6 million -LRB- for Czech -RRB- .", "h": ["English"], "t": ["spelling-checkers"]}, {"label": "USED-FOR", "tokens": "Further , a special [[ method ]] has been developed for easy << word classification >> .", "h": ["method"], "t": ["word classification"]}, {"label": "EVALUATE-FOR", "tokens": "We present a new HMM tagger that exploits context on both sides of a word to be tagged , and evaluate << it >> in both the [[ unsupervised and supervised case ]] .", "h": ["unsupervised and supervised case"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "Along the way , we present the first comprehensive comparison of [[ unsupervised methods ]] for << part-of-speech tagging >> , noting that published results to date have not been comparable across corpora or lexicons .", "h": ["unsupervised methods"], "t": ["part-of-speech tagging"]}, {"label": "EVALUATE-FOR", "tokens": "Observing that the quality of the lexicon greatly impacts the [[ accuracy ]] that can be achieved by the << algorithms >> , we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable .", "h": ["accuracy"], "t": ["algorithms"]}, {"label": "EVALUATE-FOR", "tokens": "Finally , we show how this new << tagger >> achieves state-of-the-art results in a [[ supervised , non-training intensive framework ]] .", "h": ["supervised , non-training intensive framework"], "t": ["tagger"]}, {"label": "USED-FOR", "tokens": "We propose a family of [[ non-uniform sampling strategies ]] to provably speed up a class of << stochastic optimization algorithms >> with linear convergence including Stochastic Variance Reduced Gradient -LRB- SVRG -RRB- and Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB- .", "h": ["non-uniform sampling strategies"], "t": ["stochastic optimization algorithms"]}, {"label": "FEATURE-OF", "tokens": "We propose a family of non-uniform sampling strategies to provably speed up a class of << stochastic optimization algorithms >> with [[ linear convergence ]] including Stochastic Variance Reduced Gradient -LRB- SVRG -RRB- and Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB- .", "h": ["linear convergence"], "t": ["stochastic optimization algorithms"]}, {"label": "HYPONYM-OF", "tokens": "We propose a family of non-uniform sampling strategies to provably speed up a class of << stochastic optimization algorithms >> with linear convergence including [[ Stochastic Variance Reduced Gradient -LRB- SVRG -RRB- ]] and Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB- .", "h": ["Stochastic Variance Reduced Gradient -LRB- SVRG -RRB-"], "t": ["stochastic optimization algorithms"]}, {"label": "CONJUNCTION", "tokens": "We propose a family of non-uniform sampling strategies to provably speed up a class of stochastic optimization algorithms with linear convergence including [[ Stochastic Variance Reduced Gradient -LRB- SVRG -RRB- ]] and << Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB- >> .", "h": ["Stochastic Variance Reduced Gradient -LRB- SVRG -RRB-"], "t": ["Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB-"]}, {"label": "HYPONYM-OF", "tokens": "We propose a family of non-uniform sampling strategies to provably speed up a class of << stochastic optimization algorithms >> with linear convergence including Stochastic Variance Reduced Gradient -LRB- SVRG -RRB- and [[ Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB- ]] .", "h": ["Stochastic Dual Coordinate Ascent -LRB- SDCA -RRB-"], "t": ["stochastic optimization algorithms"]}, {"label": "USED-FOR", "tokens": "For a large family of << penalized empirical risk minimization problems >> , our [[ methods ]] exploit data dependent local smoothness of the loss functions near the optimum , while maintaining convergence guarantees .", "h": ["methods"], "t": ["penalized empirical risk minimization problems"]}, {"label": "USED-FOR", "tokens": "For a large family of penalized empirical risk minimization problems , our << methods >> exploit [[ data dependent local smoothness ]] of the loss functions near the optimum , while maintaining convergence guarantees .", "h": ["data dependent local smoothness"], "t": ["methods"]}, {"label": "FEATURE-OF", "tokens": "For a large family of penalized empirical risk minimization problems , our methods exploit [[ data dependent local smoothness ]] of the << loss functions >> near the optimum , while maintaining convergence guarantees .", "h": ["data dependent local smoothness"], "t": ["loss functions"]}, {"label": "USED-FOR", "tokens": "Additionally we present << algorithms >> exploiting [[ local smoothness ]] in more aggressive ways , which perform even better in practice .", "h": ["local smoothness"], "t": ["algorithms"]}, {"label": "USED-FOR", "tokens": "Statistical language modeling remains a challenging [[ task ]] , in particular for << morphologically rich languages >> .", "h": ["task"], "t": ["morphologically rich languages"]}, {"label": "USED-FOR", "tokens": "Recently , new << approaches >> based on [[ factored language models ]] have been developed to address this problem .", "h": ["factored language models"], "t": ["approaches"]}, {"label": "HYPONYM-OF", "tokens": "These models provide principled ways of including additional << conditioning variables >> other than the preceding words , such as [[ morphological or syntactic features ]] .", "h": ["morphological or syntactic features"], "t": ["conditioning variables"]}, {"label": "COMPARE", "tokens": "This paper presents an [[ entirely data-driven model selection procedure ]] based on genetic search , which is shown to outperform both << knowledge-based and random selection procedures >> on two different language modeling tasks -LRB- Arabic and Turkish -RRB- .", "h": ["entirely data-driven model selection procedure"], "t": ["knowledge-based and random selection procedures"]}, {"label": "USED-FOR", "tokens": "This paper presents an << entirely data-driven model selection procedure >> based on [[ genetic search ]] , which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks -LRB- Arabic and Turkish -RRB- .", "h": ["genetic search"], "t": ["entirely data-driven model selection procedure"]}, {"label": "USED-FOR", "tokens": "This paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both [[ knowledge-based and random selection procedures ]] on two different << language modeling tasks >> -LRB- Arabic and Turkish -RRB- .", "h": ["knowledge-based and random selection procedures"], "t": ["language modeling tasks"]}, {"label": "HYPONYM-OF", "tokens": "This paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge-based and random selection procedures on two different << language modeling tasks >> -LRB- [[ Arabic ]] and Turkish -RRB- .", "h": ["Arabic"], "t": ["language modeling tasks"]}, {"label": "CONJUNCTION", "tokens": "This paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge-based and random selection procedures on two different language modeling tasks -LRB- [[ Arabic ]] and << Turkish >> -RRB- .", "h": ["Arabic"], "t": ["Turkish"]}, {"label": "HYPONYM-OF", "tokens": "This paper presents an entirely data-driven model selection procedure based on genetic search , which is shown to outperform both knowledge-based and random selection procedures on two different << language modeling tasks >> -LRB- Arabic and [[ Turkish ]] -RRB- .", "h": ["Turkish"], "t": ["language modeling tasks"]}, {"label": "USED-FOR", "tokens": "We address appropriate [[ user modeling ]] in order to generate << cooperative responses >> to each user in spoken dialogue systems .", "h": ["user modeling"], "t": ["cooperative responses"]}, {"label": "PART-OF", "tokens": "We address appropriate [[ user modeling ]] in order to generate cooperative responses to each user in << spoken dialogue systems >> .", "h": ["user modeling"], "t": ["spoken dialogue systems"]}, {"label": "COMPARE", "tokens": "Unlike previous [[ studies ]] that focus on user 's knowledge or typical kinds of users , the << user model >> we propose is more comprehensive .", "h": ["studies"], "t": ["user model"]}, {"label": "USED-FOR", "tokens": "Moreover , the << models >> are automatically derived by [[ decision tree learning ]] using real dialogue data collected by the system .", "h": ["decision tree learning"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "Moreover , the models are automatically derived by << decision tree learning >> using [[ real dialogue data ]] collected by the system .", "h": ["real dialogue data"], "t": ["decision tree learning"]}, {"label": "USED-FOR", "tokens": "Moreover , the models are automatically derived by decision tree learning using << real dialogue data >> collected by the [[ system ]] .", "h": ["system"], "t": ["real dialogue data"]}, {"label": "USED-FOR", "tokens": "[[ Dialogue strategies ]] based on the user modeling are implemented in << Kyoto city bus information system >> that has been developed at our laboratory .", "h": ["Dialogue strategies"], "t": ["Kyoto city bus information system"]}, {"label": "USED-FOR", "tokens": "<< Dialogue strategies >> based on the [[ user modeling ]] are implemented in Kyoto city bus information system that has been developed at our laboratory .", "h": ["user modeling"], "t": ["Dialogue strategies"]}, {"label": "USED-FOR", "tokens": "This paper proposes a novel [[ method ]] of << building polarity-tagged corpus >> from HTML documents .", "h": ["method"], "t": ["building polarity-tagged corpus"]}, {"label": "USED-FOR", "tokens": "This paper proposes a novel << method >> of building polarity-tagged corpus from [[ HTML documents ]] .", "h": ["HTML documents"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The characteristics of this method is that [[ it ]] is fully automatic and can be applied to arbitrary << HTML documents >> .", "h": ["it"], "t": ["HTML documents"]}, {"label": "USED-FOR", "tokens": "The idea behind our << method >> is to utilize certain [[ layout structures ]] and linguistic pattern .", "h": ["layout structures"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "The idea behind our method is to utilize certain [[ layout structures ]] and << linguistic pattern >> .", "h": ["layout structures"], "t": ["linguistic pattern"]}, {"label": "USED-FOR", "tokens": "The idea behind our << method >> is to utilize certain layout structures and [[ linguistic pattern ]] .", "h": ["linguistic pattern"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "Previous work has used [[ monolingual parallel corpora ]] to extract and generate << paraphrases >> .", "h": ["monolingual parallel corpora"], "t": ["paraphrases"]}, {"label": "USED-FOR", "tokens": "We show that this << task >> can be done using [[ bilingual parallel corpora ]] , a much more commonly available resource .", "h": ["bilingual parallel corpora"], "t": ["task"]}, {"label": "USED-FOR", "tokens": "Using [[ alignment techniques ]] from << phrase-based statistical machine translation >> , we show how paraphrases in one language can be identified using a phrase in another language as a pivot .", "h": ["alignment techniques"], "t": ["phrase-based statistical machine translation"]}, {"label": "PART-OF", "tokens": "We define a paraphrase probability that allows [[ paraphrases ]] extracted from a << bilingual parallel corpus >> to be ranked using translation probabilities , and show how it can be refined to take contextual information into account .", "h": ["paraphrases"], "t": ["bilingual parallel corpus"]}, {"label": "USED-FOR", "tokens": "We define a paraphrase probability that allows << paraphrases >> extracted from a bilingual parallel corpus to be ranked using [[ translation probabilities ]] , and show how it can be refined to take contextual information into account .", "h": ["translation probabilities"], "t": ["paraphrases"]}, {"label": "USED-FOR", "tokens": "We define a paraphrase probability that allows paraphrases extracted from a bilingual parallel corpus to be ranked using translation probabilities , and show how << it >> can be refined to take [[ contextual information ]] into account .", "h": ["contextual information"], "t": ["it"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate our << paraphrase extraction and ranking methods >> using a set of [[ manual word alignments ]] , and contrast the quality with paraphrases extracted from automatic alignments .", "h": ["manual word alignments"], "t": ["paraphrase extraction and ranking methods"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the [[ quality ]] with << paraphrases >> extracted from automatic alignments .", "h": ["quality"], "t": ["paraphrases"]}, {"label": "PART-OF", "tokens": "We evaluate our paraphrase extraction and ranking methods using a set of manual word alignments , and contrast the quality with [[ paraphrases ]] extracted from << automatic alignments >> .", "h": ["paraphrases"], "t": ["automatic alignments"]}, {"label": "PART-OF", "tokens": "This paper proposes an automatic , essentially << domain-independent means of evaluating Spoken Language Systems -LRB- SLS -RRB- >> which combines [[ software ]] we have developed for that purpose -LRB- the '' Comparator '' -RRB- and a set of specifications for answer expressions -LRB- the '' Common Answer Specification '' , or CAS -RRB- .", "h": ["software"], "t": ["domain-independent means of evaluating Spoken Language Systems -LRB- SLS -RRB-"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes an automatic , essentially domain-independent means of evaluating Spoken Language Systems -LRB- SLS -RRB- which combines [[ software ]] we have developed for that purpose -LRB- the '' Comparator '' -RRB- and a set of << specifications >> for answer expressions -LRB- the '' Common Answer Specification '' , or CAS -RRB- .", "h": ["software"], "t": ["specifications"]}, {"label": "PART-OF", "tokens": "This paper proposes an automatic , essentially << domain-independent means of evaluating Spoken Language Systems -LRB- SLS -RRB- >> which combines software we have developed for that purpose -LRB- the '' Comparator '' -RRB- and a set of [[ specifications ]] for answer expressions -LRB- the '' Common Answer Specification '' , or CAS -RRB- .", "h": ["specifications"], "t": ["domain-independent means of evaluating Spoken Language Systems -LRB- SLS -RRB-"]}, {"label": "USED-FOR", "tokens": "This paper proposes an automatic , essentially domain-independent means of evaluating Spoken Language Systems -LRB- SLS -RRB- which combines software we have developed for that purpose -LRB- the '' Comparator '' -RRB- and a set of [[ specifications ]] for << answer expressions >> -LRB- the '' Common Answer Specification '' , or CAS -RRB- .", "h": ["specifications"], "t": ["answer expressions"]}, {"label": "USED-FOR", "tokens": "The [[ Common Answer Specification ]] determines the << syntax of answer expressions >> , the minimal content that must be included in them , the data to be included in and excluded from test corpora , and the procedures used by the Comparator .", "h": ["Common Answer Specification"], "t": ["syntax of answer expressions"]}, {"label": "USED-FOR", "tokens": "This paper describes an [[ unsupervised learning method ]] for << associative relationships between verb phrases >> , which is important in developing reliable Q&A systems .", "h": ["unsupervised learning method"], "t": ["associative relationships between verb phrases"]}, {"label": "USED-FOR", "tokens": "This paper describes an unsupervised learning method for [[ associative relationships between verb phrases ]] , which is important in developing reliable << Q&A systems >> .", "h": ["associative relationships between verb phrases"], "t": ["Q&A systems"]}, {"label": "USED-FOR", "tokens": "Our aim is to develop an [[ unsupervised learning method ]] that can obtain such an << associative relationship >> , which we call scenario consistency .", "h": ["unsupervised learning method"], "t": ["associative relationship"]}, {"label": "USED-FOR", "tokens": "The << method >> we are currently working on uses an [[ expectation-maximization -LRB- EM -RRB- based word-clustering algorithm ]] , and we have evaluated the effectiveness of this method using Japanese verb phrases .", "h": ["expectation-maximization -LRB- EM -RRB- based word-clustering algorithm"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "The method we are currently working on uses an expectation-maximization -LRB- EM -RRB- based word-clustering algorithm , and we have evaluated the effectiveness of this << method >> using [[ Japanese verb phrases ]] .", "h": ["Japanese verb phrases"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "We describe the use of [[ text data ]] scraped from the web to augment << language models >> for Automatic Speech Recognition and Keyword Search for Low Resource Languages .", "h": ["text data"], "t": ["language models"]}, {"label": "FEATURE-OF", "tokens": "We describe the use of << text data >> scraped from the [[ web ]] to augment language models for Automatic Speech Recognition and Keyword Search for Low Resource Languages .", "h": ["web"], "t": ["text data"]}, {"label": "USED-FOR", "tokens": "We describe the use of text data scraped from the web to augment [[ language models ]] for << Automatic Speech Recognition >> and Keyword Search for Low Resource Languages .", "h": ["language models"], "t": ["Automatic Speech Recognition"]}, {"label": "USED-FOR", "tokens": "We describe the use of text data scraped from the web to augment [[ language models ]] for Automatic Speech Recognition and << Keyword Search >> for Low Resource Languages .", "h": ["language models"], "t": ["Keyword Search"]}, {"label": "CONJUNCTION", "tokens": "We describe the use of text data scraped from the web to augment language models for [[ Automatic Speech Recognition ]] and << Keyword Search >> for Low Resource Languages .", "h": ["Automatic Speech Recognition"], "t": ["Keyword Search"]}, {"label": "USED-FOR", "tokens": "We describe the use of text data scraped from the web to augment language models for << Automatic Speech Recognition >> and Keyword Search for [[ Low Resource Languages ]] .", "h": ["Low Resource Languages"], "t": ["Automatic Speech Recognition"]}, {"label": "USED-FOR", "tokens": "We describe the use of text data scraped from the web to augment language models for Automatic Speech Recognition and << Keyword Search >> for [[ Low Resource Languages ]] .", "h": ["Low Resource Languages"], "t": ["Keyword Search"]}, {"label": "HYPONYM-OF", "tokens": "We scrape text from multiple << genres >> including [[ blogs ]] , online news , translated TED talks , and subtitles .", "h": ["blogs"], "t": ["genres"]}, {"label": "CONJUNCTION", "tokens": "We scrape text from multiple genres including [[ blogs ]] , << online news >> , translated TED talks , and subtitles .", "h": ["blogs"], "t": ["online news"]}, {"label": "HYPONYM-OF", "tokens": "We scrape text from multiple << genres >> including blogs , [[ online news ]] , translated TED talks , and subtitles .", "h": ["online news"], "t": ["genres"]}, {"label": "CONJUNCTION", "tokens": "We scrape text from multiple genres including blogs , [[ online news ]] , << translated TED talks >> , and subtitles .", "h": ["online news"], "t": ["translated TED talks"]}, {"label": "HYPONYM-OF", "tokens": "We scrape text from multiple << genres >> including blogs , online news , [[ translated TED talks ]] , and subtitles .", "h": ["translated TED talks"], "t": ["genres"]}, {"label": "CONJUNCTION", "tokens": "We scrape text from multiple genres including blogs , online news , [[ translated TED talks ]] , and << subtitles >> .", "h": ["translated TED talks"], "t": ["subtitles"]}, {"label": "HYPONYM-OF", "tokens": "We scrape text from multiple << genres >> including blogs , online news , translated TED talks , and [[ subtitles ]] .", "h": ["subtitles"], "t": ["genres"]}, {"label": "USED-FOR", "tokens": "Using [[ linearly interpolated language models ]] , we find that blogs and movie subtitles are more relevant for << language modeling of conversational telephone speech >> and obtain large reductions in out-of-vocabulary keywords .", "h": ["linearly interpolated language models"], "t": ["language modeling of conversational telephone speech"]}, {"label": "CONJUNCTION", "tokens": "Using linearly interpolated language models , we find that [[ blogs ]] and << movie subtitles >> are more relevant for language modeling of conversational telephone speech and obtain large reductions in out-of-vocabulary keywords .", "h": ["blogs"], "t": ["movie subtitles"]}, {"label": "USED-FOR", "tokens": "Using linearly interpolated language models , we find that [[ blogs ]] and movie subtitles are more relevant for << language modeling of conversational telephone speech >> and obtain large reductions in out-of-vocabulary keywords .", "h": ["blogs"], "t": ["language modeling of conversational telephone speech"]}, {"label": "USED-FOR", "tokens": "Using linearly interpolated language models , we find that blogs and [[ movie subtitles ]] are more relevant for << language modeling of conversational telephone speech >> and obtain large reductions in out-of-vocabulary keywords .", "h": ["movie subtitles"], "t": ["language modeling of conversational telephone speech"]}, {"label": "USED-FOR", "tokens": "Furthermore , we show that the [[ web data ]] can improve Term Error Rate Performance by 3.8 % absolute and Maximum Term-Weighted Value in << Keyword Search >> by 0.0076-0 .1059 absolute points .", "h": ["web data"], "t": ["Keyword Search"]}, {"label": "EVALUATE-FOR", "tokens": "Furthermore , we show that the web data can improve [[ Term Error Rate Performance ]] by 3.8 % absolute and Maximum Term-Weighted Value in << Keyword Search >> by 0.0076-0 .1059 absolute points .", "h": ["Term Error Rate Performance"], "t": ["Keyword Search"]}, {"label": "EVALUATE-FOR", "tokens": "Furthermore , we show that the web data can improve Term Error Rate Performance by 3.8 % absolute and [[ Maximum Term-Weighted Value ]] in << Keyword Search >> by 0.0076-0 .1059 absolute points .", "h": ["Maximum Term-Weighted Value"], "t": ["Keyword Search"]}, {"label": "USED-FOR", "tokens": "Pipelined Natural Language Generation -LRB- NLG -RRB- systems have grown increasingly complex as [[ architectural modules ]] were added to support << language functionalities >> such as referring expressions , lexical choice , and revision .", "h": ["architectural modules"], "t": ["language functionalities"]}, {"label": "HYPONYM-OF", "tokens": "Pipelined Natural Language Generation -LRB- NLG -RRB- systems have grown increasingly complex as architectural modules were added to support << language functionalities >> such as [[ referring expressions ]] , lexical choice , and revision .", "h": ["referring expressions"], "t": ["language functionalities"]}, {"label": "CONJUNCTION", "tokens": "Pipelined Natural Language Generation -LRB- NLG -RRB- systems have grown increasingly complex as architectural modules were added to support language functionalities such as [[ referring expressions ]] , << lexical choice >> , and revision .", "h": ["referring expressions"], "t": ["lexical choice"]}, {"label": "HYPONYM-OF", "tokens": "Pipelined Natural Language Generation -LRB- NLG -RRB- systems have grown increasingly complex as architectural modules were added to support << language functionalities >> such as referring expressions , [[ lexical choice ]] , and revision .", "h": ["lexical choice"], "t": ["language functionalities"]}, {"label": "CONJUNCTION", "tokens": "Pipelined Natural Language Generation -LRB- NLG -RRB- systems have grown increasingly complex as architectural modules were added to support language functionalities such as referring expressions , [[ lexical choice ]] , and << revision >> .", "h": ["lexical choice"], "t": ["revision"]}, {"label": "HYPONYM-OF", "tokens": "Pipelined Natural Language Generation -LRB- NLG -RRB- systems have grown increasingly complex as architectural modules were added to support << language functionalities >> such as referring expressions , lexical choice , and [[ revision ]] .", "h": ["revision"], "t": ["language functionalities"]}, {"label": "PART-OF", "tokens": "This has given rise to discussions about the relative placement of these new [[ modules ]] in the << overall architecture >> .", "h": ["modules"], "t": ["overall architecture"]}, {"label": "CONJUNCTION", "tokens": "We present examples which suggest that in a pipelined NLG architecture , the best approach is to strongly tie [[ it ]] to a << revision component >> .", "h": ["it"], "t": ["revision component"]}, {"label": "PART-OF", "tokens": "We present examples which suggest that in a << pipelined NLG architecture >> , the best approach is to strongly tie it to a [[ revision component ]] .", "h": ["revision component"], "t": ["pipelined NLG architecture"]}, {"label": "EVALUATE-FOR", "tokens": "Finally , we evaluate the << approach >> in a working [[ multi-page system ]] .", "h": ["multi-page system"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "In this paper a [[ system ]] which understands and conceptualizes << scenes descriptions in natural language >> is presented .", "h": ["system"], "t": ["scenes descriptions in natural language"]}, {"label": "PART-OF", "tokens": "Specifically , the following [[ components ]] of the << system >> are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .", "h": ["components"], "t": ["system"]}, {"label": "PART-OF", "tokens": "Specifically , the following << components >> of the system are described : the [[ syntactic analyzer ]] , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .", "h": ["syntactic analyzer"], "t": ["components"]}, {"label": "CONJUNCTION", "tokens": "Specifically , the following components of the system are described : the [[ syntactic analyzer ]] , based on a Procedural Systemic Grammar , the << semantic analyzer >> relying on the Conceptual Dependency Theory , and the dictionary .", "h": ["syntactic analyzer"], "t": ["semantic analyzer"]}, {"label": "USED-FOR", "tokens": "Specifically , the following components of the system are described : the << syntactic analyzer >> , based on a [[ Procedural Systemic Grammar ]] , the semantic analyzer relying on the Conceptual Dependency Theory , and the dictionary .", "h": ["Procedural Systemic Grammar"], "t": ["syntactic analyzer"]}, {"label": "PART-OF", "tokens": "Specifically , the following << components >> of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the [[ semantic analyzer ]] relying on the Conceptual Dependency Theory , and the dictionary .", "h": ["semantic analyzer"], "t": ["components"]}, {"label": "CONJUNCTION", "tokens": "Specifically , the following components of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the [[ semantic analyzer ]] relying on the Conceptual Dependency Theory , and the << dictionary >> .", "h": ["semantic analyzer"], "t": ["dictionary"]}, {"label": "USED-FOR", "tokens": "Specifically , the following components of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the << semantic analyzer >> relying on the [[ Conceptual Dependency Theory ]] , and the dictionary .", "h": ["Conceptual Dependency Theory"], "t": ["semantic analyzer"]}, {"label": "PART-OF", "tokens": "Specifically , the following << components >> of the system are described : the syntactic analyzer , based on a Procedural Systemic Grammar , the semantic analyzer relying on the Conceptual Dependency Theory , and the [[ dictionary ]] .", "h": ["dictionary"], "t": ["components"]}, {"label": "FEATURE-OF", "tokens": "The base parser produces a set of candidate parses for each input sentence , with associated probabilities that define an initial [[ ranking ]] of these << parses >> .", "h": ["ranking"], "t": ["parses"]}, {"label": "USED-FOR", "tokens": "A second [[ model ]] then attempts to improve upon this initial << ranking >> , using additional features of the tree as evidence .", "h": ["model"], "t": ["ranking"]}, {"label": "USED-FOR", "tokens": "A second << model >> then attempts to improve upon this initial ranking , using additional [[ features ]] of the tree as evidence .", "h": ["features"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "The strength of our approach is that it allows a tree to be represented as an arbitrary set of features , without concerns about how these features interact or overlap and without the need to define a derivation or a << generative model >> which takes these [[ features ]] into account .", "h": ["features"], "t": ["generative model"]}, {"label": "USED-FOR", "tokens": "We introduce a new [[ method ]] for the << reranking task >> , based on the boosting approach to ranking problems described in Freund et al. -LRB- 1998 -RRB- .", "h": ["method"], "t": ["reranking task"]}, {"label": "USED-FOR", "tokens": "We introduce a new << method >> for the reranking task , based on the [[ boosting approach ]] to ranking problems described in Freund et al. -LRB- 1998 -RRB- .", "h": ["boosting approach"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "We introduce a new method for the reranking task , based on the [[ boosting approach ]] to << ranking problems >> described in Freund et al. -LRB- 1998 -RRB- .", "h": ["boosting approach"], "t": ["ranking problems"]}, {"label": "USED-FOR", "tokens": "We apply the [[ boosting method ]] to << parsing >> the Wall Street Journal treebank .", "h": ["boosting method"], "t": ["parsing"]}, {"label": "USED-FOR", "tokens": "We apply the << boosting method >> to parsing the [[ Wall Street Journal treebank ]] .", "h": ["Wall Street Journal treebank"], "t": ["boosting method"]}, {"label": "PART-OF", "tokens": "The << method >> combined the [[ log-likelihood ]] under a baseline model -LRB- that of Collins -LSB- 1999 -RSB- -RRB- with evidence from an additional 500,000 features over parse trees that were not included in the original model .", "h": ["log-likelihood"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "The method combined the [[ log-likelihood ]] under a << baseline model >> -LRB- that of Collins -LSB- 1999 -RSB- -RRB- with evidence from an additional 500,000 features over parse trees that were not included in the original model .", "h": ["log-likelihood"], "t": ["baseline model"]}, {"label": "EVALUATE-FOR", "tokens": "The new << model >> achieved 89.75 % [[ F-measure ]] , a 13 % relative decrease in F-measure error over the baseline model 's score of 88.2 % .", "h": ["F-measure"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "The new model achieved 89.75 % F-measure , a 13 % relative decrease in [[ F-measure ]] error over the << baseline model >> 's score of 88.2 % .", "h": ["F-measure"], "t": ["baseline model"]}, {"label": "COMPARE", "tokens": "The new << model >> achieved 89.75 % F-measure , a 13 % relative decrease in F-measure error over the [[ baseline model ]] 's score of 88.2 % .", "h": ["baseline model"], "t": ["model"]}, {"label": "USED-FOR", "tokens": "The article also introduces a new [[ algorithm ]] for the << boosting approach >> which takes advantage of the sparsity of the feature space in the parsing data .", "h": ["algorithm"], "t": ["boosting approach"]}, {"label": "USED-FOR", "tokens": "The article also introduces a new << algorithm >> for the boosting approach which takes advantage of the [[ sparsity of the feature space ]] in the parsing data .", "h": ["sparsity of the feature space"], "t": ["algorithm"]}, {"label": "FEATURE-OF", "tokens": "The article also introduces a new algorithm for the boosting approach which takes advantage of the [[ sparsity of the feature space ]] in the << parsing data >> .", "h": ["sparsity of the feature space"], "t": ["parsing data"]}, {"label": "COMPARE", "tokens": "Experiments show significant efficiency gains for the new [[ algorithm ]] over the obvious implementation of the << boosting approach >> .", "h": ["algorithm"], "t": ["boosting approach"]}, {"label": "PART-OF", "tokens": "We argue that the method is an appealing alternative - in terms of both simplicity and efficiency - to work on [[ feature selection methods ]] within << log-linear -LRB- maximum-entropy -RRB- models >> .", "h": ["feature selection methods"], "t": ["log-linear -LRB- maximum-entropy -RRB- models"]}, {"label": "USED-FOR", "tokens": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other << NLP problems >> which are naturally framed as [[ ranking tasks ]] , for example , speech recognition , machine translation , or natural language generation .", "h": ["ranking tasks"], "t": ["NLP problems"]}, {"label": "HYPONYM-OF", "tokens": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other << NLP problems >> which are naturally framed as ranking tasks , for example , [[ speech recognition ]] , machine translation , or natural language generation .", "h": ["speech recognition"], "t": ["NLP problems"]}, {"label": "HYPONYM-OF", "tokens": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other NLP problems which are naturally framed as << ranking tasks >> , for example , [[ speech recognition ]] , machine translation , or natural language generation .", "h": ["speech recognition"], "t": ["ranking tasks"]}, {"label": "CONJUNCTION", "tokens": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , [[ speech recognition ]] , << machine translation >> , or natural language generation .", "h": ["speech recognition"], "t": ["machine translation"]}, {"label": "HYPONYM-OF", "tokens": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other << NLP problems >> which are naturally framed as ranking tasks , for example , speech recognition , [[ machine translation ]] , or natural language generation .", "h": ["machine translation"], "t": ["NLP problems"]}, {"label": "HYPONYM-OF", "tokens": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other NLP problems which are naturally framed as << ranking tasks >> , for example , speech recognition , [[ machine translation ]] , or natural language generation .", "h": ["machine translation"], "t": ["ranking tasks"]}, {"label": "CONJUNCTION", "tokens": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks , for example , speech recognition , [[ machine translation ]] , or << natural language generation >> .", "h": ["machine translation"], "t": ["natural language generation"]}, {"label": "HYPONYM-OF", "tokens": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other << NLP problems >> which are naturally framed as ranking tasks , for example , speech recognition , machine translation , or [[ natural language generation ]] .", "h": ["natural language generation"], "t": ["NLP problems"]}, {"label": "HYPONYM-OF", "tokens": "Although the experiments in this article are on natural language parsing -LRB- NLP -RRB- , the approach should be applicable to many other NLP problems which are naturally framed as << ranking tasks >> , for example , speech recognition , machine translation , or [[ natural language generation ]] .", "h": ["natural language generation"], "t": ["ranking tasks"]}, {"label": "USED-FOR", "tokens": "A [[ model ]] is presented to characterize the << class of languages >> obtained by adding reduplication to context-free languages .", "h": ["model"], "t": ["class of languages"]}, {"label": "USED-FOR", "tokens": "A model is presented to characterize the class of languages obtained by adding [[ reduplication ]] to << context-free languages >> .", "h": ["reduplication"], "t": ["context-free languages"]}, {"label": "HYPONYM-OF", "tokens": "The [[ model ]] is a << pushdown automaton >> augmented with the ability to check reduplication by using the stack in a new way .", "h": ["model"], "t": ["pushdown automaton"]}, {"label": "USED-FOR", "tokens": "The model is a << pushdown automaton >> augmented with the ability to check reduplication by using the [[ stack ]] in a new way .", "h": ["stack"], "t": ["pushdown automaton"]}, {"label": "USED-FOR", "tokens": "The model is a pushdown automaton augmented with the ability to check << reduplication >> by using the [[ stack ]] in a new way .", "h": ["stack"], "t": ["reduplication"]}, {"label": "CONJUNCTION", "tokens": "The class of languages generated is shown to lie strictly between the [[ context-free languages ]] and the << indexed languages >> .", "h": ["context-free languages"], "t": ["indexed languages"]}, {"label": "USED-FOR", "tokens": "The [[ model ]] appears capable of accommodating the sort of << reduplications >> that have been observed to occur in natural languages , but it excludes many of the unnatural constructions that other formal models have permitted .", "h": ["model"], "t": ["reduplications"]}, {"label": "USED-FOR", "tokens": "We present an << image set classification algorithm >> based on [[ unsupervised clustering ]] of labeled training and unla-beled test data where labels are only used in the stopping criterion .", "h": ["unsupervised clustering"], "t": ["image set classification algorithm"]}, {"label": "USED-FOR", "tokens": "We present an image set classification algorithm based on << unsupervised clustering >> of [[ labeled training and unla-beled test data ]] where labels are only used in the stopping criterion .", "h": ["labeled training and unla-beled test data"], "t": ["unsupervised clustering"]}, {"label": "USED-FOR", "tokens": "The [[ probability distribution ]] of each class over the set of clusters is used to define a true << set based similarity measure >> .", "h": ["probability distribution"], "t": ["set based similarity measure"]}, {"label": "USED-FOR", "tokens": "In each iteration , a [[ proximity matrix ]] is efficiently recomputed to better represent the << local subspace structure >> .", "h": ["proximity matrix"], "t": ["local subspace structure"]}, {"label": "USED-FOR", "tokens": "[[ Initial clusters ]] capture the << global data structure >> and finer clusters at the later stages capture the subtle class differences not visible at the global scale .", "h": ["Initial clusters"], "t": ["global data structure"]}, {"label": "USED-FOR", "tokens": "Initial clusters capture the global data structure and [[ finer clusters ]] at the later stages capture the << subtle class differences >> not visible at the global scale .", "h": ["finer clusters"], "t": ["subtle class differences"]}, {"label": "USED-FOR", "tokens": "<< Image sets >> are compactly represented with multiple [[ Grass-mannian manifolds ]] which are subsequently embedded in Euclidean space with the proposed spectral clustering algorithm .", "h": ["Grass-mannian manifolds"], "t": ["Image sets"]}, {"label": "USED-FOR", "tokens": "Image sets are compactly represented with multiple Grass-mannian manifolds which are subsequently embedded in << Euclidean space >> with the proposed [[ spectral clustering algorithm ]] .", "h": ["spectral clustering algorithm"], "t": ["Euclidean space"]}, {"label": "USED-FOR", "tokens": "We also propose an efficient [[ eigenvector solver ]] which not only reduces the computational cost of << spectral clustering >> by many folds but also improves the clustering quality and final classification results .", "h": ["eigenvector solver"], "t": ["spectral clustering"]}, {"label": "EVALUATE-FOR", "tokens": "We also propose an efficient eigenvector solver which not only reduces the [[ computational cost ]] of << spectral clustering >> by many folds but also improves the clustering quality and final classification results .", "h": ["computational cost"], "t": ["spectral clustering"]}, {"label": "EVALUATE-FOR", "tokens": "We also propose an efficient eigenvector solver which not only reduces the computational cost of << spectral clustering >> by many folds but also improves the [[ clustering quality ]] and final classification results .", "h": ["clustering quality"], "t": ["spectral clustering"]}, {"label": "EVALUATE-FOR", "tokens": "We also propose an efficient eigenvector solver which not only reduces the computational cost of << spectral clustering >> by many folds but also improves the clustering quality and final [[ classification results ]] .", "h": ["classification results"], "t": ["spectral clustering"]}, {"label": "FEATURE-OF", "tokens": "This paper investigates some [[ computational problems ]] associated with << probabilistic translation models >> that have recently been adopted in the literature on machine translation .", "h": ["computational problems"], "t": ["probabilistic translation models"]}, {"label": "USED-FOR", "tokens": "This paper investigates some computational problems associated with [[ probabilistic translation models ]] that have recently been adopted in the literature on << machine translation >> .", "h": ["probabilistic translation models"], "t": ["machine translation"]}, {"label": "FEATURE-OF", "tokens": "These << models >> can be viewed as pairs of [[ probabilistic context-free grammars ]] working in a ` synchronous ' way .", "h": ["probabilistic context-free grammars"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "[[ Active shape models ]] are a powerful and widely used tool to interpret << complex image data >> .", "h": ["Active shape models"], "t": ["complex image data"]}, {"label": "USED-FOR", "tokens": "By building << models of shape variation >> they enable [[ search algorithms ]] to use a pri-ori knowledge in an efficient and gainful way .", "h": ["search algorithms"], "t": ["models of shape variation"]}, {"label": "USED-FOR", "tokens": "By building models of shape variation they enable << search algorithms >> to use a [[ pri-ori knowledge ]] in an efficient and gainful way .", "h": ["pri-ori knowledge"], "t": ["search algorithms"]}, {"label": "FEATURE-OF", "tokens": "However , due to the [[ linearity ]] of << PCA >> , non-linearities like rotations or independently moving sub-parts in the data can deteriorate the resulting model considerably .", "h": ["linearity"], "t": ["PCA"]}, {"label": "HYPONYM-OF", "tokens": "However , due to the linearity of PCA , << non-linearities >> like [[ rotations ]] or independently moving sub-parts in the data can deteriorate the resulting model considerably .", "h": ["rotations"], "t": ["non-linearities"]}, {"label": "USED-FOR", "tokens": "Although << non-linear extensions of active shape models >> have been proposed and application specific solutions have been used , they still need a certain amount of [[ user interaction ]] during model building .", "h": ["user interaction"], "t": ["non-linear extensions of active shape models"]}, {"label": "USED-FOR", "tokens": "In particular , we propose an << algorithm >> based on the [[ minimum description length principle ]] to find an optimal subdivision of the data into sub-parts , each adequate for linear modeling .", "h": ["minimum description length principle"], "t": ["algorithm"]}, {"label": "FEATURE-OF", "tokens": "Which in turn leads to a better << model >> in terms of [[ modes of variations ]] .", "h": ["modes of variations"], "t": ["model"]}, {"label": "EVALUATE-FOR", "tokens": "The proposed << method >> is evaluated on [[ synthetic data ]] , medical images and hand contours .", "h": ["synthetic data"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "The proposed method is evaluated on [[ synthetic data ]] , << medical images >> and hand contours .", "h": ["synthetic data"], "t": ["medical images"]}, {"label": "EVALUATE-FOR", "tokens": "The proposed << method >> is evaluated on synthetic data , [[ medical images ]] and hand contours .", "h": ["medical images"], "t": ["method"]}, {"label": "CONJUNCTION", "tokens": "The proposed method is evaluated on synthetic data , [[ medical images ]] and << hand contours >> .", "h": ["medical images"], "t": ["hand contours"]}, {"label": "EVALUATE-FOR", "tokens": "The proposed << method >> is evaluated on synthetic data , medical images and [[ hand contours ]] .", "h": ["hand contours"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "We describe a set of experiments to explore [[ statistical techniques ]] for << ranking >> and selecting the best translations in a graph of translation hypotheses .", "h": ["statistical techniques"], "t": ["ranking"]}, {"label": "USED-FOR", "tokens": "In a previous paper -LRB- Carl , 2007 -RRB- we have described how the << hypotheses graph >> is generated through [[ shallow mapping ]] and permutation rules .", "h": ["shallow mapping"], "t": ["hypotheses graph"]}, {"label": "CONJUNCTION", "tokens": "In a previous paper -LRB- Carl , 2007 -RRB- we have described how the hypotheses graph is generated through [[ shallow mapping ]] and << permutation rules >> .", "h": ["shallow mapping"], "t": ["permutation rules"]}, {"label": "USED-FOR", "tokens": "In a previous paper -LRB- Carl , 2007 -RRB- we have described how the << hypotheses graph >> is generated through shallow mapping and [[ permutation rules ]] .", "h": ["permutation rules"], "t": ["hypotheses graph"]}, {"label": "USED-FOR", "tokens": "This paper describes a number of [[ methods ]] for elaborating << statistical feature functions >> from some of the vector components .", "h": ["methods"], "t": ["statistical feature functions"]}, {"label": "USED-FOR", "tokens": "This paper describes a number of << methods >> for elaborating statistical feature functions from some of the [[ vector components ]] .", "h": ["vector components"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "The feature functions are trained off-line on different types of text and their [[ log-linear combination ]] is then used to retrieve the best M << translation paths >> in the graph .", "h": ["log-linear combination"], "t": ["translation paths"]}, {"label": "PART-OF", "tokens": "The feature functions are trained off-line on different types of text and their log-linear combination is then used to retrieve the best M [[ translation paths ]] in the << graph >> .", "h": ["translation paths"], "t": ["graph"]}, {"label": "HYPONYM-OF", "tokens": "We compare two << language modelling toolkits >> , the [[ CMU and the SRI toolkit ]] and arrive at three results : 1 -RRB- word-lemma based feature function models produce better results than token-based models , 2 -RRB- adding a PoS-tag feature function to the word-lemma model improves the output and 3 -RRB- weights for lexical translations are suitable if the training material is similar to the texts to be translated .", "h": ["CMU and the SRI toolkit"], "t": ["language modelling toolkits"]}, {"label": "COMPARE", "tokens": "We compare two language modelling toolkits , the CMU and the SRI toolkit and arrive at three results : 1 -RRB- [[ word-lemma based feature function models ]] produce better results than << token-based models >> , 2 -RRB- adding a PoS-tag feature function to the word-lemma model improves the output and 3 -RRB- weights for lexical translations are suitable if the training material is similar to the texts to be translated .", "h": ["word-lemma based feature function models"], "t": ["token-based models"]}, {"label": "PART-OF", "tokens": "We compare two language modelling toolkits , the CMU and the SRI toolkit and arrive at three results : 1 -RRB- word-lemma based feature function models produce better results than token-based models , 2 -RRB- adding a [[ PoS-tag feature function ]] to the << word-lemma model >> improves the output and 3 -RRB- weights for lexical translations are suitable if the training material is similar to the texts to be translated .", "h": ["PoS-tag feature function"], "t": ["word-lemma model"]}, {"label": "USED-FOR", "tokens": "This paper presents a specialized << editor >> for a highly [[ structured dictionary ]] .", "h": ["structured dictionary"], "t": ["editor"]}, {"label": "USED-FOR", "tokens": "The basic goal in building that [[ editor ]] was to provide an adequate tool to help lexicologists produce a valid and coherent << dictionary >> on the basis of a linguistic theory .", "h": ["editor"], "t": ["dictionary"]}, {"label": "USED-FOR", "tokens": "The basic goal in building that editor was to provide an adequate tool to help lexicologists produce a valid and coherent << dictionary >> on the basis of a [[ linguistic theory ]] .", "h": ["linguistic theory"], "t": ["dictionary"]}, {"label": "FEATURE-OF", "tokens": "Existing techniques extract term candidates by looking for << internal and contextual information >> associated with [[ domain specific terms ]] .", "h": ["domain specific terms"], "t": ["internal and contextual information"]}, {"label": "USED-FOR", "tokens": "This paper presents a novel [[ approach ]] for << term extraction >> based on delimiters which are much more stable and domain independent .", "h": ["approach"], "t": ["term extraction"]}, {"label": "USED-FOR", "tokens": "This paper presents a novel << approach >> for term extraction based on [[ delimiters ]] which are much more stable and domain independent .", "h": ["delimiters"], "t": ["approach"]}, {"label": "COMPARE", "tokens": "The proposed [[ approach ]] is not as sensitive to << term frequency >> as that of previous works .", "h": ["approach"], "t": ["term frequency"]}, {"label": "USED-FOR", "tokens": "Consequently , the proposed approach can be applied to different domains easily and [[ it ]] is especially useful for << resource-limited domains >> .", "h": ["it"], "t": ["resource-limited domains"]}, {"label": "EVALUATE-FOR", "tokens": "[[ Evaluations ]] conducted on two different domains for << Chinese term extraction >> show significant improvements over existing techniques which verifies its efficiency and domain independent nature .", "h": ["Evaluations"], "t": ["Chinese term extraction"]}, {"label": "USED-FOR", "tokens": "Experiments on new term extraction indicate that the proposed [[ approach ]] can also serve as an effective tool for << domain lexicon expansion >> .", "h": ["approach"], "t": ["domain lexicon expansion"]}, {"label": "USED-FOR", "tokens": "We describe a [[ method ]] for identifying << systematic patterns in translation data >> using part-of-speech tag sequences .", "h": ["method"], "t": ["systematic patterns in translation data"]}, {"label": "USED-FOR", "tokens": "We describe a << method >> for identifying systematic patterns in translation data using [[ part-of-speech tag sequences ]] .", "h": ["part-of-speech tag sequences"], "t": ["method"]}, {"label": "PART-OF", "tokens": "We incorporate this [[ analysis ]] into a << diagnostic tool >> intended for developers of machine translation systems , and demonstrate how our application can be used by developers to explore patterns in machine translation output .", "h": ["analysis"], "t": ["diagnostic tool"]}, {"label": "USED-FOR", "tokens": "We incorporate this analysis into a [[ diagnostic tool ]] intended for developers of << machine translation systems >> , and demonstrate how our application can be used by developers to explore patterns in machine translation output .", "h": ["diagnostic tool"], "t": ["machine translation systems"]}, {"label": "USED-FOR", "tokens": "We incorporate this analysis into a diagnostic tool intended for developers of machine translation systems , and demonstrate how our [[ application ]] can be used by developers to explore << patterns in machine translation output >> .", "h": ["application"], "t": ["patterns in machine translation output"]}, {"label": "USED-FOR", "tokens": "We study the [[ number of hidden layers ]] required by a << multilayer neu-ral network >> with threshold units to compute a function f from n d to -LCB- O , I -RCB- .", "h": ["number of hidden layers"], "t": ["multilayer neu-ral network"]}, {"label": "USED-FOR", "tokens": "We study the << number of hidden layers >> required by a multilayer neu-ral network with [[ threshold units ]] to compute a function f from n d to -LCB- O , I -RCB- .", "h": ["threshold units"], "t": ["number of hidden layers"]}, {"label": "HYPONYM-OF", "tokens": "We show that adding these conditions to Gib-son 's assumptions is not sufficient to ensure global computability with one hidden layer , by exhibiting a new << non-local configuration >> , the [[ `` critical cycle '' ]] , which implies that f is not computable with one hidden layer .", "h": ["`` critical cycle ''"], "t": ["non-local configuration"]}, {"label": "USED-FOR", "tokens": "This paper presents an [[ approach ]] to estimate the << intrinsic texture properties -LRB- albedo , shading , normal -RRB- of scenes >> from multiple view acquisition under unknown illumination conditions .", "h": ["approach"], "t": ["intrinsic texture properties -LRB- albedo , shading , normal -RRB- of scenes"]}, {"label": "USED-FOR", "tokens": "This paper presents an approach to estimate the << intrinsic texture properties -LRB- albedo , shading , normal -RRB- of scenes >> from [[ multiple view acquisition ]] under unknown illumination conditions .", "h": ["multiple view acquisition"], "t": ["intrinsic texture properties -LRB- albedo , shading , normal -RRB- of scenes"]}, {"label": "FEATURE-OF", "tokens": "This paper presents an approach to estimate the intrinsic texture properties -LRB- albedo , shading , normal -RRB- of scenes from << multiple view acquisition >> under [[ unknown illumination conditions ]] .", "h": ["unknown illumination conditions"], "t": ["multiple view acquisition"]}, {"label": "COMPARE", "tokens": "Unlike previous << video relighting methods >> , the [[ approach ]] does not assume regions of uniform albedo , which makes it applicable to richly textured scenes .", "h": ["approach"], "t": ["video relighting methods"]}, {"label": "USED-FOR", "tokens": "Unlike previous video relighting methods , the approach does not assume regions of uniform albedo , which makes [[ it ]] applicable to << richly textured scenes >> .", "h": ["it"], "t": ["richly textured scenes"]}, {"label": "USED-FOR", "tokens": "We show that [[ intrinsic image methods ]] can be used to refine an << initial , low-frequency shading estimate >> based on a global lighting reconstruction from an original texture and coarse scene geometry in order to resolve the inherent global ambiguity in shading .", "h": ["intrinsic image methods"], "t": ["initial , low-frequency shading estimate"]}, {"label": "USED-FOR", "tokens": "We show that intrinsic image methods can be used to refine an [[ initial , low-frequency shading estimate ]] based on a global lighting reconstruction from an original texture and coarse scene geometry in order to resolve the << inherent global ambiguity in shading >> .", "h": ["initial , low-frequency shading estimate"], "t": ["inherent global ambiguity in shading"]}, {"label": "USED-FOR", "tokens": "We show that << intrinsic image methods >> can be used to refine an initial , low-frequency shading estimate based on a [[ global lighting reconstruction ]] from an original texture and coarse scene geometry in order to resolve the inherent global ambiguity in shading .", "h": ["global lighting reconstruction"], "t": ["intrinsic image methods"]}, {"label": "FEATURE-OF", "tokens": "We show that intrinsic image methods can be used to refine an initial , low-frequency shading estimate based on a << global lighting reconstruction >> from an original [[ texture and coarse scene geometry ]] in order to resolve the inherent global ambiguity in shading .", "h": ["texture and coarse scene geometry"], "t": ["global lighting reconstruction"]}, {"label": "USED-FOR", "tokens": "The [[ method ]] is applied to << relight-ing of free-viewpoint rendering >> from multiple view video capture .", "h": ["method"], "t": ["relight-ing of free-viewpoint rendering"]}, {"label": "USED-FOR", "tokens": "The method is applied to << relight-ing of free-viewpoint rendering >> from [[ multiple view video capture ]] .", "h": ["multiple view video capture"], "t": ["relight-ing of free-viewpoint rendering"]}, {"label": "FEATURE-OF", "tokens": "This demonstrates << relighting >> with [[ reproduction of fine surface detail ]] .", "h": ["reproduction of fine surface detail"], "t": ["relighting"]}, {"label": "EVALUATE-FOR", "tokens": "Following recent developments in the [[ automatic evaluation ]] of << machine translation >> and document summarization , we present a similar approach , implemented in a measure called POURPRE , for automatically evaluating answers to definition questions .", "h": ["automatic evaluation"], "t": ["machine translation"]}, {"label": "EVALUATE-FOR", "tokens": "Following recent developments in the [[ automatic evaluation ]] of machine translation and << document summarization >> , we present a similar approach , implemented in a measure called POURPRE , for automatically evaluating answers to definition questions .", "h": ["automatic evaluation"], "t": ["document summarization"]}, {"label": "CONJUNCTION", "tokens": "Following recent developments in the automatic evaluation of [[ machine translation ]] and << document summarization >> , we present a similar approach , implemented in a measure called POURPRE , for automatically evaluating answers to definition questions .", "h": ["machine translation"], "t": ["document summarization"]}, {"label": "USED-FOR", "tokens": "Following recent developments in the automatic evaluation of machine translation and document summarization , we present a similar << approach >> , implemented in a [[ measure ]] called POURPRE , for automatically evaluating answers to definition questions .", "h": ["measure"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "Following recent developments in the automatic evaluation of machine translation and document summarization , we present a similar approach , implemented in a [[ measure ]] called POURPRE , for << automatically evaluating answers to definition questions >> .", "h": ["measure"], "t": ["automatically evaluating answers to definition questions"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments with the [[ TREC 2003 and TREC 2004 QA tracks ]] indicate that rankings produced by our << metric >> correlate highly with official rankings , and that POURPRE outperforms direct application of existing metrics .", "h": ["TREC 2003 and TREC 2004 QA tracks"], "t": ["metric"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments with the [[ TREC 2003 and TREC 2004 QA tracks ]] indicate that rankings produced by our metric correlate highly with official rankings , and that << POURPRE >> outperforms direct application of existing metrics .", "h": ["TREC 2003 and TREC 2004 QA tracks"], "t": ["POURPRE"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments with the [[ TREC 2003 and TREC 2004 QA tracks ]] indicate that rankings produced by our metric correlate highly with official rankings , and that POURPRE outperforms direct application of existing << metrics >> .", "h": ["TREC 2003 and TREC 2004 QA tracks"], "t": ["metrics"]}, {"label": "USED-FOR", "tokens": "Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that << rankings >> produced by our [[ metric ]] correlate highly with official rankings , and that POURPRE outperforms direct application of existing metrics .", "h": ["metric"], "t": ["rankings"]}, {"label": "COMPARE", "tokens": "Experiments with the TREC 2003 and TREC 2004 QA tracks indicate that rankings produced by our metric correlate highly with official rankings , and that [[ POURPRE ]] outperforms direct application of existing << metrics >> .", "h": ["POURPRE"], "t": ["metrics"]}, {"label": "USED-FOR", "tokens": "Recent advances in [[ Automatic Speech Recognition technology ]] have put the goal of naturally sounding << dialog systems >> within reach .", "h": ["Automatic Speech Recognition technology"], "t": ["dialog systems"]}, {"label": "PART-OF", "tokens": "The issue of [[ system response ]] to users has been extensively studied by the << natural language generation community >> , though rarely in the context of dialog systems .", "h": ["system response"], "t": ["natural language generation community"]}, {"label": "COMPARE", "tokens": "The issue of system response to users has been extensively studied by the [[ natural language generation community ]] , though rarely in the context of << dialog systems >> .", "h": ["natural language generation community"], "t": ["dialog systems"]}, {"label": "USED-FOR", "tokens": "We show how research in [[ generation ]] can be adapted to << dialog systems >> , and how the high cost of hand-crafting knowledge-based generation systems can be overcome by employing machine learning techniques .", "h": ["generation"], "t": ["dialog systems"]}, {"label": "USED-FOR", "tokens": "We show how research in generation can be adapted to dialog systems , and how the high cost of << hand-crafting knowledge-based generation systems >> can be overcome by employing [[ machine learning techniques ]] .", "h": ["machine learning techniques"], "t": ["hand-crafting knowledge-based generation systems"]}, {"label": "USED-FOR", "tokens": "We present a << tool >> , called ILIMP , which takes as input a [[ raw text in French ]] and produces as output the same text in which every occurrence of the pronoun il is tagged either with tag -LSB- ANA -RSB- for anaphoric or -LSB- IMP -RSB- for impersonal or expletive .", "h": ["raw text in French"], "t": ["tool"]}, {"label": "USED-FOR", "tokens": "This [[ tool ]] is therefore designed to distinguish between the << anaphoric occurrences of il >> , for which an anaphora resolution system has to look for an antecedent , and the expletive occurrences of this pronoun , for which it does not make sense to look for an antecedent .", "h": ["tool"], "t": ["anaphoric occurrences of il"]}, {"label": "USED-FOR", "tokens": "This tool is therefore designed to distinguish between the << anaphoric occurrences of il >> , for which an [[ anaphora resolution system ]] has to look for an antecedent , and the expletive occurrences of this pronoun , for which it does not make sense to look for an antecedent .", "h": ["anaphora resolution system"], "t": ["anaphoric occurrences of il"]}, {"label": "EVALUATE-FOR", "tokens": "The [[ precision rate ]] for << ILIMP >> is 97,5 % .", "h": ["precision rate"], "t": ["ILIMP"]}, {"label": "USED-FOR", "tokens": "Other << tasks >> using the [[ method ]] developed for ILIMP are described briefly , as well as the use of ILIMP in a modular syntactic analysis system .", "h": ["method"], "t": ["tasks"]}, {"label": "USED-FOR", "tokens": "Other tasks using the [[ method ]] developed for << ILIMP >> are described briefly , as well as the use of ILIMP in a modular syntactic analysis system .", "h": ["method"], "t": ["ILIMP"]}, {"label": "USED-FOR", "tokens": "Other tasks using the method developed for ILIMP are described briefly , as well as the use of [[ ILIMP ]] in a << modular syntactic analysis system >> .", "h": ["ILIMP"], "t": ["modular syntactic analysis system"]}, {"label": "FEATURE-OF", "tokens": "Little is thus known about the << robustness >> of [[ speech cues ]] in the wild .", "h": ["speech cues"], "t": ["robustness"]}, {"label": "CONJUNCTION", "tokens": "This study compares the effect of [[ noise ]] and << reverberation >> on depression prediction using 1 -RRB- standard mel-frequency cepstral coefficients -LRB- MFCCs -RRB- , and 2 -RRB- features designed for noise robustness , damped oscillator cepstral coefficients -LRB- DOCCs -RRB- .", "h": ["noise"], "t": ["reverberation"]}, {"label": "FEATURE-OF", "tokens": "This study compares the effect of [[ noise ]] and reverberation on << depression prediction >> using 1 -RRB- standard mel-frequency cepstral coefficients -LRB- MFCCs -RRB- , and 2 -RRB- features designed for noise robustness , damped oscillator cepstral coefficients -LRB- DOCCs -RRB- .", "h": ["noise"], "t": ["depression prediction"]}, {"label": "FEATURE-OF", "tokens": "This study compares the effect of noise and [[ reverberation ]] on << depression prediction >> using 1 -RRB- standard mel-frequency cepstral coefficients -LRB- MFCCs -RRB- , and 2 -RRB- features designed for noise robustness , damped oscillator cepstral coefficients -LRB- DOCCs -RRB- .", "h": ["reverberation"], "t": ["depression prediction"]}, {"label": "USED-FOR", "tokens": "This study compares the effect of noise and reverberation on << depression prediction >> using 1 -RRB- standard [[ mel-frequency cepstral coefficients -LRB- MFCCs -RRB- ]] , and 2 -RRB- features designed for noise robustness , damped oscillator cepstral coefficients -LRB- DOCCs -RRB- .", "h": ["mel-frequency cepstral coefficients -LRB- MFCCs -RRB-"], "t": ["depression prediction"]}, {"label": "CONJUNCTION", "tokens": "This study compares the effect of noise and reverberation on depression prediction using 1 -RRB- standard [[ mel-frequency cepstral coefficients -LRB- MFCCs -RRB- ]] , and 2 -RRB- << features >> designed for noise robustness , damped oscillator cepstral coefficients -LRB- DOCCs -RRB- .", "h": ["mel-frequency cepstral coefficients -LRB- MFCCs -RRB-"], "t": ["features"]}, {"label": "USED-FOR", "tokens": "This study compares the effect of noise and reverberation on depression prediction using 1 -RRB- standard mel-frequency cepstral coefficients -LRB- MFCCs -RRB- , and 2 -RRB- [[ features ]] designed for << noise robustness >> , damped oscillator cepstral coefficients -LRB- DOCCs -RRB- .", "h": ["features"], "t": ["noise robustness"]}, {"label": "CONJUNCTION", "tokens": "This study compares the effect of noise and reverberation on depression prediction using 1 -RRB- standard mel-frequency cepstral coefficients -LRB- MFCCs -RRB- , and 2 -RRB- [[ features ]] designed for noise robustness , << damped oscillator cepstral coefficients -LRB- DOCCs -RRB- >> .", "h": ["features"], "t": ["damped oscillator cepstral coefficients -LRB- DOCCs -RRB-"]}, {"label": "CONJUNCTION", "tokens": "Results using [[ additive noise ]] and << reverberation >> reveal a consistent pattern of findings for multiple evaluation metrics under both matched and mismatched conditions .", "h": ["additive noise"], "t": ["reverberation"]}, {"label": "CONJUNCTION", "tokens": "First and most notably : standard MFCC features suffer dramatically under test/train mismatch for both [[ noise ]] and << reverberation >> ; DOCC features are far more robust .", "h": ["noise"], "t": ["reverberation"]}, {"label": "COMPARE", "tokens": "First and most notably : standard << MFCC features >> suffer dramatically under test/train mismatch for both noise and reverberation ; [[ DOCC features ]] are far more robust .", "h": ["DOCC features"], "t": ["MFCC features"]}, {"label": "COMPARE", "tokens": "Third , [[ artificial neural networks ]] tend to outperform << support vector regression >> .", "h": ["artificial neural networks"], "t": ["support vector regression"]}, {"label": "COMPARE", "tokens": "Fourth , [[ spontaneous speech ]] appears to offer better robustness than << read speech >> .", "h": ["spontaneous speech"], "t": ["read speech"]}, {"label": "EVALUATE-FOR", "tokens": "Finally , a [[ cross-corpus -LRB- and cross-language -RRB- experiment ]] reveals better noise and reverberation robustness for << DOCCs >> than for MFCCs .", "h": ["cross-corpus -LRB- and cross-language -RRB- experiment"], "t": ["DOCCs"]}, {"label": "EVALUATE-FOR", "tokens": "Finally , a [[ cross-corpus -LRB- and cross-language -RRB- experiment ]] reveals better noise and reverberation robustness for DOCCs than for << MFCCs >> .", "h": ["cross-corpus -LRB- and cross-language -RRB- experiment"], "t": ["MFCCs"]}, {"label": "EVALUATE-FOR", "tokens": "Finally , a cross-corpus -LRB- and cross-language -RRB- experiment reveals better [[ noise and reverberation robustness ]] for << DOCCs >> than for MFCCs .", "h": ["noise and reverberation robustness"], "t": ["DOCCs"]}, {"label": "EVALUATE-FOR", "tokens": "Finally , a cross-corpus -LRB- and cross-language -RRB- experiment reveals better [[ noise and reverberation robustness ]] for DOCCs than for << MFCCs >> .", "h": ["noise and reverberation robustness"], "t": ["MFCCs"]}, {"label": "COMPARE", "tokens": "Finally , a cross-corpus -LRB- and cross-language -RRB- experiment reveals better noise and reverberation robustness for [[ DOCCs ]] than for << MFCCs >> .", "h": ["DOCCs"], "t": ["MFCCs"]}, {"label": "USED-FOR", "tokens": "This paper proposes [[ document oriented preference sets -LRB- DoPS -RRB- ]] for the << disambiguation of the dependency structure >> of sentences .", "h": ["document oriented preference sets -LRB- DoPS -RRB-"], "t": ["disambiguation of the dependency structure"]}, {"label": "USED-FOR", "tokens": "<< Sentence ambiguities >> can be resolved by using [[ domain targeted preference knowledge ]] without using complicated large knowledgebases .", "h": ["domain targeted preference knowledge"], "t": ["Sentence ambiguities"]}, {"label": "COMPARE", "tokens": "Sentence ambiguities can be resolved by using [[ domain targeted preference knowledge ]] without using complicated large << knowledgebases >> .", "h": ["domain targeted preference knowledge"], "t": ["knowledgebases"]}, {"label": "FEATURE-OF", "tokens": "Implementation and empirical results are described for the the analysis of [[ dependency structures ]] of << Japanese patent claim sentences >> .", "h": ["dependency structures"], "t": ["Japanese patent claim sentences"]}, {"label": "USED-FOR", "tokens": "<< Multimodal interfaces >> require effective [[ parsing ]] and understanding of utterances whose content is distributed across multiple input modes .", "h": ["parsing"], "t": ["Multimodal interfaces"]}, {"label": "USED-FOR", "tokens": "Johnston 1998 presents an [[ approach ]] in which strategies for << multimodal integration >> are stated declaratively using a unification-based grammar that is used by a multidimensional chart parser to compose inputs .", "h": ["approach"], "t": ["multimodal integration"]}, {"label": "USED-FOR", "tokens": "Johnston 1998 presents an approach in which strategies for << multimodal integration >> are stated declaratively using a [[ unification-based grammar ]] that is used by a multidimensional chart parser to compose inputs .", "h": ["unification-based grammar"], "t": ["multimodal integration"]}, {"label": "USED-FOR", "tokens": "Johnston 1998 presents an approach in which strategies for multimodal integration are stated declaratively using a [[ unification-based grammar ]] that is used by a << multidimensional chart parser >> to compose inputs .", "h": ["unification-based grammar"], "t": ["multidimensional chart parser"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an alternative [[ approach ]] in which << multimodal parsing and understanding >> are achieved using a weighted finite-state device which takes speech and gesture streams as inputs and outputs their joint interpretation .", "h": ["approach"], "t": ["multimodal parsing and understanding"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an alternative approach in which << multimodal parsing and understanding >> are achieved using a [[ weighted finite-state device ]] which takes speech and gesture streams as inputs and outputs their joint interpretation .", "h": ["weighted finite-state device"], "t": ["multimodal parsing and understanding"]}, {"label": "USED-FOR", "tokens": "In this paper , we present an alternative approach in which multimodal parsing and understanding are achieved using a << weighted finite-state device >> which takes [[ speech and gesture streams ]] as inputs and outputs their joint interpretation .", "h": ["speech and gesture streams"], "t": ["weighted finite-state device"]}, {"label": "USED-FOR", "tokens": "This [[ approach ]] is significantly more efficient , enables tight-coupling of multimodal understanding with speech recognition , and provides a general probabilistic framework for << multimodal ambiguity resolution >> .", "h": ["approach"], "t": ["multimodal ambiguity resolution"]}, {"label": "CONJUNCTION", "tokens": "This approach is significantly more efficient , enables tight-coupling of << multimodal understanding >> with [[ speech recognition ]] , and provides a general probabilistic framework for multimodal ambiguity resolution .", "h": ["speech recognition"], "t": ["multimodal understanding"]}, {"label": "HYPONYM-OF", "tokens": "Recently , we initiated a project to develop a << phonetically-based spoken language understanding system >> called [[ SUMMIT ]] .", "h": ["SUMMIT"], "t": ["phonetically-based spoken language understanding system"]}, {"label": "USED-FOR", "tokens": "In contrast to many of the past efforts that make use of << heuristic rules >> whose development requires intense [[ knowledge engineering ]] , our approach attempts to express the speech knowledge within a formal framework using well-defined mathematical tools .", "h": ["knowledge engineering"], "t": ["heuristic rules"]}, {"label": "USED-FOR", "tokens": "In contrast to many of the past efforts that make use of heuristic rules whose development requires intense knowledge engineering , our [[ approach ]] attempts to express the << speech knowledge >> within a formal framework using well-defined mathematical tools .", "h": ["approach"], "t": ["speech knowledge"]}, {"label": "USED-FOR", "tokens": "In contrast to many of the past efforts that make use of heuristic rules whose development requires intense knowledge engineering , our approach attempts to express the << speech knowledge >> within a formal framework using well-defined [[ mathematical tools ]] .", "h": ["mathematical tools"], "t": ["speech knowledge"]}, {"label": "CONJUNCTION", "tokens": "In our system , [[ features ]] and << decision strategies >> are discovered and trained automatically , using a large body of speech data .", "h": ["features"], "t": ["decision strategies"]}, {"label": "USED-FOR", "tokens": "In our system , features and << decision strategies >> are discovered and trained automatically , using a large body of [[ speech data ]] .", "h": ["speech data"], "t": ["decision strategies"]}, {"label": "EVALUATE-FOR", "tokens": "This paper describes an implemented << program >> that takes a [[ tagged text corpus ]] and generates a partial list of the subcategorization frames in which each verb occurs .", "h": ["tagged text corpus"], "t": ["program"]}, {"label": "USED-FOR", "tokens": "We present a [[ method ]] for estimating the << relative pose of two calibrated or uncalibrated non-overlapping surveillance cameras >> from observing a moving object .", "h": ["method"], "t": ["relative pose of two calibrated or uncalibrated non-overlapping surveillance cameras"]}, {"label": "USED-FOR", "tokens": "We show how to tackle the problem of << missing point correspondences >> heavily required by [[ SfM pipelines ]] and how to go beyond this basic paradigm .", "h": ["SfM pipelines"], "t": ["missing point correspondences"]}, {"label": "FEATURE-OF", "tokens": "We relax the [[ non-linear nature ]] of the << problem >> by accepting two assumptions which surveillance scenarios offer , ie .", "h": ["non-linear nature"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "By those assumptions we cast the << problem >> as a [[ Quadratic Eigenvalue Problem ]] offering an elegant way of treating nonlinear monomials and delivering a quasi closed-form solution as a reliable starting point for a further bundle adjustment .", "h": ["Quadratic Eigenvalue Problem"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "By those assumptions we cast the problem as a [[ Quadratic Eigenvalue Problem ]] offering an elegant way of treating << nonlinear monomials >> and delivering a quasi closed-form solution as a reliable starting point for a further bundle adjustment .", "h": ["Quadratic Eigenvalue Problem"], "t": ["nonlinear monomials"]}, {"label": "USED-FOR", "tokens": "By those assumptions we cast the problem as a [[ Quadratic Eigenvalue Problem ]] offering an elegant way of treating nonlinear monomials and delivering a << quasi closed-form solution >> as a reliable starting point for a further bundle adjustment .", "h": ["Quadratic Eigenvalue Problem"], "t": ["quasi closed-form solution"]}, {"label": "USED-FOR", "tokens": "By those assumptions we cast the problem as a Quadratic Eigenvalue Problem offering an elegant way of treating nonlinear monomials and delivering a [[ quasi closed-form solution ]] as a reliable starting point for a further << bundle adjustment >> .", "h": ["quasi closed-form solution"], "t": ["bundle adjustment"]}, {"label": "USED-FOR", "tokens": "We are the first to bring the [[ closed form solution ]] to such a very practical << problem >> arising in video surveillance .", "h": ["closed form solution"], "t": ["problem"]}, {"label": "FEATURE-OF", "tokens": "We are the first to bring the closed form solution to such a very practical << problem >> arising in [[ video surveillance ]] .", "h": ["video surveillance"], "t": ["problem"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a [[ human action recognition system ]] suitable for << embedded computer vision applications >> in security systems , human-computer interaction and intelligent environments .", "h": ["human action recognition system"], "t": ["embedded computer vision applications"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a human action recognition system suitable for [[ embedded computer vision applications ]] in << security systems >> , human-computer interaction and intelligent environments .", "h": ["embedded computer vision applications"], "t": ["security systems"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a human action recognition system suitable for [[ embedded computer vision applications ]] in security systems , << human-computer interaction >> and intelligent environments .", "h": ["embedded computer vision applications"], "t": ["human-computer interaction"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a human action recognition system suitable for [[ embedded computer vision applications ]] in security systems , human-computer interaction and << intelligent environments >> .", "h": ["embedded computer vision applications"], "t": ["intelligent environments"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we propose a human action recognition system suitable for embedded computer vision applications in [[ security systems ]] , << human-computer interaction >> and intelligent environments .", "h": ["security systems"], "t": ["human-computer interaction"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we propose a human action recognition system suitable for embedded computer vision applications in security systems , [[ human-computer interaction ]] and << intelligent environments >> .", "h": ["human-computer interaction"], "t": ["intelligent environments"]}, {"label": "USED-FOR", "tokens": "Our [[ system ]] is suitable for << embedded computer vision application >> based on three reasons .", "h": ["system"], "t": ["embedded computer vision application"]}, {"label": "USED-FOR", "tokens": "Firstly , the << system >> was based on a [[ linear Support Vector Machine -LRB- SVM -RRB- classifier ]] where classification progress can be implemented easily and quickly in embedded hardware .", "h": ["linear Support Vector Machine -LRB- SVM -RRB- classifier"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "Firstly , the system was based on a linear Support Vector Machine -LRB- SVM -RRB- classifier where << classification progress >> can be implemented easily and quickly in [[ embedded hardware ]] .", "h": ["embedded hardware"], "t": ["classification progress"]}, {"label": "USED-FOR", "tokens": "Secondly , we use << compacted motion features >> easily obtained from [[ videos ]] .", "h": ["videos"], "t": ["compacted motion features"]}, {"label": "USED-FOR", "tokens": "We address the limitations of the well known Motion History Image -LRB- MHI -RRB- and propose a new [[ Hierarchical Motion History Histogram -LRB- HMHH -RRB- feature ]] to represent the << motion information >> .", "h": ["Hierarchical Motion History Histogram -LRB- HMHH -RRB- feature"], "t": ["motion information"]}, {"label": "USED-FOR", "tokens": "[[ HMHH ]] not only provides << rich motion information >> , but also remains computationally inexpensive .", "h": ["HMHH"], "t": ["rich motion information"]}, {"label": "CONJUNCTION", "tokens": "Finally , we combine [[ MHI ]] and << HMHH >> together and extract a low dimension feature vector to be used in the SVM classifiers .", "h": ["MHI"], "t": ["HMHH"]}, {"label": "USED-FOR", "tokens": "Finally , we combine [[ MHI ]] and HMHH together and extract a << low dimension feature vector >> to be used in the SVM classifiers .", "h": ["MHI"], "t": ["low dimension feature vector"]}, {"label": "USED-FOR", "tokens": "Finally , we combine MHI and [[ HMHH ]] together and extract a << low dimension feature vector >> to be used in the SVM classifiers .", "h": ["HMHH"], "t": ["low dimension feature vector"]}, {"label": "USED-FOR", "tokens": "Finally , we combine MHI and HMHH together and extract a [[ low dimension feature vector ]] to be used in the << SVM classifiers >> .", "h": ["low dimension feature vector"], "t": ["SVM classifiers"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results show that our << system >> achieves significant improvement on the [[ recognition ]] performance .", "h": ["recognition"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "In this paper I will argue for a << model of grammatical processing >> that is based on [[ uniform processing ]] and knowledge sources .", "h": ["uniform processing"], "t": ["model of grammatical processing"]}, {"label": "USED-FOR", "tokens": "In this paper I will argue for a << model of grammatical processing >> that is based on uniform processing and [[ knowledge sources ]] .", "h": ["knowledge sources"], "t": ["model of grammatical processing"]}, {"label": "CONJUNCTION", "tokens": "In this paper I will argue for a model of grammatical processing that is based on << uniform processing >> and [[ knowledge sources ]] .", "h": ["knowledge sources"], "t": ["uniform processing"]}, {"label": "CONJUNCTION", "tokens": "The main feature of this model is to view [[ parsing ]] and << generation >> as two strongly interleaved tasks performed by a single parametrized deduction process .", "h": ["parsing"], "t": ["generation"]}, {"label": "HYPONYM-OF", "tokens": "The main feature of this model is to view [[ parsing ]] and generation as two strongly interleaved << tasks >> performed by a single parametrized deduction process .", "h": ["parsing"], "t": ["tasks"]}, {"label": "HYPONYM-OF", "tokens": "The main feature of this model is to view parsing and [[ generation ]] as two strongly interleaved << tasks >> performed by a single parametrized deduction process .", "h": ["generation"], "t": ["tasks"]}, {"label": "USED-FOR", "tokens": "The main feature of this model is to view parsing and generation as two strongly interleaved << tasks >> performed by a single [[ parametrized deduction process ]] .", "h": ["parametrized deduction process"], "t": ["tasks"]}, {"label": "USED-FOR", "tokens": "[[ Link detection ]] has been regarded as a core technology for the << Topic Detection and Tracking tasks of new event detection >> .", "h": ["Link detection"], "t": ["Topic Detection and Tracking tasks of new event detection"]}, {"label": "CONJUNCTION", "tokens": "In this paper we formulate [[ story link detection ]] and << new event detection >> as information retrieval task and hypothesize on the impact of precision and recall on both systems .", "h": ["story link detection"], "t": ["new event detection"]}, {"label": "HYPONYM-OF", "tokens": "In this paper we formulate [[ story link detection ]] and new event detection as information retrieval task and hypothesize on the impact of precision and recall on both << systems >> .", "h": ["story link detection"], "t": ["systems"]}, {"label": "HYPONYM-OF", "tokens": "In this paper we formulate story link detection and [[ new event detection ]] as information retrieval task and hypothesize on the impact of precision and recall on both << systems >> .", "h": ["new event detection"], "t": ["systems"]}, {"label": "USED-FOR", "tokens": "In this paper we formulate << story link detection >> and new event detection as [[ information retrieval task ]] and hypothesize on the impact of precision and recall on both systems .", "h": ["information retrieval task"], "t": ["story link detection"]}, {"label": "USED-FOR", "tokens": "In this paper we formulate story link detection and << new event detection >> as [[ information retrieval task ]] and hypothesize on the impact of precision and recall on both systems .", "h": ["information retrieval task"], "t": ["new event detection"]}, {"label": "CONJUNCTION", "tokens": "In this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of [[ precision ]] and << recall >> on both systems .", "h": ["precision"], "t": ["recall"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of [[ precision ]] and recall on both << systems >> .", "h": ["precision"], "t": ["systems"]}, {"label": "EVALUATE-FOR", "tokens": "In this paper we formulate story link detection and new event detection as information retrieval task and hypothesize on the impact of precision and [[ recall ]] on both << systems >> .", "h": ["recall"], "t": ["systems"]}, {"label": "PART-OF", "tokens": "Motivated by these arguments , we introduce a number of new << performance enhancing techniques >> including [[ part of speech tagging ]] , new similarity measures and expanded stop lists .", "h": ["part of speech tagging"], "t": ["performance enhancing techniques"]}, {"label": "CONJUNCTION", "tokens": "Motivated by these arguments , we introduce a number of new performance enhancing techniques including [[ part of speech tagging ]] , new << similarity measures >> and expanded stop lists .", "h": ["part of speech tagging"], "t": ["similarity measures"]}, {"label": "PART-OF", "tokens": "Motivated by these arguments , we introduce a number of new << performance enhancing techniques >> including part of speech tagging , new [[ similarity measures ]] and expanded stop lists .", "h": ["similarity measures"], "t": ["performance enhancing techniques"]}, {"label": "CONJUNCTION", "tokens": "Motivated by these arguments , we introduce a number of new performance enhancing techniques including part of speech tagging , new [[ similarity measures ]] and << expanded stop lists >> .", "h": ["similarity measures"], "t": ["expanded stop lists"]}, {"label": "PART-OF", "tokens": "Motivated by these arguments , we introduce a number of new << performance enhancing techniques >> including part of speech tagging , new similarity measures and [[ expanded stop lists ]] .", "h": ["expanded stop lists"], "t": ["performance enhancing techniques"]}, {"label": "USED-FOR", "tokens": "We attempt to understand << visual classification >> in humans using both [[ psy-chophysical and machine learning techniques ]] .", "h": ["psy-chophysical and machine learning techniques"], "t": ["visual classification"]}, {"label": "USED-FOR", "tokens": "[[ Frontal views of human faces ]] were used for a << gender classification task >> .", "h": ["Frontal views of human faces"], "t": ["gender classification task"]}, {"label": "USED-FOR", "tokens": "Several [[ hyperplane learning algorithms ]] were used on the same << classification task >> using the Principal Components of the texture and flowfield representation of the faces .", "h": ["hyperplane learning algorithms"], "t": ["classification task"]}, {"label": "USED-FOR", "tokens": "Several << hyperplane learning algorithms >> were used on the same classification task using the [[ Principal Components of the texture ]] and flowfield representation of the faces .", "h": ["Principal Components of the texture"], "t": ["hyperplane learning algorithms"]}, {"label": "USED-FOR", "tokens": "Several << hyperplane learning algorithms >> were used on the same classification task using the Principal Components of the texture and [[ flowfield representation of the faces ]] .", "h": ["flowfield representation of the faces"], "t": ["hyperplane learning algorithms"]}, {"label": "CONJUNCTION", "tokens": "Several hyperplane learning algorithms were used on the same classification task using the << Principal Components of the texture >> and [[ flowfield representation of the faces ]] .", "h": ["flowfield representation of the faces"], "t": ["Principal Components of the texture"]}, {"label": "USED-FOR", "tokens": "The << classification >> performance of the [[ learning algorithms ]] was estimated using the face database with the true gender of the faces as labels , and also with the gender estimated by the subjects .", "h": ["learning algorithms"], "t": ["classification"]}, {"label": "EVALUATE-FOR", "tokens": "The classification performance of the << learning algorithms >> was estimated using the [[ face database ]] with the true gender of the faces as labels , and also with the gender estimated by the subjects .", "h": ["face database"], "t": ["learning algorithms"]}, {"label": "USED-FOR", "tokens": "Our results suggest that << human classification >> can be modeled by some [[ hyperplane algorithms ]] in the feature space we used .", "h": ["hyperplane algorithms"], "t": ["human classification"]}, {"label": "FEATURE-OF", "tokens": "Our results suggest that human classification can be modeled by some << hyperplane algorithms >> in the [[ feature space ]] we used .", "h": ["feature space"], "t": ["hyperplane algorithms"]}, {"label": "COMPARE", "tokens": "For classification , the brain needs more processing for stimuli close to that [[ hyperplane ]] than for << those >> further away .", "h": ["hyperplane"], "t": ["those"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a [[ corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system ]] for << Dutch >> which combines statistical classification -LRB- maximum entropy -RRB- with linguistic information .", "h": ["corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system"], "t": ["Dutch"]}, {"label": "PART-OF", "tokens": "In this paper , we present a << corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system >> for Dutch which combines [[ statistical classification ]] -LRB- maximum entropy -RRB- with linguistic information .", "h": ["statistical classification"], "t": ["corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system"]}, {"label": "PART-OF", "tokens": "In this paper , we present a << corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system >> for Dutch which combines statistical classification -LRB- [[ maximum entropy ]] -RRB- with linguistic information .", "h": ["maximum entropy"], "t": ["corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system"]}, {"label": "PART-OF", "tokens": "In this paper , we present a << corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system >> for Dutch which combines statistical classification -LRB- maximum entropy -RRB- with [[ linguistic information ]] .", "h": ["linguistic information"], "t": ["corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we present a corpus-based supervised word sense disambiguation -LRB- WSD -RRB- system for Dutch which combines statistical classification -LRB- << maximum entropy >> -RRB- with [[ linguistic information ]] .", "h": ["linguistic information"], "t": ["maximum entropy"]}, {"label": "COMPARE", "tokens": "Instead of building individual [[ classifiers ]] per ambiguous wordform , we introduce a << lemma-based approach >> .", "h": ["classifiers"], "t": ["lemma-based approach"]}, {"label": "USED-FOR", "tokens": "Instead of building individual << classifiers >> per [[ ambiguous wordform ]] , we introduce a lemma-based approach .", "h": ["ambiguous wordform"], "t": ["classifiers"]}, {"label": "FEATURE-OF", "tokens": "The advantage of this novel method is that it clusters all [[ inflected forms ]] of an << ambiguous word >> in one classifier , therefore augmenting the training material available to the algorithm .", "h": ["inflected forms"], "t": ["ambiguous word"]}, {"label": "COMPARE", "tokens": "Testing the [[ lemma-based model ]] on the Dutch Senseval-2 test data , we achieve a significant increase in accuracy over the << wordform model >> .", "h": ["lemma-based model"], "t": ["wordform model"]}, {"label": "EVALUATE-FOR", "tokens": "Testing the << lemma-based model >> on the [[ Dutch Senseval-2 test data ]] , we achieve a significant increase in accuracy over the wordform model .", "h": ["Dutch Senseval-2 test data"], "t": ["lemma-based model"]}, {"label": "USED-FOR", "tokens": "We propose an exact , general and efficient [[ coarse-to-fine energy minimization strategy ]] for << semantic video segmenta-tion >> .", "h": ["coarse-to-fine energy minimization strategy"], "t": ["semantic video segmenta-tion"]}, {"label": "USED-FOR", "tokens": "Our << strategy >> is based on a [[ hierarchical abstraction of the supervoxel graph ]] that allows us to minimize an energy defined at the finest level of the hierarchy by minimizing a series of simpler energies defined over coarser graphs .", "h": ["hierarchical abstraction of the supervoxel graph"], "t": ["strategy"]}, {"label": "USED-FOR", "tokens": "It is general , i.e. , [[ it ]] can be used to minimize any << energy function >> -LRB- e.g. , unary , pairwise , and higher-order terms -RRB- with any existing energy minimization algorithm -LRB- e.g. , graph cuts and belief propagation -RRB- .", "h": ["it"], "t": ["energy function"]}, {"label": "CONJUNCTION", "tokens": "It is general , i.e. , [[ it ]] can be used to minimize any energy function -LRB- e.g. , unary , pairwise , and higher-order terms -RRB- with any existing << energy minimization algorithm >> -LRB- e.g. , graph cuts and belief propagation -RRB- .", "h": ["it"], "t": ["energy minimization algorithm"]}, {"label": "USED-FOR", "tokens": "It is general , i.e. , it can be used to minimize any << energy function >> -LRB- e.g. , unary , pairwise , and higher-order terms -RRB- with any existing [[ energy minimization algorithm ]] -LRB- e.g. , graph cuts and belief propagation -RRB- .", "h": ["energy minimization algorithm"], "t": ["energy function"]}, {"label": "HYPONYM-OF", "tokens": "It is general , i.e. , it can be used to minimize any energy function -LRB- e.g. , unary , pairwise , and higher-order terms -RRB- with any existing << energy minimization algorithm >> -LRB- e.g. , [[ graph cuts ]] and belief propagation -RRB- .", "h": ["graph cuts"], "t": ["energy minimization algorithm"]}, {"label": "CONJUNCTION", "tokens": "It is general , i.e. , it can be used to minimize any energy function -LRB- e.g. , unary , pairwise , and higher-order terms -RRB- with any existing energy minimization algorithm -LRB- e.g. , [[ graph cuts ]] and << belief propagation >> -RRB- .", "h": ["graph cuts"], "t": ["belief propagation"]}, {"label": "HYPONYM-OF", "tokens": "It is general , i.e. , it can be used to minimize any energy function -LRB- e.g. , unary , pairwise , and higher-order terms -RRB- with any existing << energy minimization algorithm >> -LRB- e.g. , graph cuts and [[ belief propagation ]] -RRB- .", "h": ["belief propagation"], "t": ["energy minimization algorithm"]}, {"label": "USED-FOR", "tokens": "[[ It ]] also gives significant speedups in << inference >> for several datasets with varying degrees of spatio-temporal continuity .", "h": ["It"], "t": ["inference"]}, {"label": "EVALUATE-FOR", "tokens": "<< It >> also gives significant speedups in inference for several [[ datasets ]] with varying degrees of spatio-temporal continuity .", "h": ["datasets"], "t": ["It"]}, {"label": "FEATURE-OF", "tokens": "It also gives significant speedups in inference for several << datasets >> with varying degrees of [[ spatio-temporal continuity ]] .", "h": ["spatio-temporal continuity"], "t": ["datasets"]}, {"label": "COMPARE", "tokens": "We also discuss the strengths and weaknesses of our [[ strategy ]] relative to existing << hierarchical approaches >> , and the kinds of image and video data that provide the best speedups .", "h": ["strategy"], "t": ["hierarchical approaches"]}, {"label": "USED-FOR", "tokens": "Motivated by the success of [[ ensemble methods ]] in << machine learning >> and other areas of natural language processing , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora .", "h": ["ensemble methods"], "t": ["machine learning"]}, {"label": "USED-FOR", "tokens": "Motivated by the success of [[ ensemble methods ]] in machine learning and other areas of << natural language processing >> , we developed a multi-strategy and multi-source approach to question answering which is based on combining the results from different answering agents searching for answers in multiple corpora .", "h": ["ensemble methods"], "t": ["natural language processing"]}, {"label": "USED-FOR", "tokens": "Motivated by the success of ensemble methods in machine learning and other areas of natural language processing , we developed a [[ multi-strategy and multi-source approach ]] to << question answering >> which is based on combining the results from different answering agents searching for answers in multiple corpora .", "h": ["multi-strategy and multi-source approach"], "t": ["question answering"]}, {"label": "USED-FOR", "tokens": "The << answering agents >> adopt fundamentally different [[ strategies ]] , one utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques .", "h": ["strategies"], "t": ["answering agents"]}, {"label": "HYPONYM-OF", "tokens": "The answering agents adopt fundamentally different << strategies >> , [[ one ]] utilizing primarily knowledge-based mechanisms and the other adopting statistical techniques .", "h": ["one"], "t": ["strategies"]}, {"label": "USED-FOR", "tokens": "The answering agents adopt fundamentally different strategies , << one >> utilizing primarily [[ knowledge-based mechanisms ]] and the other adopting statistical techniques .", "h": ["knowledge-based mechanisms"], "t": ["one"]}, {"label": "HYPONYM-OF", "tokens": "The answering agents adopt fundamentally different << strategies >> , one utilizing primarily knowledge-based mechanisms and the [[ other ]] adopting statistical techniques .", "h": ["other"], "t": ["strategies"]}, {"label": "USED-FOR", "tokens": "The answering agents adopt fundamentally different strategies , one utilizing primarily knowledge-based mechanisms and the << other >> adopting [[ statistical techniques ]] .", "h": ["statistical techniques"], "t": ["other"]}, {"label": "USED-FOR", "tokens": "We present our << multi-level answer resolution algorithm >> that combines results from the [[ answering agents ]] at the question , passage , and/or answer levels .", "h": ["answering agents"], "t": ["multi-level answer resolution algorithm"]}, {"label": "COMPARE", "tokens": "Experiments evaluating the effectiveness of our [[ answer resolution algorithm ]] show a 35.0 % relative improvement over our << baseline system >> in the number of questions correctly answered , and a 32.8 % improvement according to the average precision metric .", "h": ["answer resolution algorithm"], "t": ["baseline system"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments evaluating the effectiveness of our << answer resolution algorithm >> show a 35.0 % relative improvement over our baseline system in the number of questions correctly answered , and a 32.8 % improvement according to the [[ average precision metric ]] .", "h": ["average precision metric"], "t": ["answer resolution algorithm"]}, {"label": "EVALUATE-FOR", "tokens": "Experiments evaluating the effectiveness of our answer resolution algorithm show a 35.0 % relative improvement over our << baseline system >> in the number of questions correctly answered , and a 32.8 % improvement according to the [[ average precision metric ]] .", "h": ["average precision metric"], "t": ["baseline system"]}, {"label": "HYPONYM-OF", "tokens": "[[ Word Identification ]] has been an important and active issue in << Chinese Natural Language Processing >> .", "h": ["Word Identification"], "t": ["Chinese Natural Language Processing"]}, {"label": "USED-FOR", "tokens": "In this paper , a new [[ mechanism ]] , based on the concept of sublanguage , is proposed for identifying << unknown words >> , especially personal names , in Chinese newspapers .", "h": ["mechanism"], "t": ["unknown words"]}, {"label": "USED-FOR", "tokens": "In this paper , a new << mechanism >> , based on the concept of [[ sublanguage ]] , is proposed for identifying unknown words , especially personal names , in Chinese newspapers .", "h": ["sublanguage"], "t": ["mechanism"]}, {"label": "HYPONYM-OF", "tokens": "In this paper , a new mechanism , based on the concept of sublanguage , is proposed for identifying << unknown words >> , especially [[ personal names ]] , in Chinese newspapers .", "h": ["personal names"], "t": ["unknown words"]}, {"label": "USED-FOR", "tokens": "In this paper , a new << mechanism >> , based on the concept of sublanguage , is proposed for identifying unknown words , especially personal names , in [[ Chinese newspapers ]] .", "h": ["Chinese newspapers"], "t": ["mechanism"]}, {"label": "PART-OF", "tokens": "The proposed << mechanism >> includes [[ title-driven name recognition ]] , adaptive dynamic word formation , identification of 2-character and 3-character Chinese names without title .", "h": ["title-driven name recognition"], "t": ["mechanism"]}, {"label": "CONJUNCTION", "tokens": "The proposed mechanism includes [[ title-driven name recognition ]] , << adaptive dynamic word formation >> , identification of 2-character and 3-character Chinese names without title .", "h": ["title-driven name recognition"], "t": ["adaptive dynamic word formation"]}, {"label": "PART-OF", "tokens": "The proposed << mechanism >> includes title-driven name recognition , [[ adaptive dynamic word formation ]] , identification of 2-character and 3-character Chinese names without title .", "h": ["adaptive dynamic word formation"], "t": ["mechanism"]}, {"label": "CONJUNCTION", "tokens": "The proposed mechanism includes title-driven name recognition , [[ adaptive dynamic word formation ]] , << identification of 2-character and 3-character Chinese names without title >> .", "h": ["adaptive dynamic word formation"], "t": ["identification of 2-character and 3-character Chinese names without title"]}, {"label": "PART-OF", "tokens": "The proposed << mechanism >> includes title-driven name recognition , adaptive dynamic word formation , [[ identification of 2-character and 3-character Chinese names without title ]] .", "h": ["identification of 2-character and 3-character Chinese names without title"], "t": ["mechanism"]}, {"label": "HYPONYM-OF", "tokens": "This report describes [[ Paul ]] , a << computer text generation system >> designed to create cohesive text through the use of lexical substitutions .", "h": ["Paul"], "t": ["computer text generation system"]}, {"label": "USED-FOR", "tokens": "This report describes Paul , a [[ computer text generation system ]] designed to create << cohesive text >> through the use of lexical substitutions .", "h": ["computer text generation system"], "t": ["cohesive text"]}, {"label": "USED-FOR", "tokens": "This report describes << Paul >> , a computer text generation system designed to create cohesive text through the use of [[ lexical substitutions ]] .", "h": ["lexical substitutions"], "t": ["Paul"]}, {"label": "COMPARE", "tokens": "Specifically , this system is designed to deterministically choose between [[ pronominalization ]] , << superordinate substitution >> , and definite noun phrase reiteration .", "h": ["pronominalization"], "t": ["superordinate substitution"]}, {"label": "COMPARE", "tokens": "Specifically , this system is designed to deterministically choose between pronominalization , [[ superordinate substitution ]] , and << definite noun phrase reiteration >> .", "h": ["superordinate substitution"], "t": ["definite noun phrase reiteration"]}, {"label": "USED-FOR", "tokens": "The [[ system ]] identifies a strength of << antecedence recovery >> for each of the lexical substitutions .", "h": ["system"], "t": ["antecedence recovery"]}, {"label": "USED-FOR", "tokens": "The system identifies a strength of [[ antecedence recovery ]] for each of the << lexical substitutions >> .", "h": ["antecedence recovery"], "t": ["lexical substitutions"]}, {"label": "USED-FOR", "tokens": "It describes the automated training and evaluation of an Optimal Position Policy , a [[ method ]] of locating the likely << positions of topic-bearing sentences >> based on genre-specific regularities of discourse structure .", "h": ["method"], "t": ["positions of topic-bearing sentences"]}, {"label": "USED-FOR", "tokens": "It describes the automated training and evaluation of an Optimal Position Policy , a << method >> of locating the likely positions of topic-bearing sentences based on [[ genre-specific regularities of discourse structure ]] .", "h": ["genre-specific regularities of discourse structure"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "This [[ method ]] can be used in << applications >> such as information retrieval , routing , and text summarization .", "h": ["method"], "t": ["applications"]}, {"label": "HYPONYM-OF", "tokens": "This method can be used in << applications >> such as [[ information retrieval ]] , routing , and text summarization .", "h": ["information retrieval"], "t": ["applications"]}, {"label": "CONJUNCTION", "tokens": "This method can be used in applications such as [[ information retrieval ]] , << routing >> , and text summarization .", "h": ["information retrieval"], "t": ["routing"]}, {"label": "HYPONYM-OF", "tokens": "This method can be used in << applications >> such as information retrieval , [[ routing ]] , and text summarization .", "h": ["routing"], "t": ["applications"]}, {"label": "CONJUNCTION", "tokens": "This method can be used in applications such as information retrieval , [[ routing ]] , and << text summarization >> .", "h": ["routing"], "t": ["text summarization"]}, {"label": "HYPONYM-OF", "tokens": "This method can be used in << applications >> such as information retrieval , routing , and [[ text summarization ]] .", "h": ["text summarization"], "t": ["applications"]}, {"label": "USED-FOR", "tokens": "We describe a general [[ framework ]] for << online multiclass learning >> based on the notion of hypothesis sharing .", "h": ["framework"], "t": ["online multiclass learning"]}, {"label": "USED-FOR", "tokens": "We describe a general << framework >> for online multiclass learning based on the [[ notion of hypothesis sharing ]] .", "h": ["notion of hypothesis sharing"], "t": ["framework"]}, {"label": "USED-FOR", "tokens": "We generalize the [[ multiclass Perceptron ]] to our << framework >> and derive a unifying mistake bound analysis .", "h": ["multiclass Perceptron"], "t": ["framework"]}, {"label": "COMPARE", "tokens": "We demonstrate the merits of our approach by comparing [[ it ]] to previous << methods >> on both synthetic and natural datasets .", "h": ["it"], "t": ["methods"]}, {"label": "EVALUATE-FOR", "tokens": "We demonstrate the merits of our approach by comparing << it >> to previous methods on both [[ synthetic and natural datasets ]] .", "h": ["synthetic and natural datasets"], "t": ["it"]}, {"label": "EVALUATE-FOR", "tokens": "We demonstrate the merits of our approach by comparing it to previous << methods >> on both [[ synthetic and natural datasets ]] .", "h": ["synthetic and natural datasets"], "t": ["methods"]}, {"label": "USED-FOR", "tokens": "We describe a set of [[ supervised machine learning ]] experiments centering on the construction of << statistical models of WH-questions >> .", "h": ["supervised machine learning"], "t": ["statistical models of WH-questions"]}, {"label": "USED-FOR", "tokens": "These << models >> , which are built from [[ shallow linguistic features of questions ]] , are employed to predict target variables which represent a user 's informational goals .", "h": ["shallow linguistic features of questions"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "We argue in favor of the the use of [[ labeled directed graph ]] to represent various types of << linguistic structures >> , and illustrate how this allows one to view NLP tasks as graph transformations .", "h": ["labeled directed graph"], "t": ["linguistic structures"]}, {"label": "USED-FOR", "tokens": "We argue in favor of the the use of [[ labeled directed graph ]] to represent various types of linguistic structures , and illustrate how this allows one to view << NLP tasks >> as graph transformations .", "h": ["labeled directed graph"], "t": ["NLP tasks"]}, {"label": "USED-FOR", "tokens": "We argue in favor of the the use of labeled directed graph to represent various types of linguistic structures , and illustrate how [[ this ]] allows one to view << NLP tasks >> as graph transformations .", "h": ["this"], "t": ["NLP tasks"]}, {"label": "USED-FOR", "tokens": "We present a general [[ method ]] for learning such << transformations >> from an annotated corpus and describe experiments with two applications of the method : identification of non-local depenencies -LRB- using Penn Treebank data -RRB- and semantic role labeling -LRB- using Proposition Bank data -RRB- .", "h": ["method"], "t": ["transformations"]}, {"label": "USED-FOR", "tokens": "We present a general << method >> for learning such transformations from an [[ annotated corpus ]] and describe experiments with two applications of the method : identification of non-local depenencies -LRB- using Penn Treebank data -RRB- and semantic role labeling -LRB- using Proposition Bank data -RRB- .", "h": ["annotated corpus"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "We present a general method for learning such transformations from an annotated corpus and describe experiments with two << applications >> of the [[ method ]] : identification of non-local depenencies -LRB- using Penn Treebank data -RRB- and semantic role labeling -LRB- using Proposition Bank data -RRB- .", "h": ["method"], "t": ["applications"]}, {"label": "HYPONYM-OF", "tokens": "We present a general method for learning such transformations from an annotated corpus and describe experiments with two << applications >> of the method : [[ identification of non-local depenencies ]] -LRB- using Penn Treebank data -RRB- and semantic role labeling -LRB- using Proposition Bank data -RRB- .", "h": ["identification of non-local depenencies"], "t": ["applications"]}, {"label": "USED-FOR", "tokens": "We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method : << identification of non-local depenencies >> -LRB- using [[ Penn Treebank data ]] -RRB- and semantic role labeling -LRB- using Proposition Bank data -RRB- .", "h": ["Penn Treebank data"], "t": ["identification of non-local depenencies"]}, {"label": "HYPONYM-OF", "tokens": "We present a general method for learning such transformations from an annotated corpus and describe experiments with two << applications >> of the method : identification of non-local depenencies -LRB- using Penn Treebank data -RRB- and [[ semantic role labeling ]] -LRB- using Proposition Bank data -RRB- .", "h": ["semantic role labeling"], "t": ["applications"]}, {"label": "USED-FOR", "tokens": "We present a general method for learning such transformations from an annotated corpus and describe experiments with two applications of the method : identification of non-local depenencies -LRB- using Penn Treebank data -RRB- and << semantic role labeling >> -LRB- using [[ Proposition Bank data ]] -RRB- .", "h": ["Proposition Bank data"], "t": ["semantic role labeling"]}, {"label": "USED-FOR", "tokens": "We describe a generative probabilistic model of natural language , which we call HBG , that takes advantage of detailed [[ linguistic information ]] to resolve << ambiguity >> .", "h": ["linguistic information"], "t": ["ambiguity"]}, {"label": "USED-FOR", "tokens": "[[ HBG ]] incorporates lexical , syntactic , semantic , and structural information from the parse tree into the << disambiguation process >> in a novel way .", "h": ["HBG"], "t": ["disambiguation process"]}, {"label": "USED-FOR", "tokens": "<< HBG >> incorporates [[ lexical , syntactic , semantic , and structural information ]] from the parse tree into the disambiguation process in a novel way .", "h": ["lexical , syntactic , semantic , and structural information"], "t": ["HBG"]}, {"label": "CONJUNCTION", "tokens": "We use a [[ corpus of bracketed sentences ]] , called a Treebank , in combination with << decision tree building >> to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence .", "h": ["corpus of bracketed sentences"], "t": ["decision tree building"]}, {"label": "USED-FOR", "tokens": "We use a [[ corpus of bracketed sentences ]] , called a Treebank , in combination with decision tree building to tease out the relevant aspects of a << parse tree >> that will determine the correct parse of a sentence .", "h": ["corpus of bracketed sentences"], "t": ["parse tree"]}, {"label": "USED-FOR", "tokens": "We use a corpus of bracketed sentences , called a Treebank , in combination with [[ decision tree building ]] to tease out the relevant aspects of a << parse tree >> that will determine the correct parse of a sentence .", "h": ["decision tree building"], "t": ["parse tree"]}, {"label": "USED-FOR", "tokens": "We use a corpus of bracketed sentences , called a Treebank , in combination with decision tree building to tease out the relevant aspects of a [[ parse tree ]] that will determine the correct << parse >> of a sentence .", "h": ["parse tree"], "t": ["parse"]}, {"label": "USED-FOR", "tokens": "This stands in contrast to the usual approach of further [[ grammar tailoring ]] via the usual linguistic introspection in the hope of generating the correct << parse >> .", "h": ["grammar tailoring"], "t": ["parse"]}, {"label": "USED-FOR", "tokens": "This stands in contrast to the usual approach of further << grammar tailoring >> via the usual [[ linguistic introspection ]] in the hope of generating the correct parse .", "h": ["linguistic introspection"], "t": ["grammar tailoring"]}, {"label": "HYPONYM-OF", "tokens": "In head-to-head tests against one of the best existing << robust probabilistic parsing models >> , which we call [[ P-CFG ]] , the HBG model significantly outperforms P-CFG , increasing the parsing accuracy rate from 60 % to 75 % , a 37 % reduction in error .", "h": ["P-CFG"], "t": ["robust probabilistic parsing models"]}, {"label": "COMPARE", "tokens": "In head-to-head tests against one of the best existing robust probabilistic parsing models , which we call P-CFG , the [[ HBG model ]] significantly outperforms << P-CFG >> , increasing the parsing accuracy rate from 60 % to 75 % , a 37 % reduction in error .", "h": ["HBG model"], "t": ["P-CFG"]}, {"label": "EVALUATE-FOR", "tokens": "In head-to-head tests against one of the best existing robust probabilistic parsing models , which we call P-CFG , the << HBG model >> significantly outperforms P-CFG , increasing the [[ parsing accuracy rate ]] from 60 % to 75 % , a 37 % reduction in error .", "h": ["parsing accuracy rate"], "t": ["HBG model"]}, {"label": "USED-FOR", "tokens": "The framework of the << analysis >> is [[ model-theoretic semantics ]] .", "h": ["model-theoretic semantics"], "t": ["analysis"]}, {"label": "USED-FOR", "tokens": "This paper addresses the issue of << word-sense ambiguity >> in extraction from [[ machine-readable resources ]] for the construction of large-scale knowledge sources .", "h": ["machine-readable resources"], "t": ["word-sense ambiguity"]}, {"label": "USED-FOR", "tokens": "This paper addresses the issue of word-sense ambiguity in extraction from [[ machine-readable resources ]] for the << construction of large-scale knowledge sources >> .", "h": ["machine-readable resources"], "t": ["construction of large-scale knowledge sources"]}, {"label": "EVALUATE-FOR", "tokens": "We describe two experiments : one which ignored word-sense distinctions , resulting in 6.3 % [[ accuracy ]] for << semantic classification >> of verbs based on -LRB- Levin , 1993 -RRB- ; and one which exploited word-sense distinctions , resulting in 97.9 % accuracy .", "h": ["accuracy"], "t": ["semantic classification"]}, {"label": "CONJUNCTION", "tokens": "These experiments were dual purpose : -LRB- 1 -RRB- to validate the central thesis of the work of -LRB- Levin , 1993 -RRB- , i.e. , that [[ verb semantics ]] and << syntactic behavior >> are predictably related ; -LRB- 2 -RRB- to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses .", "h": ["verb semantics"], "t": ["syntactic behavior"]}, {"label": "USED-FOR", "tokens": "These experiments were dual purpose : -LRB- 1 -RRB- to validate the central thesis of the work of -LRB- Levin , 1993 -RRB- , i.e. , that verb semantics and syntactic behavior are predictably related ; -LRB- 2 -RRB- to demonstrate that a 15-fold improvement can be achieved in deriving << semantic information >> from [[ syntactic cues ]] if we first divide the syntactic cues into distinct groupings that correlate with different word senses .", "h": ["syntactic cues"], "t": ["semantic information"]}, {"label": "USED-FOR", "tokens": "Finally , we show that we can provide effective acquisition [[ techniques ]] for novel << word senses >> using a combination of online sources .", "h": ["techniques"], "t": ["word senses"]}, {"label": "USED-FOR", "tokens": "Finally , we show that we can provide effective acquisition << techniques >> for novel word senses using a combination of [[ online sources ]] .", "h": ["online sources"], "t": ["techniques"]}, {"label": "USED-FOR", "tokens": "The [[ TIPSTER Architecture ]] has been designed to enable a variety of different << text applications >> to use a set of common text processing modules .", "h": ["TIPSTER Architecture"], "t": ["text applications"]}, {"label": "USED-FOR", "tokens": "The TIPSTER Architecture has been designed to enable a variety of different << text applications >> to use a set of [[ common text processing modules ]] .", "h": ["common text processing modules"], "t": ["text applications"]}, {"label": "USED-FOR", "tokens": "Since [[ user interfaces ]] work best when customized for particular << applications >> , it is appropriator that no particular user interface styles or conventions are described in the TIPSTER Architecture specification .", "h": ["user interfaces"], "t": ["applications"]}, {"label": "USED-FOR", "tokens": "However , the Computing Research Laboratory -LRB- CRL -RRB- has constructed several << TIPSTER applications >> that use a common set of configurable [[ Graphical User Interface -LRB- GUI -RRB- functions ]] .", "h": ["Graphical User Interface -LRB- GUI -RRB- functions"], "t": ["TIPSTER applications"]}, {"label": "USED-FOR", "tokens": "These << GUIs >> were constructed using [[ CRL 's TIPSTER User Interface Toolkit -LRB- TUIT -RRB- ]] .", "h": ["CRL 's TIPSTER User Interface Toolkit -LRB- TUIT -RRB-"], "t": ["GUIs"]}, {"label": "HYPONYM-OF", "tokens": "[[ TUIT ]] is a << software library >> that can be used to construct multilingual TIPSTER user interfaces for a set of common user tasks .", "h": ["TUIT"], "t": ["software library"]}, {"label": "USED-FOR", "tokens": "[[ TUIT ]] is a software library that can be used to construct << multilingual TIPSTER user interfaces >> for a set of common user tasks .", "h": ["TUIT"], "t": ["multilingual TIPSTER user interfaces"]}, {"label": "USED-FOR", "tokens": "CRL developed [[ TUIT ]] to support their work to integrate << TIPSTER modules >> for the 6 and 12 month TIPSTER II demonstrations as well as their Oleada and Temple demonstration projects .", "h": ["TUIT"], "t": ["TIPSTER modules"]}, {"label": "PART-OF", "tokens": "While such decoding is an essential underpinning , much recent work suggests that natural language interfaces will never appear cooperative or graceful unless << they >> also incorporate numerous [[ non-literal aspects of communication ]] , such as robust communication procedures .", "h": ["non-literal aspects of communication"], "t": ["they"]}, {"label": "HYPONYM-OF", "tokens": "While such decoding is an essential underpinning , much recent work suggests that natural language interfaces will never appear cooperative or graceful unless they also incorporate numerous << non-literal aspects of communication >> , such as [[ robust communication procedures ]] .", "h": ["robust communication procedures"], "t": ["non-literal aspects of communication"]}, {"label": "PART-OF", "tokens": "This paper defends that view , but claims that direct imitation of human performance is not the best way to implement many of these non-literal aspects of communication ; that the new technology of powerful << personal computers >> with integral [[ graphics displays ]] offers techniques superior to those of humans for these aspects , while still satisfying human communication needs .", "h": ["graphics displays"], "t": ["personal computers"]}, {"label": "USED-FOR", "tokens": "This paper proposes a framework in which [[ Lagrangian Particle Dynamics ]] is used for the << segmentation of high density crowd flows >> and detection of flow instabilities .", "h": ["Lagrangian Particle Dynamics"], "t": ["segmentation of high density crowd flows"]}, {"label": "USED-FOR", "tokens": "This paper proposes a framework in which [[ Lagrangian Particle Dynamics ]] is used for the segmentation of high density crowd flows and << detection of flow instabilities >> .", "h": ["Lagrangian Particle Dynamics"], "t": ["detection of flow instabilities"]}, {"label": "CONJUNCTION", "tokens": "This paper proposes a framework in which Lagrangian Particle Dynamics is used for the [[ segmentation of high density crowd flows ]] and << detection of flow instabilities >> .", "h": ["segmentation of high density crowd flows"], "t": ["detection of flow instabilities"]}, {"label": "USED-FOR", "tokens": "For this purpose , a << flow field >> generated by a [[ moving crowd ]] is treated as an aperiodic dynamical system .", "h": ["moving crowd"], "t": ["flow field"]}, {"label": "USED-FOR", "tokens": "For this purpose , a << flow field >> generated by a moving crowd is treated as an [[ aperiodic dynamical system ]] .", "h": ["aperiodic dynamical system"], "t": ["flow field"]}, {"label": "USED-FOR", "tokens": "A [[ grid of particles ]] is overlaid on the << flow field >> , and is advected using a numerical integration scheme .", "h": ["grid of particles"], "t": ["flow field"]}, {"label": "USED-FOR", "tokens": "A << grid of particles >> is overlaid on the flow field , and is advected using a [[ numerical integration scheme ]] .", "h": ["numerical integration scheme"], "t": ["grid of particles"]}, {"label": "USED-FOR", "tokens": "The << evolution of particles >> through the flow is tracked using a [[ Flow Map ]] , whose spatial gradients are subsequently used to setup a Cauchy Green Deformation tensor for quantifying the amount by which the neighboring particles have diverged over the length of the integration .", "h": ["Flow Map"], "t": ["evolution of particles"]}, {"label": "USED-FOR", "tokens": "The evolution of particles through the flow is tracked using a Flow Map , whose [[ spatial gradients ]] are subsequently used to setup a << Cauchy Green Deformation tensor >> for quantifying the amount by which the neighboring particles have diverged over the length of the integration .", "h": ["spatial gradients"], "t": ["Cauchy Green Deformation tensor"]}, {"label": "FEATURE-OF", "tokens": "The [[ maximum eigenvalue ]] of the << tensor >> is used to construct a Finite Time Lyapunov Exponent -LRB- FTLE -RRB- field , which reveals the Lagrangian Coherent Structures -LRB- LCS -RRB- present in the underlying flow .", "h": ["maximum eigenvalue"], "t": ["tensor"]}, {"label": "USED-FOR", "tokens": "The [[ maximum eigenvalue ]] of the tensor is used to construct a << Finite Time Lyapunov Exponent -LRB- FTLE -RRB- field >> , which reveals the Lagrangian Coherent Structures -LRB- LCS -RRB- present in the underlying flow .", "h": ["maximum eigenvalue"], "t": ["Finite Time Lyapunov Exponent -LRB- FTLE -RRB- field"]}, {"label": "USED-FOR", "tokens": "The maximum eigenvalue of the tensor is used to construct a [[ Finite Time Lyapunov Exponent -LRB- FTLE -RRB- field ]] , which reveals the << Lagrangian Coherent Structures -LRB- LCS -RRB- >> present in the underlying flow .", "h": ["Finite Time Lyapunov Exponent -LRB- FTLE -RRB- field"], "t": ["Lagrangian Coherent Structures -LRB- LCS -RRB-"]}, {"label": "USED-FOR", "tokens": "The [[ LCS ]] divide flow into regions of qualitatively different dynamics and are used to locate << boundaries of the flow segments >> in a normalized cuts framework .", "h": ["LCS"], "t": ["boundaries of the flow segments"]}, {"label": "USED-FOR", "tokens": "The LCS divide flow into regions of qualitatively different dynamics and are used to locate << boundaries of the flow segments >> in a [[ normalized cuts framework ]] .", "h": ["normalized cuts framework"], "t": ["boundaries of the flow segments"]}, {"label": "CONJUNCTION", "tokens": "The experiments are conducted on a challenging set of videos taken from [[ Google Video ]] and a << National Geographic documentary >> .", "h": ["Google Video"], "t": ["National Geographic documentary"]}, {"label": "EVALUATE-FOR", "tokens": "Over the last decade , a variety of SMT algorithms have been built and empirically tested whereas little is known about the [[ computational complexity ]] of some of the fundamental << problems >> of SMT .", "h": ["computational complexity"], "t": ["problems"]}, {"label": "PART-OF", "tokens": "Over the last decade , a variety of SMT algorithms have been built and empirically tested whereas little is known about the computational complexity of some of the fundamental [[ problems ]] of << SMT >> .", "h": ["problems"], "t": ["SMT"]}, {"label": "EVALUATE-FOR", "tokens": "Our work aims at providing useful insights into the the [[ computational complexity ]] of those << problems >> .", "h": ["computational complexity"], "t": ["problems"]}, {"label": "COMPARE", "tokens": "We prove that while [[ IBM Models 1-2 ]] are conceptually and computationally simple , computations involving the higher -LRB- and more useful -RRB- << models >> are hard .", "h": ["IBM Models 1-2"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "Since it is unlikely that there exists a [[ polynomial time solution ]] for any of these << hard problems >> -LRB- unless P = NP and P #P = P -RRB- , our results highlight and justify the need for developing polynomial time approximations for these computations .", "h": ["polynomial time solution"], "t": ["hard problems"]}, {"label": "USED-FOR", "tokens": "Since it is unlikely that there exists a polynomial time solution for any of these hard problems -LRB- unless P = NP and P #P = P -RRB- , our results highlight and justify the need for developing [[ polynomial time approximations ]] for these << computations >> .", "h": ["polynomial time approximations"], "t": ["computations"]}, {"label": "EVALUATE-FOR", "tokens": "Most state-of-the-art [[ evaluation measures ]] for << machine translation >> assign high costs to movements of word blocks .", "h": ["evaluation measures"], "t": ["machine translation"]}, {"label": "USED-FOR", "tokens": "In this paper , we will present a new [[ evaluation measure ]] which explicitly models << block reordering >> as an edit operation .", "h": ["evaluation measure"], "t": ["block reordering"]}, {"label": "USED-FOR", "tokens": "In this paper , we will present a new evaluation measure which explicitly models << block reordering >> as an [[ edit operation ]] .", "h": ["edit operation"], "t": ["block reordering"]}, {"label": "FEATURE-OF", "tokens": "Our << measure >> can be exactly calculated in [[ quadratic time ]] .", "h": ["quadratic time"], "t": ["measure"]}, {"label": "USED-FOR", "tokens": "Furthermore , we will show how some << evaluation measures >> can be improved by the introduction of [[ word-dependent substitution costs ]] .", "h": ["word-dependent substitution costs"], "t": ["evaluation measures"]}, {"label": "COMPARE", "tokens": "The correlation of the new [[ measure ]] with << human judgment >> has been investigated systematically on two different language pairs .", "h": ["measure"], "t": ["human judgment"]}, {"label": "COMPARE", "tokens": "The experimental results will show that [[ it ]] significantly outperforms state-of-the-art << approaches >> in sentence-level correlation .", "h": ["it"], "t": ["approaches"]}, {"label": "EVALUATE-FOR", "tokens": "The experimental results will show that << it >> significantly outperforms state-of-the-art approaches in [[ sentence-level correlation ]] .", "h": ["sentence-level correlation"], "t": ["it"]}, {"label": "EVALUATE-FOR", "tokens": "The experimental results will show that it significantly outperforms state-of-the-art << approaches >> in [[ sentence-level correlation ]] .", "h": ["sentence-level correlation"], "t": ["approaches"]}, {"label": "CONJUNCTION", "tokens": "Results from experiments with word dependent substitution costs will demonstrate an additional increase of correlation between [[ automatic evaluation measures ]] and << human judgment >> .", "h": ["automatic evaluation measures"], "t": ["human judgment"]}, {"label": "HYPONYM-OF", "tokens": "The [[ Rete and Treat algorithms ]] are considered the most efficient << implementation techniques >> for Forward Chaining rule systems .", "h": ["Rete and Treat algorithms"], "t": ["implementation techniques"]}, {"label": "USED-FOR", "tokens": "The [[ Rete and Treat algorithms ]] are considered the most efficient implementation techniques for << Forward Chaining rule systems >> .", "h": ["Rete and Treat algorithms"], "t": ["Forward Chaining rule systems"]}, {"label": "USED-FOR", "tokens": "These [[ algorithms ]] support a << language of limited expressive power >> .", "h": ["algorithms"], "t": ["language of limited expressive power"]}, {"label": "USED-FOR", "tokens": "In this paper we show how to support << full unification >> in these [[ algorithms ]] .", "h": ["algorithms"], "t": ["full unification"]}, {"label": "CONJUNCTION", "tokens": "We also show that : Supporting full unification is costly ; Full unification is not used frequently ; A combination of [[ compile time ]] and << run time >> checks can determine when full unification is not needed .", "h": ["compile time"], "t": ["run time"]}, {"label": "EVALUATE-FOR", "tokens": "We also show that : Supporting full unification is costly ; Full unification is not used frequently ; A combination of [[ compile time ]] and run time checks can determine when << full unification >> is not needed .", "h": ["compile time"], "t": ["full unification"]}, {"label": "EVALUATE-FOR", "tokens": "We also show that : Supporting full unification is costly ; Full unification is not used frequently ; A combination of compile time and [[ run time ]] checks can determine when << full unification >> is not needed .", "h": ["run time"], "t": ["full unification"]}, {"label": "USED-FOR", "tokens": "A [[ method ]] for << error correction >> of ill-formed input is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs .", "h": ["method"], "t": ["error correction"]}, {"label": "USED-FOR", "tokens": "A method for << error correction >> of [[ ill-formed input ]] is described that acquires dialogue patterns in typical usage and uses these patterns to predict new inputs .", "h": ["ill-formed input"], "t": ["error correction"]}, {"label": "USED-FOR", "tokens": "A [[ dialogue acquisition and tracking algorithm ]] is presented along with a description of its implementation in a << voice interactive system >> .", "h": ["dialogue acquisition and tracking algorithm"], "t": ["voice interactive system"]}, {"label": "USED-FOR", "tokens": "A series of tests are described that show the power of the << error correction methodology >> when [[ stereotypic dialogue ]] occurs .", "h": ["stereotypic dialogue"], "t": ["error correction methodology"]}, {"label": "HYPONYM-OF", "tokens": "Traditional [[ linear Fukunaga-Koontz Transform -LRB- FKT -RRB- ]] -LSB- 1 -RSB- is a powerful << discriminative subspaces building approach >> .", "h": ["linear Fukunaga-Koontz Transform -LRB- FKT -RRB-"], "t": ["discriminative subspaces building approach"]}, {"label": "USED-FOR", "tokens": "Previous work has successfully extended [[ FKT ]] to be able to deal with << small-sample-size >> .", "h": ["FKT"], "t": ["small-sample-size"]}, {"label": "USED-FOR", "tokens": "In this paper , we extend traditional [[ linear FKT ]] to enable << it >> to work in multi-class problem and also in higher dimensional -LRB- kernel -RRB- subspaces and therefore provide enhanced discrimination ability .", "h": ["linear FKT"], "t": ["it"]}, {"label": "USED-FOR", "tokens": "In this paper , we extend traditional linear FKT to enable [[ it ]] to work in << multi-class problem >> and also in higher dimensional -LRB- kernel -RRB- subspaces and therefore provide enhanced discrimination ability .", "h": ["it"], "t": ["multi-class problem"]}, {"label": "USED-FOR", "tokens": "In this paper , we extend traditional linear FKT to enable [[ it ]] to work in multi-class problem and also in << higher dimensional -LRB- kernel -RRB- subspaces >> and therefore provide enhanced discrimination ability .", "h": ["it"], "t": ["higher dimensional -LRB- kernel -RRB- subspaces"]}, {"label": "FEATURE-OF", "tokens": "In this paper , we extend traditional linear FKT to enable [[ it ]] to work in multi-class problem and also in higher dimensional -LRB- kernel -RRB- subspaces and therefore provide enhanced << discrimination ability >> .", "h": ["it"], "t": ["discrimination ability"]}, {"label": "CONJUNCTION", "tokens": "In this paper , we extend traditional linear FKT to enable it to work in [[ multi-class problem ]] and also in << higher dimensional -LRB- kernel -RRB- subspaces >> and therefore provide enhanced discrimination ability .", "h": ["multi-class problem"], "t": ["higher dimensional -LRB- kernel -RRB- subspaces"]}, {"label": "EVALUATE-FOR", "tokens": "We verify the effectiveness of the proposed << Kernel Fukunaga-Koontz Transform >> by demonstrating its effectiveness in [[ face recognition applications ]] ; however the proposed non-linear generalization can be applied to any other domain specific problems .", "h": ["face recognition applications"], "t": ["Kernel Fukunaga-Koontz Transform"]}, {"label": "USED-FOR", "tokens": "We verify the effectiveness of the proposed Kernel Fukunaga-Koontz Transform by demonstrating its effectiveness in face recognition applications ; however the proposed [[ non-linear generalization ]] can be applied to any other << domain specific problems >> .", "h": ["non-linear generalization"], "t": ["domain specific problems"]}, {"label": "COMPARE", "tokens": "While this [[ task ]] has much in common with << paraphrases acquisition >> which aims to discover semantic equivalence between verbs , the main challenge of entailment acquisition is to capture asymmetric , or directional , relations .", "h": ["task"], "t": ["paraphrases acquisition"]}, {"label": "USED-FOR", "tokens": "While this task has much in common with [[ paraphrases acquisition ]] which aims to discover << semantic equivalence >> between verbs , the main challenge of entailment acquisition is to capture asymmetric , or directional , relations .", "h": ["paraphrases acquisition"], "t": ["semantic equivalence"]}, {"label": "USED-FOR", "tokens": "While this task has much in common with paraphrases acquisition which aims to discover semantic equivalence between verbs , the main challenge of [[ entailment acquisition ]] is to capture << asymmetric , or directional , relations >> .", "h": ["entailment acquisition"], "t": ["asymmetric , or directional , relations"]}, {"label": "USED-FOR", "tokens": "Motivated by the intuition that it often underlies the local structure of coherent text , we develop a [[ method ]] that discovers << verb entailment >> using evidence about discourse relations between clauses available in a parsed corpus .", "h": ["method"], "t": ["verb entailment"]}, {"label": "USED-FOR", "tokens": "Motivated by the intuition that it often underlies the local structure of coherent text , we develop a << method >> that discovers verb entailment using evidence about [[ discourse relations ]] between clauses available in a parsed corpus .", "h": ["discourse relations"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "Motivated by the intuition that it often underlies the local structure of coherent text , we develop a method that discovers verb entailment using evidence about << discourse relations >> between clauses available in a [[ parsed corpus ]] .", "h": ["parsed corpus"], "t": ["discourse relations"]}, {"label": "USED-FOR", "tokens": "In comparison with earlier work , the proposed [[ method ]] covers a much wider range of verb entailment types and learns the << mapping between verbs >> with highly varied argument structures .", "h": ["method"], "t": ["mapping between verbs"]}, {"label": "FEATURE-OF", "tokens": "In comparison with earlier work , the proposed method covers a much wider range of verb entailment types and learns the << mapping between verbs >> with [[ highly varied argument structures ]] .", "h": ["highly varied argument structures"], "t": ["mapping between verbs"]}, {"label": "USED-FOR", "tokens": "In this paper , we cast the problem of << point cloud matching >> as a [[ shape matching problem ]] by transforming each of the given point clouds into a shape representation called the Schr\u00f6dinger distance transform -LRB- SDT -RRB- representation .", "h": ["shape matching problem"], "t": ["point cloud matching"]}, {"label": "USED-FOR", "tokens": "In this paper , we cast the problem of point cloud matching as a shape matching problem by transforming each of the given << point clouds >> into a [[ shape representation ]] called the Schr\u00f6dinger distance transform -LRB- SDT -RRB- representation .", "h": ["shape representation"], "t": ["point clouds"]}, {"label": "HYPONYM-OF", "tokens": "In this paper , we cast the problem of point cloud matching as a shape matching problem by transforming each of the given point clouds into a << shape representation >> called the [[ Schr\u00f6dinger distance transform -LRB- SDT -RRB- representation ]] .", "h": ["Schr\u00f6dinger distance transform -LRB- SDT -RRB- representation"], "t": ["shape representation"]}, {"label": "HYPONYM-OF", "tokens": "The [[ SDT representation ]] is an << analytic expression >> and following the theoretical physics literature , can be normalized to have unit L2 norm-making it a square-root density , which is identified with a point on a unit Hilbert sphere , whose intrinsic geometry is fully known .", "h": ["SDT representation"], "t": ["analytic expression"]}, {"label": "USED-FOR", "tokens": "The SDT representation is an analytic expression and following the theoretical physics literature , can be normalized to have unit L2 norm-making << it >> a [[ square-root density ]] , which is identified with a point on a unit Hilbert sphere , whose intrinsic geometry is fully known .", "h": ["square-root density"], "t": ["it"]}, {"label": "FEATURE-OF", "tokens": "The SDT representation is an analytic expression and following the theoretical physics literature , can be normalized to have unit L2 norm-making it a square-root density , which is identified with a point on a << unit Hilbert sphere >> , whose [[ intrinsic geometry ]] is fully known .", "h": ["intrinsic geometry"], "t": ["unit Hilbert sphere"]}, {"label": "USED-FOR", "tokens": "The Fisher-Rao metric , a [[ natural metric ]] for the << space of densities >> leads to analytic expressions for the geodesic distance between points on this sphere .", "h": ["natural metric"], "t": ["space of densities"]}, {"label": "USED-FOR", "tokens": "The Fisher-Rao metric , a natural metric for the space of densities leads to [[ analytic expressions ]] for the << geodesic distance >> between points on this sphere .", "h": ["analytic expressions"], "t": ["geodesic distance"]}, {"label": "USED-FOR", "tokens": "In this paper , we use the well known [[ Riemannian framework ]] never before used for << point cloud matching >> , and present a novel matching algorithm .", "h": ["Riemannian framework"], "t": ["point cloud matching"]}, {"label": "USED-FOR", "tokens": "We pose << point set matching >> under [[ rigid and non-rigid transformations ]] in this framework and solve for the transformations using standard nonlinear optimization techniques .", "h": ["rigid and non-rigid transformations"], "t": ["point set matching"]}, {"label": "USED-FOR", "tokens": "We pose << point set matching >> under rigid and non-rigid transformations in this [[ framework ]] and solve for the transformations using standard nonlinear optimization techniques .", "h": ["framework"], "t": ["point set matching"]}, {"label": "USED-FOR", "tokens": "We pose point set matching under rigid and non-rigid transformations in this framework and solve for the << transformations >> using standard [[ nonlinear optimization techniques ]] .", "h": ["nonlinear optimization techniques"], "t": ["transformations"]}, {"label": "COMPARE", "tokens": "The experiments show that our [[ algorithm ]] outperforms state-of-the-art << point set registration algorithms >> on many quantitative metrics .", "h": ["algorithm"], "t": ["point set registration algorithms"]}, {"label": "EVALUATE-FOR", "tokens": "The experiments show that our << algorithm >> outperforms state-of-the-art point set registration algorithms on many [[ quantitative metrics ]] .", "h": ["quantitative metrics"], "t": ["algorithm"]}, {"label": "EVALUATE-FOR", "tokens": "The experiments show that our algorithm outperforms state-of-the-art << point set registration algorithms >> on many [[ quantitative metrics ]] .", "h": ["quantitative metrics"], "t": ["point set registration algorithms"]}, {"label": "USED-FOR", "tokens": "Using [[ natural language processing ]] , we carried out a << trend survey on Japanese natural language processing studies >> that have been done over the last ten years .", "h": ["natural language processing"], "t": ["trend survey on Japanese natural language processing studies"]}, {"label": "USED-FOR", "tokens": "This paper is useful for both recognizing trends in Japanese NLP and constructing a method of supporting << trend surveys >> using [[ NLP ]] .", "h": ["NLP"], "t": ["trend surveys"]}, {"label": "USED-FOR", "tokens": "HFOs additionally serve as a prototypical example of challenges in the << analysis of discrete events >> in [[ high-temporal resolution , intracranial EEG data ]] .", "h": ["high-temporal resolution , intracranial EEG data"], "t": ["analysis of discrete events"]}, {"label": "USED-FOR", "tokens": "However , previous << HFO analysis >> have assumed a [[ linear manifold ]] , global across time , space -LRB- i.e. recording electrode/channel -RRB- , and individual patients .", "h": ["linear manifold"], "t": ["HFO analysis"]}, {"label": "HYPONYM-OF", "tokens": "We also estimate bounds on the Bayes classification error to quantify the distinction between two classes of << HFOs >> -LRB- [[ those ]] occurring during seizures and those occurring due to other processes -RRB- .", "h": ["those"], "t": ["HFOs"]}, {"label": "CONJUNCTION", "tokens": "We also estimate bounds on the Bayes classification error to quantify the distinction between two classes of HFOs -LRB- [[ those ]] occurring during seizures and << those >> occurring due to other processes -RRB- .", "h": ["those"], "t": ["those"]}, {"label": "HYPONYM-OF", "tokens": "We also estimate bounds on the Bayes classification error to quantify the distinction between two classes of << HFOs >> -LRB- those occurring during seizures and [[ those ]] occurring due to other processes -RRB- .", "h": ["those"], "t": ["HFOs"]}, {"label": "HYPONYM-OF", "tokens": "This analysis provides the foundation for future clinical use of HFO features and guides the analysis for other << discrete events >> , such as individual [[ action potentials ]] or multi-unit activity .", "h": ["action potentials"], "t": ["discrete events"]}, {"label": "CONJUNCTION", "tokens": "This analysis provides the foundation for future clinical use of HFO features and guides the analysis for other discrete events , such as individual [[ action potentials ]] or << multi-unit activity >> .", "h": ["action potentials"], "t": ["multi-unit activity"]}, {"label": "HYPONYM-OF", "tokens": "This analysis provides the foundation for future clinical use of HFO features and guides the analysis for other << discrete events >> , such as individual action potentials or [[ multi-unit activity ]] .", "h": ["multi-unit activity"], "t": ["discrete events"]}, {"label": "USED-FOR", "tokens": "In this paper we present ONTOSCORE , a << system >> for scoring sets of concepts on the basis of an [[ ontology ]] .", "h": ["ontology"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "We apply our [[ system ]] to the task of scoring alternative << speech recognition hypotheses -LRB- SRH -RRB- >> in terms of their semantic coherence .", "h": ["system"], "t": ["speech recognition hypotheses -LRB- SRH -RRB-"]}, {"label": "USED-FOR", "tokens": "We propose an efficient [[ dialogue management ]] for an << information navigation system >> based on a document knowledge base .", "h": ["dialogue management"], "t": ["information navigation system"]}, {"label": "USED-FOR", "tokens": "We propose an efficient dialogue management for an << information navigation system >> based on a [[ document knowledge base ]] .", "h": ["document knowledge base"], "t": ["information navigation system"]}, {"label": "CONJUNCTION", "tokens": "It is expected that incorporation of appropriate [[ N-best candidates of ASR ]] and << contextual information >> will improve the system performance .", "h": ["N-best candidates of ASR"], "t": ["contextual information"]}, {"label": "USED-FOR", "tokens": "It is expected that incorporation of appropriate [[ N-best candidates of ASR ]] and contextual information will improve the << system >> performance .", "h": ["N-best candidates of ASR"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "It is expected that incorporation of appropriate N-best candidates of ASR and [[ contextual information ]] will improve the << system >> performance .", "h": ["contextual information"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "The [[ system ]] also has several choices in << generating responses or confirmations >> .", "h": ["system"], "t": ["generating responses or confirmations"]}, {"label": "USED-FOR", "tokens": "In this paper , this selection is optimized as << minimization of Bayes risk >> based on [[ reward ]] for correct information presentation and penalty for redundant turns .", "h": ["reward"], "t": ["minimization of Bayes risk"]}, {"label": "USED-FOR", "tokens": "In this paper , this selection is optimized as minimization of Bayes risk based on [[ reward ]] for << correct information presentation >> and penalty for redundant turns .", "h": ["reward"], "t": ["correct information presentation"]}, {"label": "CONJUNCTION", "tokens": "In this paper , this selection is optimized as minimization of Bayes risk based on [[ reward ]] for correct information presentation and << penalty >> for redundant turns .", "h": ["reward"], "t": ["penalty"]}, {"label": "USED-FOR", "tokens": "In this paper , this selection is optimized as << minimization of Bayes risk >> based on reward for correct information presentation and [[ penalty ]] for redundant turns .", "h": ["penalty"], "t": ["minimization of Bayes risk"]}, {"label": "USED-FOR", "tokens": "In this paper , this selection is optimized as minimization of Bayes risk based on reward for correct information presentation and [[ penalty ]] for << redundant turns >> .", "h": ["penalty"], "t": ["redundant turns"]}, {"label": "EVALUATE-FOR", "tokens": "We have evaluated this << strategy >> with our [[ spoken dialogue system '' Dialogue Navigator for Kyoto City '' ]] , which also has question-answering capability .", "h": ["spoken dialogue system '' Dialogue Navigator for Kyoto City ''"], "t": ["strategy"]}, {"label": "FEATURE-OF", "tokens": "We have evaluated this strategy with our << spoken dialogue system '' Dialogue Navigator for Kyoto City '' >> , which also has [[ question-answering capability ]] .", "h": ["question-answering capability"], "t": ["spoken dialogue system '' Dialogue Navigator for Kyoto City ''"]}, {"label": "EVALUATE-FOR", "tokens": "Effectiveness of the proposed << framework >> was confirmed in the [[ success rate of retrieval ]] and the average number of turns for information access .", "h": ["success rate of retrieval"], "t": ["framework"]}, {"label": "CONJUNCTION", "tokens": "Effectiveness of the proposed framework was confirmed in the [[ success rate of retrieval ]] and the << average number of turns >> for information access .", "h": ["success rate of retrieval"], "t": ["average number of turns"]}, {"label": "EVALUATE-FOR", "tokens": "Effectiveness of the proposed << framework >> was confirmed in the success rate of retrieval and the [[ average number of turns ]] for information access .", "h": ["average number of turns"], "t": ["framework"]}, {"label": "USED-FOR", "tokens": "Effectiveness of the proposed framework was confirmed in the success rate of retrieval and the [[ average number of turns ]] for << information access >> .", "h": ["average number of turns"], "t": ["information access"]}, {"label": "CONJUNCTION", "tokens": "They are probability , [[ rank ]] , and << entropy >> .", "h": ["rank"], "t": ["entropy"]}, {"label": "USED-FOR", "tokens": "We evaluated the performance of the three [[ pruning criteria ]] in a real application of << Chinese text input >> in terms of character error rate -LRB- CER -RRB- .", "h": ["pruning criteria"], "t": ["Chinese text input"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluated the performance of the three << pruning criteria >> in a real application of Chinese text input in terms of [[ character error rate -LRB- CER -RRB- ]] .", "h": ["character error rate -LRB- CER -RRB-"], "t": ["pruning criteria"]}, {"label": "EVALUATE-FOR", "tokens": "We also show that the high-performance of << rank >> lies in its strong correlation with [[ error rate ]] .", "h": ["error rate"], "t": ["rank"]}, {"label": "USED-FOR", "tokens": "We then present a novel [[ method ]] of combining two criteria in << model pruning >> .", "h": ["method"], "t": ["model pruning"]}, {"label": "USED-FOR", "tokens": "This paper proposes an [[ annotating scheme ]] that encodes << honorifics >> -LRB- respectful words -RRB- .", "h": ["annotating scheme"], "t": ["honorifics"]}, {"label": "HYPONYM-OF", "tokens": "This paper proposes an annotating scheme that encodes [[ honorifics ]] -LRB- << respectful words >> -RRB- .", "h": ["honorifics"], "t": ["respectful words"]}, {"label": "USED-FOR", "tokens": "[[ Honorifics ]] are used extensively in << Japanese >> , reflecting the social relationship -LRB- e.g. social ranks and age -RRB- of the referents .", "h": ["Honorifics"], "t": ["Japanese"]}, {"label": "USED-FOR", "tokens": "This [[ referential information ]] is vital for resolving << zero pronouns >> and improving machine translation outputs .", "h": ["referential information"], "t": ["zero pronouns"]}, {"label": "USED-FOR", "tokens": "This [[ referential information ]] is vital for resolving zero pronouns and improving << machine translation outputs >> .", "h": ["referential information"], "t": ["machine translation outputs"]}, {"label": "USED-FOR", "tokens": "<< Visually-guided arm reaching movements >> are produced by [[ distributed neural networks ]] within parietal and frontal regions of the cerebral cortex .", "h": ["distributed neural networks"], "t": ["Visually-guided arm reaching movements"]}, {"label": "USED-FOR", "tokens": "Experimental data indicate that -LRB- I -RRB- single neurons in these regions are broadly tuned to parameters of movement ; -LRB- 2 -RRB- appropriate commands are elaborated by populations of neurons ; -LRB- 3 -RRB- the << coordinated action of neu-rons >> can be visualized using a [[ neuronal population vector -LRB- NPV -RRB- ]] .", "h": ["neuronal population vector -LRB- NPV -RRB-"], "t": ["coordinated action of neu-rons"]}, {"label": "USED-FOR", "tokens": "We designed a [[ model ]] of the << cortical motor command >> to investigate the relation between the desired direction of the movement , the actual direction of movement and the direction of the NPV in motor cortex .", "h": ["model"], "t": ["cortical motor command"]}, {"label": "USED-FOR", "tokens": "We designed a model of the cortical motor command to investigate the relation between the desired direction of the movement , the actual direction of movement and the direction of the [[ NPV ]] in << motor cortex >> .", "h": ["NPV"], "t": ["motor cortex"]}, {"label": "USED-FOR", "tokens": "The model is a [[ two-layer self-organizing neural network ]] which combines broadly-tuned -LRB- muscular -RRB- proprioceptive and -LRB- cartesian -RRB- visual information to calculate << -LRB- angular -RRB- motor commands >> for the initial part of the movement of a two-link arm .", "h": ["two-layer self-organizing neural network"], "t": ["-LRB- angular -RRB- motor commands"]}, {"label": "USED-FOR", "tokens": "The model is a << two-layer self-organizing neural network >> which combines [[ broadly-tuned -LRB- muscular -RRB- proprioceptive ]] and -LRB- cartesian -RRB- visual information to calculate -LRB- angular -RRB- motor commands for the initial part of the movement of a two-link arm .", "h": ["broadly-tuned -LRB- muscular -RRB- proprioceptive"], "t": ["two-layer self-organizing neural network"]}, {"label": "CONJUNCTION", "tokens": "The model is a two-layer self-organizing neural network which combines [[ broadly-tuned -LRB- muscular -RRB- proprioceptive ]] and << -LRB- cartesian -RRB- visual information >> to calculate -LRB- angular -RRB- motor commands for the initial part of the movement of a two-link arm .", "h": ["broadly-tuned -LRB- muscular -RRB- proprioceptive"], "t": ["-LRB- cartesian -RRB- visual information"]}, {"label": "USED-FOR", "tokens": "The model is a << two-layer self-organizing neural network >> which combines broadly-tuned -LRB- muscular -RRB- proprioceptive and [[ -LRB- cartesian -RRB- visual information ]] to calculate -LRB- angular -RRB- motor commands for the initial part of the movement of a two-link arm .", "h": ["-LRB- cartesian -RRB- visual information"], "t": ["two-layer self-organizing neural network"]}, {"label": "FEATURE-OF", "tokens": "These results suggest the NPV does not give a faithful << image of cortical processing >> during [[ arm reaching movements ]] .", "h": ["arm reaching movements"], "t": ["image of cortical processing"]}, {"label": "USED-FOR", "tokens": "It is well-known that diversity among [[ base classifiers ]] is crucial for constructing a strong << ensemble >> .", "h": ["base classifiers"], "t": ["ensemble"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose an alternative way for << ensemble construction >> by [[ resampling pairwise constraints ]] that specify whether a pair of instances belongs to the same class or not .", "h": ["resampling pairwise constraints"], "t": ["ensemble construction"]}, {"label": "USED-FOR", "tokens": "Using [[ pairwise constraints ]] for << ensemble construction >> is challenging because it remains unknown how to influence the base classifiers with the sampled pairwise constraints .", "h": ["pairwise constraints"], "t": ["ensemble construction"]}, {"label": "USED-FOR", "tokens": "First , we transform the original instances into a new << data representation >> using [[ projections ]] learnt from pairwise constraints .", "h": ["projections"], "t": ["data representation"]}, {"label": "USED-FOR", "tokens": "First , we transform the original instances into a new data representation using << projections >> learnt from [[ pairwise constraints ]] .", "h": ["pairwise constraints"], "t": ["projections"]}, {"label": "USED-FOR", "tokens": "Then , we build the << base clas-sifiers >> with the new [[ data representation ]] .", "h": ["data representation"], "t": ["base clas-sifiers"]}, {"label": "USED-FOR", "tokens": "We propose two methods for << resampling pairwise constraints >> following the standard [[ Bagging and Boosting algorithms ]] , respectively .", "h": ["Bagging and Boosting algorithms"], "t": ["resampling pairwise constraints"]}, {"label": "USED-FOR", "tokens": "A new [[ algorithm ]] for solving the three << dimensional container packing problem >> is proposed in this paper .", "h": ["algorithm"], "t": ["dimensional container packing problem"]}, {"label": "COMPARE", "tokens": "This new [[ algorithm ]] deviates from the traditional << approach of wall building and layering >> .", "h": ["algorithm"], "t": ["approach of wall building and layering"]}, {"label": "EVALUATE-FOR", "tokens": "We tested our << method >> using all 760 test cases from the [[ OR-Library ]] .", "h": ["OR-Library"], "t": ["method"]}, {"label": "EVALUATE-FOR", "tokens": "Experimental results indicate that the new << algorithm >> is able to achieve an [[ average packing utilization ]] of more than 87 % .", "h": ["average packing utilization"], "t": ["algorithm"]}, {"label": "USED-FOR", "tokens": "Current [[ approaches ]] to << object category recognition >> require datasets of training images to be manually prepared , with varying degrees of supervision .", "h": ["approaches"], "t": ["object category recognition"]}, {"label": "USED-FOR", "tokens": "Current << approaches >> to object category recognition require [[ datasets ]] of training images to be manually prepared , with varying degrees of supervision .", "h": ["datasets"], "t": ["approaches"]}, {"label": "USED-FOR", "tokens": "We present an [[ approach ]] that can learn an << object category >> from just its name , by utilizing the raw output of image search engines available on the Internet .", "h": ["approach"], "t": ["object category"]}, {"label": "USED-FOR", "tokens": "We develop a new model , << TSI-pLSA >> , which extends [[ pLSA ]] -LRB- as applied to visual words -RRB- to include spatial information in a translation and scale invariant manner .", "h": ["pLSA"], "t": ["TSI-pLSA"]}, {"label": "USED-FOR", "tokens": "We develop a new model , TSI-pLSA , which extends [[ pLSA ]] -LRB- as applied to << visual words >> -RRB- to include spatial information in a translation and scale invariant manner .", "h": ["pLSA"], "t": ["visual words"]}, {"label": "PART-OF", "tokens": "We develop a new model , << TSI-pLSA >> , which extends pLSA -LRB- as applied to visual words -RRB- to include [[ spatial information ]] in a translation and scale invariant manner .", "h": ["spatial information"], "t": ["TSI-pLSA"]}, {"label": "USED-FOR", "tokens": "Our [[ approach ]] can handle the high << intra-class variability >> and large proportion of unrelated images returned by search engines .", "h": ["approach"], "t": ["intra-class variability"]}, {"label": "USED-FOR", "tokens": "Our [[ approach ]] can handle the high intra-class variability and large proportion of << unrelated images >> returned by search engines .", "h": ["approach"], "t": ["unrelated images"]}, {"label": "CONJUNCTION", "tokens": "Our approach can handle the high [[ intra-class variability ]] and large proportion of << unrelated images >> returned by search engines .", "h": ["intra-class variability"], "t": ["unrelated images"]}, {"label": "USED-FOR", "tokens": "Our approach can handle the high intra-class variability and large proportion of << unrelated images >> returned by [[ search engines ]] .", "h": ["search engines"], "t": ["unrelated images"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate the << models >> on standard [[ test sets ]] , showing performance competitive with existing methods trained on hand prepared datasets .", "h": ["test sets"], "t": ["models"]}, {"label": "EVALUATE-FOR", "tokens": "We evaluate the models on standard [[ test sets ]] , showing performance competitive with existing << methods >> trained on hand prepared datasets .", "h": ["test sets"], "t": ["methods"]}, {"label": "COMPARE", "tokens": "We evaluate the << models >> on standard test sets , showing performance competitive with existing [[ methods ]] trained on hand prepared datasets .", "h": ["methods"], "t": ["models"]}, {"label": "USED-FOR", "tokens": "We evaluate the models on standard test sets , showing performance competitive with existing << methods >> trained on [[ hand prepared datasets ]] .", "h": ["hand prepared datasets"], "t": ["methods"]}, {"label": "CONJUNCTION", "tokens": "The paper provides an overview of the research conducted at LIMSI in the field of [[ speech processing ]] , but also in the related areas of << Human-Machine Communication >> , including Natural Language Processing , Non Verbal and Multimodal Communication .", "h": ["speech processing"], "t": ["Human-Machine Communication"]}, {"label": "HYPONYM-OF", "tokens": "The paper provides an overview of the research conducted at LIMSI in the field of speech processing , but also in the related areas of << Human-Machine Communication >> , including [[ Natural Language Processing ]] , Non Verbal and Multimodal Communication .", "h": ["Natural Language Processing"], "t": ["Human-Machine Communication"]}, {"label": "CONJUNCTION", "tokens": "The paper provides an overview of the research conducted at LIMSI in the field of speech processing , but also in the related areas of Human-Machine Communication , including [[ Natural Language Processing ]] , << Non Verbal and Multimodal Communication >> .", "h": ["Natural Language Processing"], "t": ["Non Verbal and Multimodal Communication"]}, {"label": "HYPONYM-OF", "tokens": "The paper provides an overview of the research conducted at LIMSI in the field of speech processing , but also in the related areas of << Human-Machine Communication >> , including Natural Language Processing , [[ Non Verbal and Multimodal Communication ]] .", "h": ["Non Verbal and Multimodal Communication"], "t": ["Human-Machine Communication"]}, {"label": "USED-FOR", "tokens": "We have calculated << analytical expressions >> for how the bias and variance of the estimators provided by various temporal difference value estimation algorithms change with offline updates over trials in absorbing Markov chains using [[ lookup table representations ]] .", "h": ["lookup table representations"], "t": ["analytical expressions"]}, {"label": "PART-OF", "tokens": "In this paper , we describe the [[ pronominal anaphora resolution module ]] of << Lucy >> , a portable English understanding system .", "h": ["pronominal anaphora resolution module"], "t": ["Lucy"]}, {"label": "HYPONYM-OF", "tokens": "In this paper , we describe the pronominal anaphora resolution module of [[ Lucy ]] , a portable << English understanding system >> .", "h": ["Lucy"], "t": ["English understanding system"]}, {"label": "USED-FOR", "tokens": "In this paper , we reported experiments of << unsupervised automatic acquisition of Italian and English verb subcategorization frames -LRB- SCFs -RRB- >> from [[ general and domain corpora ]] .", "h": ["general and domain corpora"], "t": ["unsupervised automatic acquisition of Italian and English verb subcategorization frames -LRB- SCFs -RRB-"]}, {"label": "USED-FOR", "tokens": "The proposed << technique >> operates on [[ syntactically shallow-parsed corpora ]] on the basis of a limited number of search heuristics not relying on any previous lexico-syntactic knowledge about SCFs .", "h": ["syntactically shallow-parsed corpora"], "t": ["technique"]}, {"label": "USED-FOR", "tokens": "The proposed << technique >> operates on syntactically shallow-parsed corpora on the basis of a limited number of [[ search heuristics ]] not relying on any previous lexico-syntactic knowledge about SCFs .", "h": ["search heuristics"], "t": ["technique"]}, {"label": "FEATURE-OF", "tokens": "The proposed technique operates on syntactically shallow-parsed corpora on the basis of a limited number of search heuristics not relying on any previous << lexico-syntactic knowledge >> about [[ SCFs ]] .", "h": ["SCFs"], "t": ["lexico-syntactic knowledge"]}, {"label": "USED-FOR", "tokens": "[[ Graph-cuts optimization ]] is prevalent in << vision and graphics problems >> .", "h": ["Graph-cuts optimization"], "t": ["vision and graphics problems"]}, {"label": "USED-FOR", "tokens": "It is thus of great practical importance to parallelize the << graph-cuts optimization >> using to-day 's ubiquitous [[ multi-core machines ]] .", "h": ["multi-core machines"], "t": ["graph-cuts optimization"]}, {"label": "HYPONYM-OF", "tokens": "However , the current best << serial algorithm >> by Boykov and Kolmogorov -LSB- 4 -RSB- -LRB- called the [[ BK algorithm ]] -RRB- still has the superior empirical performance .", "h": ["BK algorithm"], "t": ["serial algorithm"]}, {"label": "USED-FOR", "tokens": "In this paper , we propose a novel [[ adaptive bottom-up approach ]] to parallelize the << BK algorithm >> .", "h": ["adaptive bottom-up approach"], "t": ["BK algorithm"]}, {"label": "EVALUATE-FOR", "tokens": "Extensive experiments in common [[ applications ]] such as 2D/3D image segmentations and 3D surface fitting demonstrate the effectiveness of our << approach >> .", "h": ["applications"], "t": ["approach"]}, {"label": "HYPONYM-OF", "tokens": "Extensive experiments in common << applications >> such as [[ 2D/3D image segmentations ]] and 3D surface fitting demonstrate the effectiveness of our approach .", "h": ["2D/3D image segmentations"], "t": ["applications"]}, {"label": "CONJUNCTION", "tokens": "Extensive experiments in common applications such as [[ 2D/3D image segmentations ]] and << 3D surface fitting >> demonstrate the effectiveness of our approach .", "h": ["2D/3D image segmentations"], "t": ["3D surface fitting"]}, {"label": "HYPONYM-OF", "tokens": "Extensive experiments in common << applications >> such as 2D/3D image segmentations and [[ 3D surface fitting ]] demonstrate the effectiveness of our approach .", "h": ["3D surface fitting"], "t": ["applications"]}, {"label": "HYPONYM-OF", "tokens": "We study the question of how to make loss-aware predictions in image segmentation settings where the << evaluation function >> is the [[ Intersection-over-Union -LRB- IoU -RRB- measure ]] that is used widely in evaluating image segmentation systems .", "h": ["Intersection-over-Union -LRB- IoU -RRB- measure"], "t": ["evaluation function"]}, {"label": "EVALUATE-FOR", "tokens": "We study the question of how to make loss-aware predictions in image segmentation settings where the evaluation function is the [[ Intersection-over-Union -LRB- IoU -RRB- measure ]] that is used widely in evaluating << image segmentation systems >> .", "h": ["Intersection-over-Union -LRB- IoU -RRB- measure"], "t": ["image segmentation systems"]}, {"label": "HYPONYM-OF", "tokens": "Currently , there are two << dominant approaches >> : the [[ first ]] approximates the Expected-IoU -LRB- EIoU -RRB- score as Expected-Intersection-over-Expected-Union -LRB- EIoEU -RRB- ; and the second approach is to compute exact EIoU but only over a small set of high-quality candidate solutions .", "h": ["first"], "t": ["dominant approaches"]}, {"label": "HYPONYM-OF", "tokens": "Currently , there are two << dominant approaches >> : the first approximates the Expected-IoU -LRB- EIoU -RRB- score as Expected-Intersection-over-Expected-Union -LRB- EIoEU -RRB- ; and the [[ second approach ]] is to compute exact EIoU but only over a small set of high-quality candidate solutions .", "h": ["second approach"], "t": ["dominant approaches"]}, {"label": "USED-FOR", "tokens": "Our new << methods >> use the [[ EIoEU approximation ]] paired with high quality candidate solutions .", "h": ["EIoEU approximation"], "t": ["methods"]}, {"label": "EVALUATE-FOR", "tokens": "Experimentally we show that our new << approaches >> lead to improved performance on both [[ image segmentation tasks ]] .", "h": ["image segmentation tasks"], "t": ["approaches"]}, {"label": "HYPONYM-OF", "tokens": "Later , however , Breiman cast serious doubt on this explanation by introducing a << boosting algorithm >> , [[ arc-gv ]] , that can generate a higher margins distribution than AdaBoost and yet performs worse .", "h": ["arc-gv"], "t": ["boosting algorithm"]}, {"label": "USED-FOR", "tokens": "Later , however , Breiman cast serious doubt on this explanation by introducing a boosting algorithm , [[ arc-gv ]] , that can generate a higher << margins distribution >> than AdaBoost and yet performs worse .", "h": ["arc-gv"], "t": ["margins distribution"]}, {"label": "COMPARE", "tokens": "Later , however , Breiman cast serious doubt on this explanation by introducing a boosting algorithm , [[ arc-gv ]] , that can generate a higher margins distribution than << AdaBoost >> and yet performs worse .", "h": ["arc-gv"], "t": ["AdaBoost"]}, {"label": "EVALUATE-FOR", "tokens": "Although we can reproduce his main finding , we find that the poorer performance of arc-gv can be explained by the increased [[ complexity ]] of the << base classifiers >> it uses , an explanation supported by our experiments and entirely consistent with the margins theory .", "h": ["complexity"], "t": ["base classifiers"]}, {"label": "HYPONYM-OF", "tokens": "Although we can reproduce his main finding , we find that the poorer performance of << arc-gv >> can be explained by the increased complexity of the [[ base classifiers ]] it uses , an explanation supported by our experiments and entirely consistent with the margins theory .", "h": ["base classifiers"], "t": ["arc-gv"]}, {"label": "PART-OF", "tokens": "The [[ transfer phase ]] in << machine translation -LRB- MT -RRB- systems >> has been considered to be more complicated than analysis and generation , since it is inherently a conglomeration of individual lexical rules .", "h": ["transfer phase"], "t": ["machine translation -LRB- MT -RRB- systems"]}, {"label": "COMPARE", "tokens": "The [[ transfer phase ]] in machine translation -LRB- MT -RRB- systems has been considered to be more complicated than << analysis >> and generation , since it is inherently a conglomeration of individual lexical rules .", "h": ["transfer phase"], "t": ["analysis"]}, {"label": "COMPARE", "tokens": "The [[ transfer phase ]] in machine translation -LRB- MT -RRB- systems has been considered to be more complicated than analysis and << generation >> , since it is inherently a conglomeration of individual lexical rules .", "h": ["transfer phase"], "t": ["generation"]}, {"label": "CONJUNCTION", "tokens": "The transfer phase in machine translation -LRB- MT -RRB- systems has been considered to be more complicated than [[ analysis ]] and << generation >> , since it is inherently a conglomeration of individual lexical rules .", "h": ["analysis"], "t": ["generation"]}, {"label": "USED-FOR", "tokens": "Currently some attempts are being made to use [[ case-based reasoning ]] in << machine translation >> , that is , to make decisions on the basis of translation examples at appropriate pints in MT .", "h": ["case-based reasoning"], "t": ["machine translation"]}, {"label": "HYPONYM-OF", "tokens": "This paper proposes a new type of << transfer system >> , called a [[ Similarity-driven Transfer System -LRB- SimTran -RRB- ]] , for use in such case-based MT -LRB- CBMT -RRB- .", "h": ["Similarity-driven Transfer System -LRB- SimTran -RRB-"], "t": ["transfer system"]}, {"label": "USED-FOR", "tokens": "This paper proposes a new type of transfer system , called a [[ Similarity-driven Transfer System -LRB- SimTran -RRB- ]] , for use in such << case-based MT -LRB- CBMT -RRB- >> .", "h": ["Similarity-driven Transfer System -LRB- SimTran -RRB-"], "t": ["case-based MT -LRB- CBMT -RRB-"]}, {"label": "USED-FOR", "tokens": "This paper addresses the problem of [[ optimal alignment of non-rigid surfaces ]] from multi-view video observations to obtain a << temporally consistent representation >> .", "h": ["optimal alignment of non-rigid surfaces"], "t": ["temporally consistent representation"]}, {"label": "USED-FOR", "tokens": "This paper addresses the problem of << optimal alignment of non-rigid surfaces >> from [[ multi-view video observations ]] to obtain a temporally consistent representation .", "h": ["multi-view video observations"], "t": ["optimal alignment of non-rigid surfaces"]}, {"label": "USED-FOR", "tokens": "Conventional << non-rigid surface tracking >> performs [[ frame-to-frame alignment ]] which is subject to the accumulation of errors resulting in a drift over time .", "h": ["frame-to-frame alignment"], "t": ["non-rigid surface tracking"]}, {"label": "USED-FOR", "tokens": "Recently , << non-sequential tracking approaches >> have been introduced which reorder the input data based on a [[ dissimilarity measure ]] .", "h": ["dissimilarity measure"], "t": ["non-sequential tracking approaches"]}, {"label": "FEATURE-OF", "tokens": "They demonstrate a reduced drift and increased [[ robustness ]] to large << non-rigid deformations >> .", "h": ["robustness"], "t": ["non-rigid deformations"]}, {"label": "USED-FOR", "tokens": "[[ Optimisation of the tree ]] for << non-sequential tracking >> , which minimises the errors in temporal consistency due to both the drift and the jumps , is proposed .", "h": ["Optimisation of the tree"], "t": ["non-sequential tracking"]}, {"label": "EVALUATE-FOR", "tokens": "<< Optimisation of the tree >> for non-sequential tracking , which minimises the errors in [[ temporal consistency ]] due to both the drift and the jumps , is proposed .", "h": ["temporal consistency"], "t": ["Optimisation of the tree"]}, {"label": "USED-FOR", "tokens": "A novel [[ cluster tree ]] enforces << sequential tracking in local segments >> of the sequence while allowing global non-sequential traversal among these segments .", "h": ["cluster tree"], "t": ["sequential tracking in local segments"]}, {"label": "USED-FOR", "tokens": "A novel [[ cluster tree ]] enforces sequential tracking in local segments of the sequence while allowing << global non-sequential traversal >> among these segments .", "h": ["cluster tree"], "t": ["global non-sequential traversal"]}, {"label": "HYPONYM-OF", "tokens": "Comprehensive evaluation is performed on a variety of challenging << non-rigid surfaces >> including [[ face ]] , cloth and people .", "h": ["face"], "t": ["non-rigid surfaces"]}, {"label": "CONJUNCTION", "tokens": "Comprehensive evaluation is performed on a variety of challenging non-rigid surfaces including [[ face ]] , << cloth >> and people .", "h": ["face"], "t": ["cloth"]}, {"label": "HYPONYM-OF", "tokens": "Comprehensive evaluation is performed on a variety of challenging << non-rigid surfaces >> including face , [[ cloth ]] and people .", "h": ["cloth"], "t": ["non-rigid surfaces"]}, {"label": "CONJUNCTION", "tokens": "Comprehensive evaluation is performed on a variety of challenging non-rigid surfaces including face , [[ cloth ]] and << people >> .", "h": ["cloth"], "t": ["people"]}, {"label": "HYPONYM-OF", "tokens": "Comprehensive evaluation is performed on a variety of challenging << non-rigid surfaces >> including face , cloth and [[ people ]] .", "h": ["people"], "t": ["non-rigid surfaces"]}, {"label": "COMPARE", "tokens": "It demonstrates that the proposed [[ cluster tree ]] achieves better temporal consistency than the previous << sequential and non-sequential tracking approaches >> .", "h": ["cluster tree"], "t": ["sequential and non-sequential tracking approaches"]}, {"label": "EVALUATE-FOR", "tokens": "It demonstrates that the proposed << cluster tree >> achieves better [[ temporal consistency ]] than the previous sequential and non-sequential tracking approaches .", "h": ["temporal consistency"], "t": ["cluster tree"]}, {"label": "EVALUATE-FOR", "tokens": "Quantitative analysis on a created [[ synthetic facial performance ]] also shows an improvement by the << cluster tree >> .", "h": ["synthetic facial performance"], "t": ["cluster tree"]}, {"label": "USED-FOR", "tokens": "The << translation of English text into American Sign Language -LRB- ASL -RRB- animation >> tests the limits of traditional [[ MT architectural designs ]] .", "h": ["MT architectural designs"], "t": ["translation of English text into American Sign Language -LRB- ASL -RRB- animation"]}, {"label": "USED-FOR", "tokens": "A new [[ semantic representation ]] is proposed that uses virtual reality 3D scene modeling software to produce << spatially complex ASL phenomena >> called '' classifier predicates . ''", "h": ["semantic representation"], "t": ["spatially complex ASL phenomena"]}, {"label": "USED-FOR", "tokens": "A new << semantic representation >> is proposed that uses [[ virtual reality 3D scene modeling software ]] to produce spatially complex ASL phenomena called '' classifier predicates . ''", "h": ["virtual reality 3D scene modeling software"], "t": ["semantic representation"]}, {"label": "HYPONYM-OF", "tokens": "A new semantic representation is proposed that uses virtual reality 3D scene modeling software to produce << spatially complex ASL phenomena >> called '' [[ classifier predicates ]] . ''", "h": ["classifier predicates"], "t": ["spatially complex ASL phenomena"]}, {"label": "CONJUNCTION", "tokens": "The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates [[ transfer ]] and << direct approaches >> into a single system .", "h": ["transfer"], "t": ["direct approaches"]}, {"label": "PART-OF", "tokens": "The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates [[ transfer ]] and direct approaches into a single << system >> .", "h": ["transfer"], "t": ["system"]}, {"label": "PART-OF", "tokens": "The model acts as an interlingua within a new multi-pathway MT architecture design that also incorporates transfer and [[ direct approaches ]] into a single << system >> .", "h": ["direct approaches"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "An << extension >> to the [[ GPSG grammatical formalism ]] is proposed , allowing non-terminals to consist of finite sequences of category labels , and allowing schematic variables to range over such sequences .", "h": ["GPSG grammatical formalism"], "t": ["extension"]}, {"label": "USED-FOR", "tokens": "The [[ extension ]] is shown to be sufficient to provide a strongly adequate << grammar >> for crossed serial dependencies , as found in e.g. Dutch subordinate clauses .", "h": ["extension"], "t": ["grammar"]}, {"label": "USED-FOR", "tokens": "The extension is shown to be sufficient to provide a strongly adequate [[ grammar ]] for << crossed serial dependencies >> , as found in e.g. Dutch subordinate clauses .", "h": ["grammar"], "t": ["crossed serial dependencies"]}, {"label": "USED-FOR", "tokens": "The << extension >> is shown to be parseable by a simple [[ extension ]] to an existing parsing method for GPSG .", "h": ["extension"], "t": ["extension"]}, {"label": "USED-FOR", "tokens": "The extension is shown to be parseable by a simple << extension >> to an existing [[ parsing method ]] for GPSG .", "h": ["parsing method"], "t": ["extension"]}, {"label": "USED-FOR", "tokens": "The extension is shown to be parseable by a simple extension to an existing [[ parsing method ]] for << GPSG >> .", "h": ["parsing method"], "t": ["GPSG"]}, {"label": "USED-FOR", "tokens": "This paper presents an [[ approach ]] to << localizing functional objects >> in surveillance videos without domain knowledge about semantic object classes that may appear in the scene .", "h": ["approach"], "t": ["localizing functional objects"]}, {"label": "USED-FOR", "tokens": "This paper presents an approach to << localizing functional objects >> in [[ surveillance videos ]] without domain knowledge about semantic object classes that may appear in the scene .", "h": ["surveillance videos"], "t": ["localizing functional objects"]}, {"label": "FEATURE-OF", "tokens": "This paper presents an approach to localizing functional objects in surveillance videos without << domain knowledge >> about [[ semantic object classes ]] that may appear in the scene .", "h": ["semantic object classes"], "t": ["domain knowledge"]}, {"label": "USED-FOR", "tokens": "A [[ Bayesian framework ]] is used to probabilistically model : << people 's trajectories and intents >> , constraint map of the scene , and locations of functional objects .", "h": ["Bayesian framework"], "t": ["people 's trajectories and intents"]}, {"label": "USED-FOR", "tokens": "A [[ Bayesian framework ]] is used to probabilistically model : people 's trajectories and intents , << constraint map of the scene >> , and locations of functional objects .", "h": ["Bayesian framework"], "t": ["constraint map of the scene"]}, {"label": "USED-FOR", "tokens": "A [[ Bayesian framework ]] is used to probabilistically model : people 's trajectories and intents , constraint map of the scene , and << locations of functional objects >> .", "h": ["Bayesian framework"], "t": ["locations of functional objects"]}, {"label": "CONJUNCTION", "tokens": "A Bayesian framework is used to probabilistically model : [[ people 's trajectories and intents ]] , << constraint map of the scene >> , and locations of functional objects .", "h": ["people 's trajectories and intents"], "t": ["constraint map of the scene"]}, {"label": "CONJUNCTION", "tokens": "A Bayesian framework is used to probabilistically model : people 's trajectories and intents , [[ constraint map of the scene ]] , and << locations of functional objects >> .", "h": ["constraint map of the scene"], "t": ["locations of functional objects"]}, {"label": "USED-FOR", "tokens": "A [[ data-driven Markov Chain Monte Carlo -LRB- MCMC -RRB- process ]] is used for << inference >> .", "h": ["data-driven Markov Chain Monte Carlo -LRB- MCMC -RRB- process"], "t": ["inference"]}, {"label": "EVALUATE-FOR", "tokens": "Our evaluation on [[ videos of public squares and courtyards ]] demonstrates our effectiveness in << localizing functional objects >> and predicting people 's trajectories in unobserved parts of the video footage .", "h": ["videos of public squares and courtyards"], "t": ["localizing functional objects"]}, {"label": "EVALUATE-FOR", "tokens": "Our evaluation on [[ videos of public squares and courtyards ]] demonstrates our effectiveness in localizing functional objects and << predicting people 's trajectories >> in unobserved parts of the video footage .", "h": ["videos of public squares and courtyards"], "t": ["predicting people 's trajectories"]}, {"label": "CONJUNCTION", "tokens": "Our evaluation on videos of public squares and courtyards demonstrates our effectiveness in [[ localizing functional objects ]] and << predicting people 's trajectories >> in unobserved parts of the video footage .", "h": ["localizing functional objects"], "t": ["predicting people 's trajectories"]}, {"label": "USED-FOR", "tokens": "We propose a [[ process model ]] for << hierarchical perceptual sound organization >> , which recognizes perceptual sounds included in incoming sound signals .", "h": ["process model"], "t": ["hierarchical perceptual sound organization"]}, {"label": "PART-OF", "tokens": "We propose a process model for hierarchical perceptual sound organization , which recognizes [[ perceptual sounds ]] included in << incoming sound signals >> .", "h": ["perceptual sounds"], "t": ["incoming sound signals"]}, {"label": "USED-FOR", "tokens": "We consider << perceptual sound organization >> as a [[ scene analysis problem ]] in the auditory domain .", "h": ["scene analysis problem"], "t": ["perceptual sound organization"]}, {"label": "FEATURE-OF", "tokens": "We consider perceptual sound organization as a << scene analysis problem >> in the [[ auditory domain ]] .", "h": ["auditory domain"], "t": ["scene analysis problem"]}, {"label": "PART-OF", "tokens": "Our << model >> consists of multiple [[ processing modules ]] and a hypothesis network for quantitative integration of multiple sources of information .", "h": ["processing modules"], "t": ["model"]}, {"label": "CONJUNCTION", "tokens": "Our model consists of multiple [[ processing modules ]] and a << hypothesis network >> for quantitative integration of multiple sources of information .", "h": ["processing modules"], "t": ["hypothesis network"]}, {"label": "PART-OF", "tokens": "Our << model >> consists of multiple processing modules and a [[ hypothesis network ]] for quantitative integration of multiple sources of information .", "h": ["hypothesis network"], "t": ["model"]}, {"label": "PART-OF", "tokens": "On the << hypothesis network >> , individual information is integrated and an optimal [[ internal model ]] of perceptual sounds is automatically constructed .", "h": ["internal model"], "t": ["hypothesis network"]}, {"label": "USED-FOR", "tokens": "On the hypothesis network , individual information is integrated and an optimal [[ internal model ]] of << perceptual sounds >> is automatically constructed .", "h": ["internal model"], "t": ["perceptual sounds"]}, {"label": "USED-FOR", "tokens": "Based on the model , a [[ music scene analysis system ]] has been developed for << acoustic signals of ensemble music >> , which recognizes rhythm , chords , and source-separated musical notes .", "h": ["music scene analysis system"], "t": ["acoustic signals of ensemble music"]}, {"label": "USED-FOR", "tokens": "Based on the model , a [[ music scene analysis system ]] has been developed for acoustic signals of ensemble music , which recognizes << rhythm >> , chords , and source-separated musical notes .", "h": ["music scene analysis system"], "t": ["rhythm"]}, {"label": "USED-FOR", "tokens": "Based on the model , a [[ music scene analysis system ]] has been developed for acoustic signals of ensemble music , which recognizes rhythm , << chords >> , and source-separated musical notes .", "h": ["music scene analysis system"], "t": ["chords"]}, {"label": "USED-FOR", "tokens": "Based on the model , a [[ music scene analysis system ]] has been developed for acoustic signals of ensemble music , which recognizes rhythm , chords , and << source-separated musical notes >> .", "h": ["music scene analysis system"], "t": ["source-separated musical notes"]}, {"label": "CONJUNCTION", "tokens": "Based on the model , a music scene analysis system has been developed for acoustic signals of ensemble music , which recognizes [[ rhythm ]] , << chords >> , and source-separated musical notes .", "h": ["rhythm"], "t": ["chords"]}, {"label": "CONJUNCTION", "tokens": "Based on the model , a music scene analysis system has been developed for acoustic signals of ensemble music , which recognizes rhythm , [[ chords ]] , and << source-separated musical notes >> .", "h": ["chords"], "t": ["source-separated musical notes"]}, {"label": "FEATURE-OF", "tokens": "Experimental results show that our << method >> has permitted autonomous , stable and effective [[ information integration ]] to construct the internal model of hierarchical perceptual sounds .", "h": ["information integration"], "t": ["method"]}, {"label": "USED-FOR", "tokens": "Experimental results show that our method has permitted autonomous , stable and effective [[ information integration ]] to construct the << internal model >> of hierarchical perceptual sounds .", "h": ["information integration"], "t": ["internal model"]}, {"label": "USED-FOR", "tokens": "Experimental results show that our method has permitted autonomous , stable and effective information integration to construct the [[ internal model ]] of << hierarchical perceptual sounds >> .", "h": ["internal model"], "t": ["hierarchical perceptual sounds"]}, {"label": "USED-FOR", "tokens": "We directly investigate a subject of much recent debate : do [[ word sense disambigation models ]] help << statistical machine translation quality >> ?", "h": ["word sense disambigation models"], "t": ["statistical machine translation quality"]}, {"label": "USED-FOR", "tokens": "Using a state-of-the-art [[ Chinese word sense disambiguation model ]] to choose << translation candidates >> for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone .", "h": ["Chinese word sense disambiguation model"], "t": ["translation candidates"]}, {"label": "USED-FOR", "tokens": "Using a state-of-the-art Chinese word sense disambiguation model to choose [[ translation candidates ]] for a typical << IBM statistical MT system >> , we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone .", "h": ["translation candidates"], "t": ["IBM statistical MT system"]}, {"label": "COMPARE", "tokens": "Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that [[ word sense disambiguation ]] does not yield significantly better translation quality than the << statistical machine translation system >> alone .", "h": ["word sense disambiguation"], "t": ["statistical machine translation system"]}, {"label": "EVALUATE-FOR", "tokens": "Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that << word sense disambiguation >> does not yield significantly better [[ translation quality ]] than the statistical machine translation system alone .", "h": ["translation quality"], "t": ["word sense disambiguation"]}, {"label": "EVALUATE-FOR", "tokens": "Using a state-of-the-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system , we find that word sense disambiguation does not yield significantly better [[ translation quality ]] than the << statistical machine translation system >> alone .", "h": ["translation quality"], "t": ["statistical machine translation system"]}, {"label": "USED-FOR", "tokens": "[[ Image sequence processing techniques ]] are used to study << exchange , growth , and transport processes >> and to tackle key questions in environmental physics and biology .", "h": ["Image sequence processing techniques"], "t": ["exchange , growth , and transport processes"]}, {"label": "CONJUNCTION", "tokens": "Image sequence processing techniques are used to study exchange , growth , and transport processes and to tackle key questions in [[ environmental physics ]] and << biology >> .", "h": ["environmental physics"], "t": ["biology"]}, {"label": "EVALUATE-FOR", "tokens": "These applications require high [[ accuracy ]] for the << estimation of the motion field >> since the most interesting parameters of the dynamical processes studied are contained in first-order derivatives of the motion field or in dynamical changes of the moving objects .", "h": ["accuracy"], "t": ["estimation of the motion field"]}, {"label": "USED-FOR", "tokens": "These << applications >> require high accuracy for the [[ estimation of the motion field ]] since the most interesting parameters of the dynamical processes studied are contained in first-order derivatives of the motion field or in dynamical changes of the moving objects .", "h": ["estimation of the motion field"], "t": ["applications"]}, {"label": "CONJUNCTION", "tokens": "These applications require high accuracy for the estimation of the motion field since the most interesting parameters of the dynamical processes studied are contained in [[ first-order derivatives of the motion field ]] or in << dynamical changes of the moving objects >> .", "h": ["first-order derivatives of the motion field"], "t": ["dynamical changes of the moving objects"]}, {"label": "USED-FOR", "tokens": "A << tensor method >> tuned with carefully optimized [[ derivative filters ]] yields reliable and dense displacement vector fields -LRB- DVF -RRB- with an accuracy of up to a few hundredth pixels/frame for real-world images .", "h": ["derivative filters"], "t": ["tensor method"]}, {"label": "EVALUATE-FOR", "tokens": "A tensor method tuned with carefully optimized derivative filters yields reliable and dense << displacement vector fields -LRB- DVF -RRB- >> with an accuracy of up to a few hundredth [[ pixels/frame ]] for real-world images .", "h": ["pixels/frame"], "t": ["displacement vector fields -LRB- DVF -RRB-"]}, {"label": "USED-FOR", "tokens": "A tensor method tuned with carefully optimized derivative filters yields reliable and dense displacement vector fields -LRB- DVF -RRB- with an accuracy of up to a few hundredth << pixels/frame >> for [[ real-world images ]] .", "h": ["real-world images"], "t": ["pixels/frame"]}, {"label": "EVALUATE-FOR", "tokens": "The [[ accuracy ]] of the << tensor method >> is verified with computer-generated sequences and a calibrated image sequence .", "h": ["accuracy"], "t": ["tensor method"]}, {"label": "EVALUATE-FOR", "tokens": "The accuracy of the << tensor method >> is verified with [[ computer-generated sequences ]] and a calibrated image sequence .", "h": ["computer-generated sequences"], "t": ["tensor method"]}, {"label": "CONJUNCTION", "tokens": "The accuracy of the tensor method is verified with [[ computer-generated sequences ]] and a << calibrated image sequence >> .", "h": ["computer-generated sequences"], "t": ["calibrated image sequence"]}, {"label": "EVALUATE-FOR", "tokens": "The accuracy of the << tensor method >> is verified with computer-generated sequences and a [[ calibrated image sequence ]] .", "h": ["calibrated image sequence"], "t": ["tensor method"]}, {"label": "EVALUATE-FOR", "tokens": "With the improvements in [[ accuracy ]] the << motion estimation >> is now rather limited by imperfections in the CCD sensors , especially the spatial nonuni-formity in the responsivity .", "h": ["accuracy"], "t": ["motion estimation"]}, {"label": "USED-FOR", "tokens": "With the improvements in accuracy the << motion estimation >> is now rather limited by imperfections in the [[ CCD sensors ]] , especially the spatial nonuni-formity in the responsivity .", "h": ["CCD sensors"], "t": ["motion estimation"]}, {"label": "FEATURE-OF", "tokens": "With the improvements in accuracy the motion estimation is now rather limited by imperfections in the CCD sensors , especially the [[ spatial nonuni-formity ]] in the << responsivity >> .", "h": ["spatial nonuni-formity"], "t": ["responsivity"]}, {"label": "FEATURE-OF", "tokens": "With the improvements in accuracy the motion estimation is now rather limited by imperfections in the << CCD sensors >> , especially the spatial nonuni-formity in the [[ responsivity ]] .", "h": ["responsivity"], "t": ["CCD sensors"]}, {"label": "USED-FOR", "tokens": "The application of the [[ techniques ]] to the << analysis of plant growth >> , to ocean surface microturbulence in IR image sequences , and to sediment transport is demonstrated .", "h": ["techniques"], "t": ["analysis of plant growth"]}, {"label": "USED-FOR", "tokens": "The application of the [[ techniques ]] to the analysis of plant growth , to << ocean surface microturbulence in IR image sequences >> , and to sediment transport is demonstrated .", "h": ["techniques"], "t": ["ocean surface microturbulence in IR image sequences"]}, {"label": "USED-FOR", "tokens": "The application of the [[ techniques ]] to the analysis of plant growth , to ocean surface microturbulence in IR image sequences , and to << sediment transport >> is demonstrated .", "h": ["techniques"], "t": ["sediment transport"]}, {"label": "CONJUNCTION", "tokens": "The application of the techniques to the [[ analysis of plant growth ]] , to << ocean surface microturbulence in IR image sequences >> , and to sediment transport is demonstrated .", "h": ["analysis of plant growth"], "t": ["ocean surface microturbulence in IR image sequences"]}, {"label": "CONJUNCTION", "tokens": "The application of the techniques to the analysis of plant growth , to [[ ocean surface microturbulence in IR image sequences ]] , and to << sediment transport >> is demonstrated .", "h": ["ocean surface microturbulence in IR image sequences"], "t": ["sediment transport"]}, {"label": "USED-FOR", "tokens": "We present a [[ Czech-English statistical machine translation system ]] which performs << tree-to-tree translation of dependency structures >> .", "h": ["Czech-English statistical machine translation system"], "t": ["tree-to-tree translation of dependency structures"]}, {"label": "USED-FOR", "tokens": "The only << bilingual resource >> required is a [[ sentence-aligned parallel corpus ]] .", "h": ["sentence-aligned parallel corpus"], "t": ["bilingual resource"]}, {"label": "COMPARE", "tokens": "We also refer to an evaluation method and plan to compare our [[ system ]] 's output with a << benchmark system >> .", "h": ["system"], "t": ["benchmark system"]}, {"label": "FEATURE-OF", "tokens": "This paper describes the understanding process of the << spatial descriptions >> in [[ Japanese ]] .", "h": ["Japanese"], "t": ["spatial descriptions"]}, {"label": "USED-FOR", "tokens": "To reconstruct the model , the authors extract the qualitative spatial constraints from the text , and represent them as the << numerical constraints >> on the [[ spatial attributes of the entities ]] .", "h": ["spatial attributes of the entities"], "t": ["numerical constraints"]}, {"label": "USED-FOR", "tokens": "Such [[ context information ]] is therefore important to characterize the << intrinsic representation of a video frame >> .", "h": ["context information"], "t": ["intrinsic representation of a video frame"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a novel [[ approach ]] to learn the << deep video representation >> by exploring both local and holistic contexts .", "h": ["approach"], "t": ["deep video representation"]}, {"label": "USED-FOR", "tokens": "In this paper , we present a novel << approach >> to learn the deep video representation by exploring both [[ local and holistic contexts ]] .", "h": ["local and holistic contexts"], "t": ["approach"]}, {"label": "USED-FOR", "tokens": "Specifically , we propose a [[ triplet sampling mechanism ]] to encode the << local temporal relationship of adjacent frames >> based on their deep representations .", "h": ["triplet sampling mechanism"], "t": ["local temporal relationship of adjacent frames"]}, {"label": "USED-FOR", "tokens": "Specifically , we propose a << triplet sampling mechanism >> to encode the local temporal relationship of adjacent frames based on their [[ deep representations ]] .", "h": ["deep representations"], "t": ["triplet sampling mechanism"]}, {"label": "USED-FOR", "tokens": "In addition , we incorporate the [[ graph structure of the video ]] , as a << priori >> , to holistically preserve the inherent correlations among video frames .", "h": ["graph structure of the video"], "t": ["priori"]}, {"label": "USED-FOR", "tokens": "Our << approach >> is fully unsupervised and trained in an [[ end-to-end deep convolutional neu-ral network architecture ]] .", "h": ["end-to-end deep convolutional neu-ral network architecture"], "t": ["approach"]}, {"label": "COMPARE", "tokens": "By extensive experiments , we show that our [[ learned representation ]] can significantly boost several video recognition tasks -LRB- retrieval , classification , and highlight detection -RRB- over traditional << video representations >> .", "h": ["learned representation"], "t": ["video representations"]}, {"label": "EVALUATE-FOR", "tokens": "By extensive experiments , we show that our << learned representation >> can significantly boost several [[ video recognition tasks ]] -LRB- retrieval , classification , and highlight detection -RRB- over traditional video representations .", "h": ["video recognition tasks"], "t": ["learned representation"]}, {"label": "EVALUATE-FOR", "tokens": "By extensive experiments , we show that our learned representation can significantly boost several [[ video recognition tasks ]] -LRB- retrieval , classification , and highlight detection -RRB- over traditional << video representations >> .", "h": ["video recognition tasks"], "t": ["video representations"]}, {"label": "HYPONYM-OF", "tokens": "By extensive experiments , we show that our learned representation can significantly boost several << video recognition tasks >> -LRB- [[ retrieval ]] , classification , and highlight detection -RRB- over traditional video representations .", "h": ["retrieval"], "t": ["video recognition tasks"]}, {"label": "CONJUNCTION", "tokens": "By extensive experiments , we show that our learned representation can significantly boost several video recognition tasks -LRB- [[ retrieval ]] , << classification >> , and highlight detection -RRB- over traditional video representations .", "h": ["retrieval"], "t": ["classification"]}, {"label": "HYPONYM-OF", "tokens": "By extensive experiments , we show that our learned representation can significantly boost several << video recognition tasks >> -LRB- retrieval , [[ classification ]] , and highlight detection -RRB- over traditional video representations .", "h": ["classification"], "t": ["video recognition tasks"]}, {"label": "CONJUNCTION", "tokens": "By extensive experiments , we show that our learned representation can significantly boost several video recognition tasks -LRB- retrieval , [[ classification ]] , and << highlight detection >> -RRB- over traditional video representations .", "h": ["classification"], "t": ["highlight detection"]}, {"label": "HYPONYM-OF", "tokens": "By extensive experiments , we show that our learned representation can significantly boost several << video recognition tasks >> -LRB- retrieval , classification , and [[ highlight detection ]] -RRB- over traditional video representations .", "h": ["highlight detection"], "t": ["video recognition tasks"]}, {"label": "FEATURE-OF", "tokens": "For << mobile speech application >> , [[ speaker DOA estimation accuracy ]] , interference robustness and compact physical size are three key factors .", "h": ["speaker DOA estimation accuracy"], "t": ["mobile speech application"]}, {"label": "CONJUNCTION", "tokens": "For mobile speech application , [[ speaker DOA estimation accuracy ]] , << interference robustness >> and compact physical size are three key factors .", "h": ["speaker DOA estimation accuracy"], "t": ["interference robustness"]}, {"label": "FEATURE-OF", "tokens": "For << mobile speech application >> , speaker DOA estimation accuracy , [[ interference robustness ]] and compact physical size are three key factors .", "h": ["interference robustness"], "t": ["mobile speech application"]}, {"label": "CONJUNCTION", "tokens": "For mobile speech application , speaker DOA estimation accuracy , [[ interference robustness ]] and << compact physical size >> are three key factors .", "h": ["interference robustness"], "t": ["compact physical size"]}, {"label": "FEATURE-OF", "tokens": "For << mobile speech application >> , speaker DOA estimation accuracy , interference robustness and [[ compact physical size ]] are three key factors .", "h": ["compact physical size"], "t": ["mobile speech application"]}, {"label": "USED-FOR", "tokens": "[[ It ]] is achieved by deriving the inter-sensor data ratio model of an AVS in bispectrum domain -LRB- BISDR -RRB- and exploring the << favorable properties >> of bispectrum , such as zero value of Gaussian process and different distribution of speech and NSI .", "h": ["It"], "t": ["favorable properties"]}, {"label": "USED-FOR", "tokens": "It is achieved by deriving the [[ inter-sensor data ratio model ]] of an << AVS >> in bispectrum domain -LRB- BISDR -RRB- and exploring the favorable properties of bispectrum , such as zero value of Gaussian process and different distribution of speech and NSI .", "h": ["inter-sensor data ratio model"], "t": ["AVS"]}, {"label": "USED-FOR", "tokens": "It is achieved by deriving the inter-sensor data ratio model of an << AVS >> in [[ bispectrum domain -LRB- BISDR -RRB- ]] and exploring the favorable properties of bispectrum , such as zero value of Gaussian process and different distribution of speech and NSI .", "h": ["bispectrum domain -LRB- BISDR -RRB-"], "t": ["AVS"]}, {"label": "HYPONYM-OF", "tokens": "It is achieved by deriving the inter-sensor data ratio model of an AVS in bispectrum domain -LRB- BISDR -RRB- and exploring the << favorable properties >> of bispectrum , such as [[ zero value of Gaussian process ]] and different distribution of speech and NSI .", "h": ["zero value of Gaussian process"], "t": ["favorable properties"]}, {"label": "CONJUNCTION", "tokens": "It is achieved by deriving the inter-sensor data ratio model of an AVS in bispectrum domain -LRB- BISDR -RRB- and exploring the favorable properties of bispectrum , such as [[ zero value of Gaussian process ]] and different << distribution of speech and NSI >> .", "h": ["zero value of Gaussian process"], "t": ["distribution of speech and NSI"]}, {"label": "HYPONYM-OF", "tokens": "It is achieved by deriving the inter-sensor data ratio model of an AVS in bispectrum domain -LRB- BISDR -RRB- and exploring the << favorable properties >> of bispectrum , such as zero value of Gaussian process and different [[ distribution of speech and NSI ]] .", "h": ["distribution of speech and NSI"], "t": ["favorable properties"]}, {"label": "USED-FOR", "tokens": "Specifically , a reliable [[ bispectrum mask ]] is generated to guarantee that the << speaker DOA cues >> , derived from BISDR , are robust to NSI in terms of speech sparsity and large bispectrum amplitude of the captured signals .", "h": ["bispectrum mask"], "t": ["speaker DOA cues"]}, {"label": "USED-FOR", "tokens": "Specifically , a reliable bispectrum mask is generated to guarantee that the << speaker DOA cues >> , derived from [[ BISDR ]] , are robust to NSI in terms of speech sparsity and large bispectrum amplitude of the captured signals .", "h": ["BISDR"], "t": ["speaker DOA cues"]}, {"label": "USED-FOR", "tokens": "Intensive experiments demonstrate an improved performance of our proposed [[ algorithm ]] under various << NSI conditions >> even when SIR is smaller than 0dB .", "h": ["algorithm"], "t": ["NSI conditions"]}, {"label": "PART-OF", "tokens": "In this paper , we want to show how the [[ morphological component ]] of an existing << NLP-system for Dutch -LRB- Dutch Medical Language Processor - DMLP -RRB- >> has been extended in order to produce output that is compatible with the language independent modules of the LSP-MLP system -LRB- Linguistic String Project - Medical Language Processor -RRB- of the New York University .", "h": ["morphological component"], "t": ["NLP-system for Dutch -LRB- Dutch Medical Language Processor - DMLP -RRB-"]}, {"label": "PART-OF", "tokens": "In this paper , we want to show how the morphological component of an existing NLP-system for Dutch -LRB- Dutch Medical Language Processor - DMLP -RRB- has been extended in order to produce output that is compatible with the [[ language independent modules ]] of the << LSP-MLP system -LRB- Linguistic String Project - Medical Language Processor -RRB- >> of the New York University .", "h": ["language independent modules"], "t": ["LSP-MLP system -LRB- Linguistic String Project - Medical Language Processor -RRB-"]}, {"label": "USED-FOR", "tokens": "The << former >> can take advantage of the language independent developments of the [[ latter ]] , while focusing on idiosyncrasies for Dutch .", "h": ["latter"], "t": ["former"]}, {"label": "USED-FOR", "tokens": "The former can take advantage of the language independent developments of the latter , while focusing on << idiosyncrasies >> for [[ Dutch ]] .", "h": ["Dutch"], "t": ["idiosyncrasies"]}, {"label": "PART-OF", "tokens": "This general strategy will be illustrated by a practical application , namely the highlighting of [[ relevant information ]] in a << patient discharge summary -LRB- PDS -RRB- >> by means of modern HyperText Mark-Up Language -LRB- HTML -RRB- technology .", "h": ["relevant information"], "t": ["patient discharge summary -LRB- PDS -RRB-"]}, {"label": "USED-FOR", "tokens": "This general strategy will be illustrated by a practical application , namely the << highlighting of relevant information >> in a patient discharge summary -LRB- PDS -RRB- by means of modern [[ HyperText Mark-Up Language -LRB- HTML -RRB- technology ]] .", "h": ["HyperText Mark-Up Language -LRB- HTML -RRB- technology"], "t": ["highlighting of relevant information"]}, {"label": "USED-FOR", "tokens": "Such an [[ application ]] can be of use for << medical administrative purposes >> in a hospital environment .", "h": ["application"], "t": ["medical administrative purposes"]}, {"label": "PART-OF", "tokens": "<< CriterionSM Online Essay Evaluation Service >> includes a capability that labels sentences in student writing with [[ essay-based discourse elements ]] -LRB- e.g. , thesis statements -RRB- .", "h": ["essay-based discourse elements"], "t": ["CriterionSM Online Essay Evaluation Service"]}, {"label": "HYPONYM-OF", "tokens": "CriterionSM Online Essay Evaluation Service includes a capability that labels sentences in student writing with << essay-based discourse elements >> -LRB- e.g. , [[ thesis statements ]] -RRB- .", "h": ["thesis statements"], "t": ["essay-based discourse elements"]}, {"label": "USED-FOR", "tokens": "We describe a new [[ system ]] that enhances << Criterion 's capability >> , by evaluating multiple aspects of coherence in essays .", "h": ["system"], "t": ["Criterion 's capability"]}, {"label": "EVALUATE-FOR", "tokens": "We describe a new << system >> that enhances Criterion 's capability , by evaluating multiple aspects of [[ coherence in essays ]] .", "h": ["coherence in essays"], "t": ["system"]}, {"label": "USED-FOR", "tokens": "This [[ system ]] identifies << features >> of sentences based on semantic similarity measures and discourse structure .", "h": ["system"], "t": ["features"]}, {"label": "USED-FOR", "tokens": "This system identifies << features >> of sentences based on [[ semantic similarity measures ]] and discourse structure .", "h": ["semantic similarity measures"], "t": ["features"]}, {"label": "USED-FOR", "tokens": "This system identifies << features >> of sentences based on semantic similarity measures and [[ discourse structure ]] .", "h": ["discourse structure"], "t": ["features"]}, {"label": "CONJUNCTION", "tokens": "This system identifies features of sentences based on << semantic similarity measures >> and [[ discourse structure ]] .", "h": ["discourse structure"], "t": ["semantic similarity measures"]}, {"label": "USED-FOR", "tokens": "A << support vector machine >> uses these [[ features ]] to capture breakdowns in coherence due to relatedness to the essay question and relatedness between discourse elements .", "h": ["features"], "t": ["support vector machine"]}, {"label": "USED-FOR", "tokens": "A support vector machine uses these [[ features ]] to capture << breakdowns in coherence >> due to relatedness to the essay question and relatedness between discourse elements .", "h": ["features"], "t": ["breakdowns in coherence"]}, {"label": "EVALUATE-FOR", "tokens": "<< Intra-sentential quality >> is evaluated with [[ rule-based heuristics ]] .", "h": ["rule-based heuristics"], "t": ["Intra-sentential quality"]}, {"label": "COMPARE", "tokens": "Results indicate that the [[ system ]] yields higher performance than a << baseline >> on all three aspects .", "h": ["system"], "t": ["baseline"]}, {"label": "USED-FOR", "tokens": "This paper presents an [[ algorithm ]] for << labeling curvilinear structure >> at multiple scales in line drawings and edge images Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .", "h": ["algorithm"], "t": ["labeling curvilinear structure"]}, {"label": "FEATURE-OF", "tokens": "This paper presents an algorithm for << labeling curvilinear structure >> at multiple scales in [[ line drawings ]] and edge images Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .", "h": ["line drawings"], "t": ["labeling curvilinear structure"]}, {"label": "CONJUNCTION", "tokens": "This paper presents an algorithm for labeling curvilinear structure at multiple scales in [[ line drawings ]] and << edge images >> Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .", "h": ["line drawings"], "t": ["edge images"]}, {"label": "FEATURE-OF", "tokens": "This paper presents an algorithm for << labeling curvilinear structure >> at multiple scales in line drawings and [[ edge images ]] Symbolic CURVE-ELEMENT tokens residing in a spatially-indexed and scale-indexed data structure denote circular arcs fit to image data .", "h": ["edge images"], "t": ["labeling curvilinear structure"]}, {"label": "PART-OF", "tokens": "This paper presents an algorithm for labeling curvilinear structure at multiple scales in line drawings and edge images Symbolic [[ CURVE-ELEMENT tokens ]] residing in a << spatially-indexed and scale-indexed data structure >> denote circular arcs fit to image data .", "h": ["CURVE-ELEMENT tokens"], "t": ["spatially-indexed and scale-indexed data structure"]}]